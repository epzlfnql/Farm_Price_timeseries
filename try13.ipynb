{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyOfeDQlSboM4ua7kwvC1B60"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1C4Fxm1gBe7u","executionInfo":{"status":"ok","timestamp":1664370792535,"user_tz":-540,"elapsed":20529,"user":{"displayName":"이태영","userId":"15551978220746215296"}},"outputId":"ce343981-5fba-408f-ba51-ba399ad9b9a0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["<pre>\n","- ols feature selection\n","- 화요일 지우기\n","- (거의 try11이랑 똑같이 적용하기) feature selection만 ols로 해보기\n","- 7번 채우기는 일단 강사님코드 점수확인해보고 적용할지 말지 생각\n","-> 적용은 하자(0.004는 오르니깐)\n","\n","\n","결과 : 0.408\n","(lasso feature selection보다 0.0.07높다)\n","그리고 화요일 drop 안하는게 더 나을 것 같다."],"metadata":{"id":"nppokbB7Bg49"}},{"cell_type":"code","source":["!pip install talib-binary"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NFxD2xjKBqP4","executionInfo":{"status":"ok","timestamp":1664370817723,"user_tz":-540,"elapsed":3978,"user":{"displayName":"이태영","userId":"15551978220746215296"}},"outputId":"3d7593fe-5221-4aed-f5d1-dbc68f6ac594"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting talib-binary\n","  Downloading talib_binary-0.4.19-cp37-cp37m-manylinux1_x86_64.whl (2.4 MB)\n","\u001b[K     |████████████████████████████████| 2.4 MB 8.1 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from talib-binary) (1.21.6)\n","Installing collected packages: talib-binary\n","Successfully installed talib-binary-0.4.19\n"]}]},{"cell_type":"code","source":["!pip install pandasql"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ou0yf0wmTER5","executionInfo":{"status":"ok","timestamp":1664370823918,"user_tz":-540,"elapsed":3882,"user":{"displayName":"이태영","userId":"15551978220746215296"}},"outputId":"93163a1a-e589-4fc7-b1fa-3963f2146bce"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pandasql\n","  Downloading pandasql-0.7.3.tar.gz (26 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pandasql) (1.21.6)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from pandasql) (1.3.5)\n","Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.7/dist-packages (from pandasql) (1.4.41)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pandasql) (2022.2.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pandasql) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->pandasql) (1.15.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->pandasql) (1.1.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->pandasql) (4.12.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy->pandasql) (3.8.1)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy->pandasql) (4.1.1)\n","Building wheels for collected packages: pandasql\n","  Building wheel for pandasql (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pandasql: filename=pandasql-0.7.3-py3-none-any.whl size=26784 sha256=2958b8997ee6324f570ffa335d4552478125d1494b2fbc2466baa0accf842d93\n","  Stored in directory: /root/.cache/pip/wheels/5c/4b/ec/41f4e116c8053c3654e2c2a47c62b4fca34cc67ef7b55deb7f\n","Successfully built pandasql\n","Installing collected packages: pandasql\n","Successfully installed pandasql-0.7.3\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","import warnings\n","from glob import glob\n","import tensorflow as tf\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from sklearn.model_selection import train_test_split\n","from pandasql import sqldf\n","from sklearn.preprocessing import StandardScaler\n","import os\n","import warnings\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","warnings.filterwarnings(action='ignore')\n","from matplotlib import font_manager, rc\n","from sklearn.model_selection import train_test_split\n","import statsmodels.api as sm\n","\n","# font_path = \"C:\\\\Windows\\\\Fonts\\\\\\x7f\\x7f\\x7f\\x7fBOLD.TTF\"\n","# font = font_manager.FontProperties(fname=font_path).get_name()\n","# rc('font', family=font)\n","from sklearn.manifold import TSNE\n","import re\n","import talib as tb"],"metadata":{"id":"CXzBOG7CTFMw","executionInfo":{"status":"ok","timestamp":1664370832891,"user_tz":-540,"elapsed":4096,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def weather(주소,name = 'train'):\n","    t=0\n","    if name =='test':\n","        test_dir = 주소.split('/')[-2][-1]\n","    for i in tqdm(range(0,37)):\n","        globals()[f'{name}_weather_'+str(i)]=pd.DataFrame()\n","\n","        for k in range(0,3):\n","            try:\n","                weather = pd.read_csv(주소+f'/weather_{i}_{t}.csv')\n","                if len(weather)==0:\n","                    weather = pd.read_csv(주소+f'/weather_0_0.csv')\n","                    weather=weather.set_index('datadate')\n","                    weather[weather.columns] = 0\n","                    weather=weather.reset_index()\n","                weather = weather.set_index('datadate')\n","                weather['습도(%)']=weather['습도(%)'].apply(lambda x : np.nan if x==' ' else x)\n","                weather['습도(%)']=weather['습도(%)'].astype('float')\n","                weather.columns = weather.columns+f'_{k%3}'\n","                globals()[f'{name}_weather_'+str(i)]=pd.merge(globals()[f'{name}_weather_'+str(i)],weather,how='right',left_index=True, right_index=True)    \n","\n","\n","            except:\n","                break\n","            t+=1\n","\n","\n","\n","        globals()[f'{name}_weather_'+str(i)]=globals()[f'{name}_weather_'+str(i)].reset_index()\n","        cols = globals()[f'{name}_weather_'+str(i)].columns\n","        globals()[f'{name}_weather_'+str(i)]['월일']=globals()[f'{name}_weather_'+str(i)]['datadate']%10000\n","       \n","\n","        \n","        if name == 'train':\n","            qwe =  globals()[f'{name}_weather_'+str(i)].groupby('월일').mean().reset_index()\n","            qwe.columns = qwe.columns+'_평균'\n","            globals()[f'train_weather_평균_'+str(i)]= qwe\n","\n","\n","\n","            \n","\n","\n","        globals()[f'{name}_weather_'+str(i)] = pd.merge(globals()[f'{name}_weather_'+str(i)],globals()[f'train_weather_평균_'+str(i)],how='left',left_on = '월일',right_on='월일_평균')\n","        for c in cols:\n","            globals()[f'{name}_weather_'+str(i)][c]=globals()[f'{name}_weather_'+str(i)].apply(lambda x: x[c] if np.isnan(x[c]) == False else (0 if np.isnan(x[c+'_평균']) else x[c+'_평균']),axis=1)\n","                \n","    \n","        \n","        \n","        if name=='test':\n","            globals()['sep_'+test_dir+f'_{name}_weather_'+str(i)] = globals()[f'{name}_weather_'+str(i)][cols]\n","\n","        else:\n","            globals()[f'{name}_weather_'+str(i)] = globals()[f'{name}_weather_'+str(i)][cols]\n","\n","        "],"metadata":{"id":"Z2bFU-3YTIpc","executionInfo":{"status":"ok","timestamp":1664370836067,"user_tz":-540,"elapsed":682,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def add_dosomae(주소,name='train', option=1):\n","        if name == 'test':\n","            test_dir = 주소.split('/')[-2][-1]\n","        data_list = glob(주소+'*') # train raw 데이터 넣을지 / test raw 데이터 넣을지 경로\n","        domae = []\n","        somae = []\n","\n","        for i in data_list:\n","            if 'domae' in i:\n","                domae.append(i)\n","            if 'somae' in i:\n","                somae.append(i)\n","        \n","        sum_df = pd.DataFrame()\n","                \n","        if option == 1:\n","            df = domae\n","            text = 'domae'\n","        else:\n","            df = somae\n","            text = 'somae'\n","\n","\n","        for i in tqdm(df):\n","            test = pd.read_csv(i)\n","            \n","            k=i.split('/')[-1].split(\"_\")[1].split(\".\")[0]\n","            test.fillna(0,inplace=True) # 널값 0으로 채워주고\n","            if len(test)==0:\n","                test = pd.read_csv(주소+f'/domae_0.csv')\n","                test=test.set_index('datadate')\n","                test[test.columns] = 0\n","                test=test.reset_index()\n","                \n","\n","            \n","            test['조사단위(kg)'] = test['조사단위(kg)'].replace('g$|kg$|개|포기|접', '',regex=True)\n","            test=test.astype({'조사단위(kg)':'float'})\n","            test.loc[test['조사단위(kg)'] >= 100, '단위당가격'] = test['가격(원)']* test['조사단위(kg)'] /1000\n","            test.loc[test['조사단위(kg)'] < 100, '단위당가격'] = test['가격(원)']/ test['조사단위(kg)']\n","            \n","            sep = test.loc[(test['등급명'] == '상품') | (test['등급명'] == 'S과') | (test['등급명'] == 0)]   # 모든 상품에 대해서 수행하지 않고 GRAD_NM이 '상품', 'S과' 만 해당하는 품목 가져옴\n","            sep = sep[['datadate', '등급명', '조사단위(kg)', '가격(원)']]\n","\n","            sep.rename(columns={\"가격(원)\": \"가격\"}, inplace=True)\n","            sep2 = sqldf(\n","                f\"select datadate, max(가격) as '일자별_{text}가격_최대(원)', avg(가격) as '일자별_{text}가격_평균(원)', min(가격) as '일자별_{text}가격_최소(원)' from sep group by datadate\")\n","\n","            sep2.fillna(0,inplace=True)\n","#             if len(sep2) == 0:\n","#                 if name=='test':\n","#                     sep2 =  globals()['sep_'+test_dir+f'_{name}_{text}_0'] \n","#                     sep2\n","\n","#                 else:\n","#                     sep2 =  globals()[f'{name}_{text}_0'] \n","#                     sep2[sep2.columns] = 0 \n","                \n","                \n","            if name=='test':\n","                globals()['sep_'+test_dir+f'_{name}_{text}_{k}'] = sep2\n","                \n","            else:\n","                globals()[f'{name}_{text}_{k}'] = sep2\n"," \n","                \n","\n","\n","        \n","                               \n","               \n","             "],"metadata":{"id":"36w44dGDTQ_k","executionInfo":{"status":"ok","timestamp":1664370838521,"user_tz":-540,"elapsed":2,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def pummok(주소,name='train'):\n","    if name == 'test':\n","        test_dir = 주소.split('/')[-2][-1]\n","    for i in tqdm(range(0,37)):\n","\n","        try:\n","            pummok = pd.read_csv(주소+f'/pummok_{i}.csv')\n","            if len(pummok) == 0 :\n","                pummok = pd.read_csv(주소+f'/pummok_0.csv')\n","                pummok = pummok.groupby('datadate').mean()\n","                pummok = pummok[['단가(원)','거래량','해당일자_전체평균가격(원)']]\n","                pummok[pummok.columns] = 0\n","                pummok = pummok.reset_index()\n","                \n","            else:   \n","                pummok = pummok.groupby('datadate').mean()\n","                pummok = pummok[['단가(원)','거래량','해당일자_전체평균가격(원)']]\n","\n","                # 새로 추가한 부분\n","                # pummok['해당일자_전체평균가격(원)'].fillna(0, inplace=True)\n","                # pummok['wma14'] =  tb.WMA(pummok['해당일자_전체평균가격(원)'], 14)\n","                # pummok['wma14'].fillna(method='bfill', inplace=True)\n","                # pummok['해당일자_전체평균가격(원)'] = np.where(pummok['해당일자_전체평균가격(원)']==0, pummok['wma14'], pummok['해당일자_전체평균가격(원)'])\n","                # pummok.drop('wma14', axis=1, inplace=True)\n","\n","\n","                \n","                pummok = pummok.reset_index()\n","                pummok.fillna(0, inplace = True)\n","                # pummok.fillna(0,inplace = True)\n","                \n","                \n","                \n","\n","        except:\n","            continue\n","\n","        if name=='test':\n","            globals()['sep_'+test_dir+f'_{name}_pummok_{i}'] = pummok\n","\n","        else:\n","            globals()[f'{name}_pummok_{i}'] = pummok\n"],"metadata":{"id":"aTfjyo17TSpE","executionInfo":{"status":"ok","timestamp":1664370841054,"user_tz":-540,"elapsed":552,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def imexport(주소, name='train'):\n","    if name == 'test':\n","        test_dir = 주소.split('/')[-2][-1]\n","    for i in tqdm(range(0,37)):\n","        try:\n","            imexport=pd.read_csv(주소+f'/imexport_{i}.csv')\n","            \n","            if len(imexport) == 0:\n","                imexport = pd.read_csv(주소+f'/imexport_0.csv')\n","                imexport = imexport.groupby('datadate').mean()\n","                imexport[imexport.columns] = 0\n","\n","            else:  \n","                imexport = imexport.groupby('datadate').mean()\n","\n","\n","        except:\n","            \n","            imexport = pd.read_csv(주소+f'/imexport_0.csv')\n","            imexport = imexport.groupby('datadate').mean()\n","            imexport[imexport.columns] = 0\n","            pass\n","\n","\n","        imexport.fillna(0,inplace=True)\n","        if name=='test':\n","            globals()['sep_'+test_dir+f'_{name}_imexport_{i}'] = imexport\n","\n","        else:\n","            globals()[f'{name}_imexport_{i}'] = imexport\n","\n","\n","    \n","    "],"metadata":{"id":"Am4fvgNjTToB","executionInfo":{"status":"ok","timestamp":1664370843504,"user_tz":-540,"elapsed":365,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def make_csv(주소,name = 'train'):\n","    imexport(주소,name)\n","    pummok(주소,name)\n","    add_dosomae(주소,name)\n","    add_dosomae(주소,name,2)\n","    weather(주소,name)\n","    for i in range(0,37):\n","        if name == 'test':\n","\n","            num = 주소.split('/')[-2][-1]\n","            \n","            \n","            temp = globals()[f'sep_{num}_{name}_pummok_{i}']\n","\n","            temp1 = globals()[f'sep_{num}_{name}_weather_{i}']\n","\n","            temp2 = globals()[f'sep_{num}_{name}_domae_{i}']\n","   \n","            temp3 = globals()[f'sep_{num}_{name}_somae_{i}']\n","\n","            temp4 = globals()[f'sep_{num}_{name}_imexport_{i}']\n","            temp.set_index('datadate',inplace=True)\n","            temp1.set_index('datadate',inplace=True)\n","            temp2.set_index('datadate',inplace=True)\n","            temp3.set_index('datadate',inplace=True)\n","\n","            temp = temp.join(temp1)\n","            temp = temp.join(temp2)\n","            temp = temp.join(temp3)\n","            temp.reset_index(inplace = True)\n","            temp['datadate'] = temp['datadate'].astype('str')\n","            temp['yyyymm'] =  temp['datadate'].apply(lambda x: x[:6] )\n","            temp4.reset_index(inplace=True)\n","            temp4['yyyymm'] = temp4['datadate'].astype('str')\n","            temp4.drop('datadate',inplace=True,axis=1)\n","            temp = temp.merge(temp4, how='left',on='yyyymm')\n","            temp.set_index('datadate',inplace = True)\n","            temp.drop('yyyymm',inplace=True,axis=1)\n","\n","\n","            temp.fillna(0, inplace = True) # 내가 추가한 코드\n","            globals()[f'{name}_total_sep_{num}_{i}'] = temp\n","\n","\n","        else:\n","            temp = globals()[f'{name}_pummok_{i}']\n","            temp1 = globals()[f'{name}_weather_{i}']\n","            temp2 = globals()[f'{name}_domae_{i}']\n","            temp3 = globals()[f'{name}_somae_{i}']\n","            temp4 = globals()[f'{name}_imexport_{i}']\n","            temp.set_index('datadate',inplace=True)\n","            temp1.set_index('datadate',inplace=True)\n","            temp2.set_index('datadate',inplace=True)\n","            temp3.set_index('datadate',inplace=True)\n","            temp = temp.join(temp1)\n","            temp = temp.join(temp2)\n","            temp = temp.join(temp3)\n","            temp.reset_index(inplace = True)\n","            temp['datadate'] = temp['datadate'].astype('str')\n","            temp['yyyymm'] =  temp['datadate'].apply(lambda x: x[:6] )\n","            temp4.reset_index(inplace=True)\n","            temp4['yyyymm'] = temp4['datadate'].astype('str')\n","            temp4.drop('datadate',inplace=True,axis=1)\n","            temp = temp.merge(temp4, how='left',on='yyyymm')\n","            temp.set_index('datadate',inplace = True)\n","            temp.drop('yyyymm',inplace=True,axis=1)\n","\n","            temp.fillna(0, inplace = True)\n","            globals()[f'{name}_total_{i}'] = temp\n","        \n","    "],"metadata":{"id":"MUYtVQO8TUzq","executionInfo":{"status":"ok","timestamp":1664370854058,"user_tz":-540,"elapsed":431,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# train_data만들기 train_total_0 ~ train_total_36까지 만들어준다.\n","make_csv('/content/drive/MyDrive/농산물예측/aT_data/aT_train_raw/','train')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1GWUMRwvTVs9","executionInfo":{"status":"ok","timestamp":1664370943059,"user_tz":-540,"elapsed":83967,"user":{"displayName":"이태영","userId":"15551978220746215296"}},"outputId":"29e8a94c-80be-48d0-c0bb-14716d588bca"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 37/37 [00:12<00:00,  3.01it/s]\n","100%|██████████| 37/37 [00:26<00:00,  1.38it/s]\n","100%|██████████| 37/37 [00:07<00:00,  4.99it/s]\n","100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n","100%|██████████| 37/37 [00:16<00:00,  2.26it/s]\n"]}]},{"cell_type":"code","source":["# 강사님이 만들어주신거 값 대체\n","tmp_7 = pd.read_csv('/content/drive/MyDrive/농산물예측/aT_data/7번결측채운거_완.csv')\n","globals()['train_total_7']['해당일자_전체평균가격(원)'] = tmp_7['pred'].values\n","globals()['train_total_7']['해당일자_전체평균가격(원)'].plot()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":297},"id":"XaWQXy9PTW70","executionInfo":{"status":"ok","timestamp":1664370970257,"user_tz":-540,"elapsed":1107,"user":{"displayName":"이태영","userId":"15551978220746215296"}},"outputId":"23917e56-b12f-4d4d-c034-a152af59c6d7"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7f0770974590>"]},"metadata":{},"execution_count":11},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["def drop_tuesday(df):\n","  dfcp = df.reset_index()\n","  dfcp['datadate'] = pd.to_datetime(dfcp['datadate'])\n","  dfcp['요일정보'] = dfcp['datadate'].dt.weekday\n","\n","  dfcp = dfcp[dfcp['요일정보']!=1]\n","  dfcp.drop('요일정보',axis=1, inplace=True)\n","  dfcp = dfcp.set_index('datadate')\n","\n","  return dfcp\n"],"metadata":{"id":"XU3B6IDoTaca","executionInfo":{"status":"ok","timestamp":1664370970664,"user_tz":-540,"elapsed":1,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# 화요일 drop하는거 train data에 적용하기\n","for i in range(37):\n","  globals()[f'train_total_drop{i}'] = drop_tuesday(globals()[f'train_total_{i}'])\n","  print(len(globals()[f'train_total_drop{i}']), globals()[f'train_total_drop{i}'].isna().sum().sum())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sia3qbhATb7o","executionInfo":{"status":"ok","timestamp":1664370972821,"user_tz":-540,"elapsed":456,"user":{"displayName":"이태영","userId":"15551978220746215296"}},"outputId":"267cd25f-5d38-4804-faa6-d63f57cdde37"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n"]}]},{"cell_type":"code","source":["def fill_target_wma(df):\n","  df['해당일자_전체평균가격(원)'].fillna(0, inplace=True)# 혹시 모르니 다시 0으로 채우기\n","  df['wma13'] = tb.WMA(df['해당일자_전체평균가격(원)'], 13)\n","  df['wma13'].fillna(method='bfill', inplace=True)\n","  df['해당일자_전체평균가격(원)'] = np.where(df['해당일자_전체평균가격(원)']==0, df['wma13'], df['해당일자_전체평균가격(원)'])\n","  df.drop('wma13', axis=1, inplace=True)\n","\n","  return df"],"metadata":{"id":"NMRxmf_OTdHA","executionInfo":{"status":"ok","timestamp":1664370974880,"user_tz":-540,"elapsed":2,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# train data 타겟값 wma13 가중치로 채워넣기~~\n","for i in range(37):\n","  globals()[f'train_fill_target_{i}'] = fill_target_wma(globals()[f'train_total_drop{i}'])\n","  print(len(globals()[f'train_fill_target_{i}']), globals()[f'train_fill_target_{i}'].isna().sum().sum())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aQonVYcSTdPB","executionInfo":{"status":"ok","timestamp":1664370976798,"user_tz":-540,"elapsed":2,"user":{"displayName":"이태영","userId":"15551978220746215296"}},"outputId":"a0efb8b7-ba3f-495a-e75e-b43c6faa11ee"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n"]}]},{"cell_type":"code","source":["# 0인 가격이 없다. (아마 거의 없을거야)\n","globals()['train_fill_target_7'][globals()['train_fill_target_7']['해당일자_전체평균가격(원)']<=0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":171},"id":"K6QsGPHYTdja","executionInfo":{"status":"ok","timestamp":1664370977892,"user_tz":-540,"elapsed":3,"user":{"displayName":"이태영","userId":"15551978220746215296"}},"outputId":"1eb82fb2-d4cd-45ce-9053-6aa665824e90"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Empty DataFrame\n","Columns: [단가(원), 거래량, 해당일자_전체평균가격(원), 초기온도(℃)_0, 최대온도(℃)_0, 최저온도(℃)_0, 평균온도(℃)_0, 강수량(ml)_0, 습도(%)_0, 초기온도(℃)_1, 최대온도(℃)_1, 최저온도(℃)_1, 평균온도(℃)_1, 강수량(ml)_1, 습도(%)_1, 일자별_domae가격_최대(원), 일자별_domae가격_평균(원), 일자별_domae가격_최소(원), 일자별_somae가격_최대(원), 일자별_somae가격_평균(원), 일자별_somae가격_최소(원), 수출중량(kg), 수출금액(달러), 수입중량(kg), 수입금액(달러), 무역수지(달러)]\n","Index: []\n","\n","[0 rows x 26 columns]"],"text/html":["\n","  <div id=\"df-a1d64ebd-ec6b-479f-81bc-49b206592f71\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>단가(원)</th>\n","      <th>거래량</th>\n","      <th>해당일자_전체평균가격(원)</th>\n","      <th>초기온도(℃)_0</th>\n","      <th>최대온도(℃)_0</th>\n","      <th>최저온도(℃)_0</th>\n","      <th>평균온도(℃)_0</th>\n","      <th>강수량(ml)_0</th>\n","      <th>습도(%)_0</th>\n","      <th>초기온도(℃)_1</th>\n","      <th>...</th>\n","      <th>일자별_domae가격_평균(원)</th>\n","      <th>일자별_domae가격_최소(원)</th>\n","      <th>일자별_somae가격_최대(원)</th>\n","      <th>일자별_somae가격_평균(원)</th>\n","      <th>일자별_somae가격_최소(원)</th>\n","      <th>수출중량(kg)</th>\n","      <th>수출금액(달러)</th>\n","      <th>수입중량(kg)</th>\n","      <th>수입금액(달러)</th>\n","      <th>무역수지(달러)</th>\n","    </tr>\n","    <tr>\n","      <th>datadate</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","<p>0 rows × 26 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a1d64ebd-ec6b-479f-81bc-49b206592f71')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-a1d64ebd-ec6b-479f-81bc-49b206592f71 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-a1d64ebd-ec6b-479f-81bc-49b206592f71');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["## OLS feature selection"],"metadata":{"id":"XdXo0TTBT7Bh"}},{"cell_type":"code","source":["import statsmodels.api as sm\n","\n","\n","# OLS Feature selection을 위한 함수\n","def OLS_report(df, seed):\n","    \n","    \n","    y=df['해당일자_전체평균가격']\n","    x=df.drop(['해당일자_전체평균가격'],axis=1)\n","\n","    model = sm.OLS(y, x)\n","    if len(df.columns)<len(globals()['train_fill_target_0'].columns): # 주산지2의 날씨 데이터가 없다. 예외처리\n","      model = sm.OLS(y, x)\n","    else:\n","      model = sm.OLS(y, x)\n","    return model.fit()\n","\n","\n","\n","\n","def results_summary_to_dataframe(results):\n","    '''take the result of an statsmodel results table and transforms it into a dataframe'''\n","    pvals = results.pvalues\n","    coeff = results.params\n","    # conf_lower = results.conf_int()[0]\n","    # conf_higher = results.conf_int()[1]\n","\n","    results_df = pd.DataFrame({\"pvals\":pvals,\n","                               \"coeff\":coeff\n","                              #  \"conf_lower\":conf_lower,\n","                              #  \"conf_higher\":conf_higher\n","                                })\n","\n","    #Reordering...\n","    results_df = results_df[[\"coeff\",\"pvals\"]]\n","    return results_df\n","\n","\n","\n","\n","\n","def OLS_feature_selection(df):\n","  df.columns = df.columns.str.replace(r'\\([^)]*\\)','',regex= True) # 컬럼에서 문자 기호 같은거 있으면 OLS 안돌아간다.     \n","                                                                  # 테스트 데이터에도 적용해야한다. 컬럼명 바꿔야하니깐.\n","  result = OLS_report(df,42)\n","  ols_report_df = results_summary_to_dataframe(result).reset_index()\n","  ols_report_df = ols_report_df[ols_report_df['index'] !='Intercept']\n","  selected_col = list(ols_report_df[ols_report_df['pvals']<0.05]['index'].values) \n","  selected_df = df[selected_col]\n","  selected_df['해당일자_전체평균가격'] = df['해당일자_전체평균가격']\n","  return selected_df        "],"metadata":{"id":"oRy5OBAWTmNJ","executionInfo":{"status":"ok","timestamp":1664371040791,"user_tz":-540,"elapsed":346,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# 유의수준을 토대로 columns selection\n","for i in range(37):\n","  globals()[f'total_{i}'] = OLS_feature_selection(globals()[f'train_fill_target_{i}'])\n","  \n","  \n","\n"],"metadata":{"id":"R-Fach7hTmPb","executionInfo":{"status":"ok","timestamp":1664371077889,"user_tz":-540,"elapsed":1538,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# 품목별로 컬럼 확인\n","for i in range(37):\n","  print(f\"{i} 품목의 selected_columns : {list(globals()[f'total_{i}'].columns)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JlvX9DMdTmRw","executionInfo":{"status":"ok","timestamp":1664371082426,"user_tz":-540,"elapsed":301,"user":{"displayName":"이태영","userId":"15551978220746215296"}},"outputId":"b7a9e731-c073-43c6-c619-3858eb79a4a1"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["0 품목의 selected_columns : ['단가', '거래량', '최저온도_1', '습도_1', '일자별_somae가격_최대', '일자별_somae가격_최소', '해당일자_전체평균가격']\n","1 품목의 selected_columns : ['단가', '거래량', '습도_0', '최대온도_1', '평균온도_1', '습도_1', '최저온도_2', '평균온도_2', '습도_2', '일자별_somae가격_최대', '일자별_somae가격_평균', '일자별_somae가격_최소', '해당일자_전체평균가격']\n","2 품목의 selected_columns : ['단가', '거래량', '최저온도_0', '강수량_0', '습도_0', '초기온도_1', '최대온도_1', '습도_2', '일자별_somae가격_평균', '일자별_somae가격_최소', '수출중량', '수출금액', '수입금액', '무역수지', '해당일자_전체평균가격']\n","3 품목의 selected_columns : ['단가', '거래량', '습도_0', '평균온도_1', '습도_1', '습도_2', '일자별_domae가격_평균', '일자별_somae가격_평균', '일자별_somae가격_최소', '수출중량', '수출금액', '수입금액', '무역수지', '해당일자_전체평균가격']\n","4 품목의 selected_columns : ['단가', '거래량', '최저온도_1', '평균온도_1', '습도_2', '일자별_somae가격_최대', '일자별_somae가격_최소', '수출중량', '해당일자_전체평균가격']\n","5 품목의 selected_columns : ['단가', '거래량', '최대온도_0', '평균온도_0', '강수량_0', '습도_0', '습도_1', '습도_2', '일자별_domae가격_평균', '일자별_domae가격_최소', '일자별_somae가격_최대', '일자별_somae가격_평균', '일자별_somae가격_최소', '수출중량', '수출금액', '수입중량', '수입금액', '무역수지', '해당일자_전체평균가격']\n","6 품목의 selected_columns : ['단가', '거래량', '습도_1', '초기온도_2', '최대온도_2', '무역수지', '해당일자_전체평균가격']\n","7 품목의 selected_columns : ['단가', '거래량', '강수량_0', '습도_1', '해당일자_전체평균가격']\n","8 품목의 selected_columns : ['단가', '거래량', '최대온도_1', '평균온도_1', '최저온도_2', '습도_2', '일자별_domae가격_평균', '일자별_domae가격_최소', '일자별_somae가격_최대', '일자별_somae가격_최소', '해당일자_전체평균가격']\n","9 품목의 selected_columns : ['단가', '거래량', '최대온도_0', '습도_0', '초기온도_2', '최저온도_2', '평균온도_2', '습도_2', '일자별_domae가격_최대', '일자별_domae가격_평균', '일자별_domae가격_최소', '일자별_somae가격_평균', '일자별_somae가격_최소', '수입중량', '해당일자_전체평균가격']\n","10 품목의 selected_columns : ['단가', '거래량', '최저온도_0', '습도_0', '최대온도_1', '초기온도_2', '최저온도_2', '강수량_2', '습도_2', '일자별_domae가격_최대', '일자별_domae가격_평균', '일자별_domae가격_최소', '일자별_somae가격_최대', '일자별_somae가격_평균', '해당일자_전체평균가격']\n","11 품목의 selected_columns : ['단가', '거래량', '최저온도_0', '습도_1', '평균온도_2', '일자별_somae가격_최대', '일자별_somae가격_평균', '수출중량', '해당일자_전체평균가격']\n","12 품목의 selected_columns : ['단가', '거래량', '습도_0', '습도_1', '초기온도_2', '최저온도_2', '평균온도_2', '강수량_2', '일자별_domae가격_최소', '수출중량', '수출금액', '수입금액', '무역수지', '해당일자_전체평균가격']\n","13 품목의 selected_columns : ['단가', '거래량', '최대온도_0', '최저온도_0', '강수량_0', '습도_0', '초기온도_2', '최저온도_2', '습도_2', '일자별_domae가격_최대', '일자별_domae가격_평균', '일자별_domae가격_최소', '일자별_somae가격_최대', '일자별_somae가격_평균', '수출중량', '수출금액', '수입중량', '수입금액', '무역수지', '해당일자_전체평균가격']\n","14 품목의 selected_columns : ['단가', '거래량', '평균온도_0', '습도_0', '평균온도_1', '습도_1', '강수량_2', '습도_2', '일자별_somae가격_최대', '해당일자_전체평균가격']\n","15 품목의 selected_columns : ['단가', '거래량', '최저온도_2', '습도_2', '일자별_somae가격_최대', '수출중량', '수출금액', '수입금액', '무역수지', '해당일자_전체평균가격']\n","16 품목의 selected_columns : ['단가', '거래량', '습도_1', '습도_2', '해당일자_전체평균가격']\n","17 품목의 selected_columns : ['단가', '거래량', '최대온도_0', '습도_0', '습도_1', '습도_2', '일자별_domae가격_평균', '일자별_somae가격_평균', '수출중량', '수출금액', '무역수지', '해당일자_전체평균가격']\n","18 품목의 selected_columns : ['단가', '거래량', '평균온도_2', '습도_2', '수출중량', '해당일자_전체평균가격']\n","19 품목의 selected_columns : ['단가', '거래량', '초기온도_0', '최대온도_0', '습도_0', '습도_2', '일자별_domae가격_최대', '일자별_domae가격_최소', '일자별_somae가격_최대', '해당일자_전체평균가격']\n","20 품목의 selected_columns : ['단가', '거래량', '최대온도_0', '평균온도_0', '습도_1', '습도_2', '일자별_domae가격_최소', '일자별_somae가격_평균', '일자별_somae가격_최소', '수출중량', '수출금액', '수입금액', '무역수지', '해당일자_전체평균가격']\n","21 품목의 selected_columns : ['단가', '습도_0', '최저온도_2', '강수량_2', '습도_2', '일자별_domae가격_평균', '일자별_domae가격_최소', '일자별_somae가격_최대', '일자별_somae가격_평균', '수출중량', '수출금액', '수입금액', '무역수지', '해당일자_전체평균가격']\n","22 품목의 selected_columns : ['단가', '거래량', '평균온도_0', '초기온도_1', '강수량_1', '최저온도_2', '평균온도_2', '습도_2', '일자별_domae가격_최대', '일자별_domae가격_최소', '일자별_somae가격_평균', '일자별_somae가격_최소', '수출중량', '수출금액', '수입중량', '수입금액', '해당일자_전체평균가격']\n","23 품목의 selected_columns : ['단가', '거래량', '초기온도_2', '최대온도_2', '습도_2', '일자별_domae가격_최대', '일자별_domae가격_평균', '일자별_domae가격_최소', '일자별_somae가격_평균', '해당일자_전체평균가격']\n","24 품목의 selected_columns : ['단가', '거래량', '최대온도_0', '최저온도_1', '습도_1', '최대온도_2', '일자별_somae가격_최대', '일자별_somae가격_평균', '일자별_somae가격_최소', '해당일자_전체평균가격']\n","25 품목의 selected_columns : ['단가', '거래량', '초기온도_0', '습도_0', '일자별_domae가격_최대', '수입중량', '무역수지', '해당일자_전체평균가격']\n","26 품목의 selected_columns : ['단가', '거래량', '최대온도_0', '습도_2', '일자별_domae가격_최대', '일자별_domae가격_평균', '일자별_domae가격_최소', '일자별_somae가격_평균', '일자별_somae가격_최소', '해당일자_전체평균가격']\n","27 품목의 selected_columns : ['단가', '거래량', '초기온도_0', '최대온도_0', '최저온도_0', '습도_0', '습도_2', '일자별_somae가격_평균', '일자별_somae가격_최소', '해당일자_전체평균가격']\n","28 품목의 selected_columns : ['단가', '거래량', '습도_0', '최대온도_1', '평균온도_1', '습도_1', '최저온도_2', '일자별_somae가격_최대', '일자별_somae가격_평균', '일자별_somae가격_최소', '수출중량', '수출금액', '수입금액', '해당일자_전체평균가격']\n","29 품목의 selected_columns : ['단가', '최저온도_0', '초기온도_1', '습도_1', '일자별_domae가격_최소', '일자별_somae가격_평균', '수출중량', '수출금액', '수입중량', '수입금액', '무역수지', '해당일자_전체평균가격']\n","30 품목의 selected_columns : ['단가', '거래량', '평균온도_0', '최대온도_1', '습도_1', '최대온도_2', '최저온도_2', '일자별_domae가격_최대', '일자별_domae가격_평균', '일자별_domae가격_최소', '일자별_somae가격_최대', '일자별_somae가격_평균', '일자별_somae가격_최소', '수입금액', '해당일자_전체평균가격']\n","31 품목의 selected_columns : ['단가', '거래량', '평균온도_0', '습도_1', '최저온도_2', '평균온도_2', '습도_2', '일자별_domae가격_최대', '일자별_domae가격_최소', '일자별_somae가격_평균', '해당일자_전체평균가격']\n","32 품목의 selected_columns : ['단가', '거래량', '최대온도_1', '최저온도_1', '습도_1', '습도_2', '일자별_domae가격_최대', '수출중량', '수입중량', '무역수지', '해당일자_전체평균가격']\n","33 품목의 selected_columns : ['단가', '거래량', '최저온도_0', '습도_0', '습도_1', '일자별_domae가격_최소', '일자별_somae가격_최대', '일자별_somae가격_평균', '일자별_somae가격_최소', '수출중량', '해당일자_전체평균가격']\n","34 품목의 selected_columns : ['단가', '거래량', '습도_0', '초기온도_1', '습도_1', '일자별_somae가격_평균', '수출중량', '해당일자_전체평균가격']\n","35 품목의 selected_columns : ['단가', '거래량', '최저온도_0', '습도_0', '초기온도_1', '최저온도_1', '일자별_somae가격_최소', '해당일자_전체평균가격']\n","36 품목의 selected_columns : ['단가', '거래량', '초기온도_0', '최대온도_0', '최저온도_0', '습도_0', '일자별_domae가격_평균', '일자별_domae가격_최소', '일자별_somae가격_최대', '일자별_somae가격_평균', '해당일자_전체평균가격']\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"O7iu4NUlYbU7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Scaling"],"metadata":{"id":"hstGT1WiYcpq"}},{"cell_type":"code","source":["def scaling_df(df):\n","  scaler = StandardScaler()\n","\n","  tmp = df.drop('해당일자_전체평균가격', axis=1).copy()\n","  scaler.fit(tmp)\n","  df_scaled = scaler.transform(tmp)\n","  df_scaled = pd.DataFrame(data=df_scaled, columns = tmp.columns)\n","  \n","\n","  df_scaled['log_target'] = np.log1p(df['해당일자_전체평균가격']).values \n","  return df_scaled"],"metadata":{"id":"rP0gBDXsTdl7","executionInfo":{"status":"ok","timestamp":1664371090686,"user_tz":-540,"elapsed":870,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# train 데이터 스케일링\n","for i in range(37):\n","  globals()[f'train_scaled_{i}'] = scaling_df(globals()[f'train_fill_target_{i}'])\n","  print(len(globals()[f'train_scaled_{i}']), globals()[f'train_scaled_{i}'].isna().sum().sum())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6fhJlBNuTdoT","executionInfo":{"status":"ok","timestamp":1664371092798,"user_tz":-540,"elapsed":1,"user":{"displayName":"이태영","userId":"15551978220746215296"}},"outputId":"061d6552-eaa2-4421-9c2b-3a77eaacfee5"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n","1252 0\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"IrhCEW87ZGbq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Li-XPxDSZGeK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"mnLk956iZGgp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 모델링\n"],"metadata":{"id":"JQdodmUuY_NL"}},{"cell_type":"code","source":["try_cnt = 13"],"metadata":{"id":"PdvVtV9dZL_C","executionInfo":{"status":"ok","timestamp":1664371097499,"user_tz":-540,"elapsed":359,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["def make_Tensor(array):\n","    return tf.convert_to_tensor(array, dtype=tf.float32)\n","\n","def astype_data(data):\n","    df = data.astype(np.float32)\n","    return make_Tensor(df)"],"metadata":{"id":"uPotf0i-ZMBZ","executionInfo":{"status":"ok","timestamp":1664371097908,"user_tz":-540,"elapsed":1,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n","\n","    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n","    x = layers.MultiHeadAttention(\n","        key_dim=head_size, num_heads=num_heads, dropout=dropout\n","    )(x, x)\n","    x = layers.Dropout(dropout)(x)\n","    res = x + inputs\n","\n","    x = layers.LayerNormalization(epsilon=1e-6)(res)\n","    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n","    x = layers.Dropout(dropout)(x)\n","    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n","    return x + res"],"metadata":{"id":"ayLPNVBdZMDi","executionInfo":{"status":"ok","timestamp":1664371099754,"user_tz":-540,"elapsed":433,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["def build_model(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout=0, mlp_dropout=0):\n","    inputs = keras.Input(shape=input_shape)\n","    x = inputs\n","    for _ in range(num_transformer_blocks):\n","        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n","\n","    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n","    for dim in mlp_units:\n","        x = layers.Dense(dim, activation=\"relu\")(x)\n","        x = layers.Dropout(mlp_dropout)(x)\n","    outputs = layers.Dense(28)(x) # 4주 예측\n","    return keras.Model(inputs, outputs)"],"metadata":{"id":"EfTcv-HoZMFz","executionInfo":{"status":"ok","timestamp":1664371101800,"user_tz":-540,"elapsed":2,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["def call_back_set(name, epoch, batch_size):\n","    early_stopping = EarlyStopping(monitor='val_loss', patience=100)\n","\n","    if os.path.exists(f'/content/drive/MyDrive/농산물예측/aT_data/check{try_cnt}') == False:\n","        os.mkdir(f'/content/drive/MyDrive/농산물예측/aT_data/check{try_cnt}')\n","\n","    filename = f'/content/drive/MyDrive/농산물예측/aT_data/check{try_cnt}/{name}-{epoch}-{batch_size}.h5'\n","\n","    checkpoint = ModelCheckpoint(filename,\n","                                 monitor='val_loss',\n","                                 verbose=1,\n","                                 save_best_only=True,\n","                                 save_weights_only=True,\n","                                 mode='auto'\n","                                 )\n","    return [early_stopping, checkpoint]"],"metadata":{"id":"UAJqqZDGZQdJ","executionInfo":{"status":"ok","timestamp":1664371104030,"user_tz":-540,"elapsed":1,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["def train(x_train, y_train, x_val, y_val, name, epoch, batch_size, learning_rate = 0.001, verbose = 1):\n","\n","\n","    model = build_model(\n","    x_train.shape[1:],\n","    head_size=256,\n","    num_heads=4,\n","    ff_dim=4,\n","    num_transformer_blocks=4,\n","    mlp_units=[128],\n","    mlp_dropout=0.4,\n","    dropout=0.25,\n","    )\n","\n","    model.compile(\n","        loss=\"mean_squared_error\",\n","        optimizer=keras.optimizers.Adam(learning_rate=learning_rate)\n","    )\n","\n","\n","    # Train the model\n","    with tf.device('/device:GPU:0'):\n","        history1 = model.fit(\n","            x_train, y_train,\n","            epochs = epoch,\n","            steps_per_epoch=len(x_train) / batch_size,\n","            batch_size=batch_size,\n","            validation_data=(x_val, y_val),\n","            validation_steps=len(x_val) / batch_size,\n","            shuffle=False,\n","            callbacks=call_back_set(name, epoch, batch_size),\n","            verbose=verbose)\n","\n","    return model"],"metadata":{"id":"IvgtlBlxZTF7","executionInfo":{"status":"ok","timestamp":1664371108178,"user_tz":-540,"elapsed":315,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["def time_window(df, t, t_sep):\n","    seq_len = t\n","    seqence_length = seq_len + t_sep\n","\n","    result = []\n","    for index in tqdm(range(len(df) - seqence_length)):\n","        result.append(df[index: index + seqence_length].values)\n","\n","    return np.array(result)"],"metadata":{"id":"CsvMmRxWZTIk","executionInfo":{"status":"ok","timestamp":1664371111829,"user_tz":-540,"elapsed":1,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["epoch = 1000\n","batch = 15"],"metadata":{"id":"PxxjytLvZTLB","executionInfo":{"status":"ok","timestamp":1664371114784,"user_tz":-540,"elapsed":328,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["for i in range(37):\n","\n","\n","    df_number = i\n","    df = globals()[f'train_scaled_{i}']\n","    \n","    # nan 처리\n","    df.fillna(0, inplace = True)\n","    \n","  \n","\n","    \n","\n","    # 변수와 타겟 분리\n","    x, y = df[[i for i in df.columns if i != 'log_target']], df['log_target']\n","\n","    # 2주 입력을 통한 이후 4주 예측을 위해 y의 첫 14일을 제외\n","    y = y[14:]\n","\n","    # time series window 생성\n","    data_x = time_window(x, 13, 1)\n","    data_y = time_window(y, 27, 1)\n","\n","    # y의 길이와 같은 길이로 설정\n","    xdata = data_x[:len(data_y)]\n","    ydata = data_y\n","\n","    # train, validation 분리 (9 : 1)\n","    x_train, x_val, y_train, y_val = train_test_split(xdata, ydata, test_size=0.1, shuffle=False, random_state=119)\n","\n","    # transformer 모델 훈련\n","    transformer = train(astype_data(x_train), y_train, astype_data(x_val), y_val, f'transformer-{df_number}', epoch, batch)\n","    transformer.load_weights(f'/content/drive/MyDrive/농산물예측/aT_data/check{try_cnt}/transformer-{df_number}-{epoch}-{batch}.h5')\n","\n","    if os.path.exists(f'/content/drive/MyDrive/농산물예측/aT_data/model{try_cnt}') == False:\n","         os.mkdir(f'/content/drive/MyDrive/농산물예측/aT_data/model{try_cnt}')\n","\n","    \n","    \n","    \n","    # 모델 저장\n","    transformer.save(f'/content/drive/MyDrive/농산물예측/aT_data/model{try_cnt}/transformer-{df_number}-{epoch}-{batch}.h5')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"muRF4cNmZTNv","outputId":"b561444d-1d1e-4157-e244-1571a9e3bb51","executionInfo":{"status":"ok","timestamp":1664372041643,"user_tz":-540,"elapsed":922796,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":32,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1238/1238 [00:00<00:00, 33607.42it/s]\n","100%|██████████| 1210/1210 [00:00<00:00, 31758.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/1000\n","71/72 [============================>.] - ETA: 0s - loss: 23.4988\n","Epoch 1: val_loss improved from inf to 6.21508, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-32-1000-15.h5\n","72/72 [==============================] - 15s 21ms/step - loss: 23.4739 - val_loss: 6.2151\n","Epoch 2/1000\n","71/72 [============================>.] - ETA: 0s - loss: 8.6462\n","Epoch 2: val_loss improved from 6.21508 to 2.46115, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-32-1000-15.h5\n","72/72 [==============================] - 1s 12ms/step - loss: 8.5481 - val_loss: 2.4612\n","Epoch 3/1000\n","71/72 [============================>.] - ETA: 0s - loss: 9.7494\n","Epoch 3: val_loss did not improve from 2.46115\n","72/72 [==============================] - 1s 11ms/step - loss: 9.6249 - val_loss: 3.3079\n","Epoch 4/1000\n","71/72 [============================>.] - ETA: 0s - loss: 7.8058\n","Epoch 4: val_loss improved from 2.46115 to 1.71199, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-32-1000-15.h5\n","72/72 [==============================] - 1s 12ms/step - loss: 7.7198 - val_loss: 1.7120\n","Epoch 5/1000\n","71/72 [============================>.] - ETA: 0s - loss: 3.2698\n","Epoch 5: val_loss improved from 1.71199 to 1.11618, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-32-1000-15.h5\n","72/72 [==============================] - 1s 12ms/step - loss: 3.2667 - val_loss: 1.1162\n","Epoch 6/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.6647\n","Epoch 6: val_loss improved from 1.11618 to 0.57568, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-32-1000-15.h5\n","72/72 [==============================] - 1s 12ms/step - loss: 2.6470 - val_loss: 0.5757\n","Epoch 7/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.1449\n","Epoch 7: val_loss did not improve from 0.57568\n","72/72 [==============================] - 1s 11ms/step - loss: 2.1365 - val_loss: 0.7861\n","Epoch 8/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.1385\n","Epoch 8: val_loss did not improve from 0.57568\n","72/72 [==============================] - 1s 11ms/step - loss: 2.1452 - val_loss: 0.7110\n","Epoch 9/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.2734\n","Epoch 9: val_loss did not improve from 0.57568\n","72/72 [==============================] - 1s 11ms/step - loss: 2.3945 - val_loss: 2.0599\n","Epoch 10/1000\n","71/72 [============================>.] - ETA: 0s - loss: 3.7246\n","Epoch 10: val_loss did not improve from 0.57568\n","72/72 [==============================] - 1s 11ms/step - loss: 3.7982 - val_loss: 2.7324\n","Epoch 11/1000\n","73/72 [==============================] - ETA: 0s - loss: 2.7750\n","Epoch 11: val_loss did not improve from 0.57568\n","72/72 [==============================] - 1s 11ms/step - loss: 2.7750 - val_loss: 1.9590\n","Epoch 12/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.0204\n","Epoch 12: val_loss did not improve from 0.57568\n","72/72 [==============================] - 1s 11ms/step - loss: 2.0107 - val_loss: 1.3864\n","Epoch 13/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 1.6407\n","Epoch 13: val_loss did not improve from 0.57568\n","72/72 [==============================] - 1s 11ms/step - loss: 1.6371 - val_loss: 1.3011\n","Epoch 14/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.5062\n","Epoch 14: val_loss did not improve from 0.57568\n","72/72 [==============================] - 1s 11ms/step - loss: 1.4933 - val_loss: 1.2828\n","Epoch 15/1000\n","72/72 [============================>.] - ETA: 0s - loss: 1.4121\n","Epoch 15: val_loss did not improve from 0.57568\n","72/72 [==============================] - 1s 11ms/step - loss: 1.4123 - val_loss: 1.9303\n","Epoch 16/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.6665\n","Epoch 16: val_loss did not improve from 0.57568\n","72/72 [==============================] - 1s 11ms/step - loss: 1.6613 - val_loss: 3.9734\n","Epoch 17/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.9158\n","Epoch 17: val_loss did not improve from 0.57568\n","72/72 [==============================] - 1s 11ms/step - loss: 1.9146 - val_loss: 0.9318\n","Epoch 18/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.8079\n","Epoch 18: val_loss did not improve from 0.57568\n","72/72 [==============================] - 1s 11ms/step - loss: 1.7953 - val_loss: 1.8200\n","Epoch 19/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.6548\n","Epoch 19: val_loss did not improve from 0.57568\n","72/72 [==============================] - 1s 11ms/step - loss: 1.6380 - val_loss: 1.6014\n","Epoch 20/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.2199\n","Epoch 20: val_loss did not improve from 0.57568\n","72/72 [==============================] - 1s 11ms/step - loss: 1.2433 - val_loss: 2.1767\n","Epoch 21/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.2767\n","Epoch 21: val_loss did not improve from 0.57568\n","72/72 [==============================] - 1s 11ms/step - loss: 2.2527 - val_loss: 5.2054\n","Epoch 22/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.9008\n","Epoch 22: val_loss improved from 0.57568 to 0.31543, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-32-1000-15.h5\n","72/72 [==============================] - 1s 16ms/step - loss: 2.8652 - val_loss: 0.3154\n","Epoch 23/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.8030\n","Epoch 23: val_loss did not improve from 0.31543\n","72/72 [==============================] - 1s 11ms/step - loss: 1.7834 - val_loss: 0.4574\n","Epoch 24/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.9430\n","Epoch 24: val_loss did not improve from 0.31543\n","72/72 [==============================] - 1s 11ms/step - loss: 0.9456 - val_loss: 0.6941\n","Epoch 25/1000\n","72/72 [============================>.] - ETA: 0s - loss: 1.2032\n","Epoch 25: val_loss did not improve from 0.31543\n","72/72 [==============================] - 1s 11ms/step - loss: 1.2001 - val_loss: 0.4932\n","Epoch 26/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.0242\n","Epoch 26: val_loss did not improve from 0.31543\n","72/72 [==============================] - 1s 11ms/step - loss: 1.0477 - val_loss: 1.7261\n","Epoch 27/1000\n","73/72 [==============================] - ETA: 0s - loss: 1.7565\n","Epoch 27: val_loss did not improve from 0.31543\n","72/72 [==============================] - 1s 11ms/step - loss: 1.7565 - val_loss: 2.1466\n","Epoch 28/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.7057\n","Epoch 28: val_loss did not improve from 0.31543\n","72/72 [==============================] - 1s 11ms/step - loss: 2.8089 - val_loss: 1.5694\n","Epoch 29/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.9129\n","Epoch 29: val_loss did not improve from 0.31543\n","72/72 [==============================] - 1s 11ms/step - loss: 1.8941 - val_loss: 2.4004\n","Epoch 30/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.8396\n","Epoch 30: val_loss did not improve from 0.31543\n","72/72 [==============================] - 1s 11ms/step - loss: 1.9373 - val_loss: 4.6094\n","Epoch 31/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.7549\n","Epoch 31: val_loss improved from 0.31543 to 0.12223, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-32-1000-15.h5\n","72/72 [==============================] - 1s 17ms/step - loss: 1.7422 - val_loss: 0.1222\n","Epoch 32/1000\n","71/72 [============================>.] - ETA: 0s - loss: 3.1235\n","Epoch 32: val_loss did not improve from 0.12223\n","72/72 [==============================] - 1s 11ms/step - loss: 3.1022 - val_loss: 0.7053\n","Epoch 33/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.1920\n","Epoch 33: val_loss did not improve from 0.12223\n","72/72 [==============================] - 1s 11ms/step - loss: 1.1885 - val_loss: 1.0452\n","Epoch 34/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.0975\n","Epoch 34: val_loss did not improve from 0.12223\n","72/72 [==============================] - 1s 11ms/step - loss: 1.1404 - val_loss: 0.8120\n","Epoch 35/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.1367\n","Epoch 35: val_loss did not improve from 0.12223\n","72/72 [==============================] - 1s 11ms/step - loss: 1.1273 - val_loss: 0.9563\n","Epoch 36/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.1342\n","Epoch 36: val_loss did not improve from 0.12223\n","72/72 [==============================] - 1s 11ms/step - loss: 1.1348 - val_loss: 0.8710\n","Epoch 37/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.1557\n","Epoch 37: val_loss did not improve from 0.12223\n","72/72 [==============================] - 1s 11ms/step - loss: 1.1444 - val_loss: 2.2751\n","Epoch 38/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.4689\n","Epoch 38: val_loss did not improve from 0.12223\n","72/72 [==============================] - 1s 11ms/step - loss: 1.4872 - val_loss: 0.6004\n","Epoch 39/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.4806\n","Epoch 39: val_loss did not improve from 0.12223\n","72/72 [==============================] - 1s 11ms/step - loss: 1.4696 - val_loss: 2.5515\n","Epoch 40/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.4377\n","Epoch 40: val_loss did not improve from 0.12223\n","72/72 [==============================] - 1s 11ms/step - loss: 2.5480 - val_loss: 3.1508\n","Epoch 41/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.1613\n","Epoch 41: val_loss did not improve from 0.12223\n","72/72 [==============================] - 1s 11ms/step - loss: 2.1375 - val_loss: 2.9340\n","Epoch 42/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.7758\n","Epoch 42: val_loss did not improve from 0.12223\n","72/72 [==============================] - 1s 11ms/step - loss: 1.7698 - val_loss: 1.6341\n","Epoch 43/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.1127\n","Epoch 43: val_loss did not improve from 0.12223\n","72/72 [==============================] - 1s 11ms/step - loss: 2.1031 - val_loss: 1.7216\n","Epoch 44/1000\n","72/72 [============================>.] - ETA: 0s - loss: 1.6241\n","Epoch 44: val_loss did not improve from 0.12223\n","72/72 [==============================] - 1s 11ms/step - loss: 1.6204 - val_loss: 0.2613\n","Epoch 45/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.5447\n","Epoch 45: val_loss improved from 0.12223 to 0.11876, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-32-1000-15.h5\n","72/72 [==============================] - 1s 16ms/step - loss: 1.5267 - val_loss: 0.1188\n","Epoch 46/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.5073\n","Epoch 46: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 2.5002 - val_loss: 0.5609\n","Epoch 47/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.5106\n","Epoch 47: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 1.6302 - val_loss: 5.1564\n","Epoch 48/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.7370\n","Epoch 48: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 1.7245 - val_loss: 0.6391\n","Epoch 49/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.1907\n","Epoch 49: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 1.1944 - val_loss: 0.6166\n","Epoch 50/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8218\n","Epoch 50: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.8330 - val_loss: 2.5094\n","Epoch 51/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.9462\n","Epoch 51: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.9343 - val_loss: 0.5091\n","Epoch 52/1000\n","72/72 [============================>.] - ETA: 0s - loss: 0.7648\n","Epoch 52: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.7648 - val_loss: 0.7326\n","Epoch 53/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8504\n","Epoch 53: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.8463 - val_loss: 0.7969\n","Epoch 54/1000\n","72/72 [============================>.] - ETA: 0s - loss: 0.9886\n","Epoch 54: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.9836 - val_loss: 0.2716\n","Epoch 55/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.9076\n","Epoch 55: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.9074 - val_loss: 1.2666\n","Epoch 56/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.7805\n","Epoch 56: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.7826 - val_loss: 1.2410\n","Epoch 57/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.9244\n","Epoch 57: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.9211 - val_loss: 1.0946\n","Epoch 58/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8244\n","Epoch 58: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.8230 - val_loss: 3.2438\n","Epoch 59/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.0024\n","Epoch 59: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.9869 - val_loss: 6.3543\n","Epoch 60/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.6611\n","Epoch 60: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 1.6407 - val_loss: 2.5480\n","Epoch 61/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.2731\n","Epoch 61: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 1.2672 - val_loss: 4.3391\n","Epoch 62/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.3017\n","Epoch 62: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 1.2934 - val_loss: 2.7791\n","Epoch 63/1000\n","72/72 [============================>.] - ETA: 0s - loss: 1.0774\n","Epoch 63: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 1.0722 - val_loss: 6.9854\n","Epoch 64/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.5417\n","Epoch 64: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 1.5181 - val_loss: 5.8431\n","Epoch 65/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.5133\n","Epoch 65: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 1.4960 - val_loss: 7.9638\n","Epoch 66/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.2851\n","Epoch 66: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 1.2814 - val_loss: 8.9291\n","Epoch 67/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.3013\n","Epoch 67: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 2.2722 - val_loss: 9.5504\n","Epoch 68/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.0840\n","Epoch 68: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 12ms/step - loss: 2.1326 - val_loss: 11.2273\n","Epoch 69/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.7038\n","Epoch 69: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 2.6910 - val_loss: 15.5929\n","Epoch 70/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.7715\n","Epoch 70: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 2.7314 - val_loss: 14.0281\n","Epoch 71/1000\n","71/72 [============================>.] - ETA: 0s - loss: 5.9518\n","Epoch 71: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 6.2660 - val_loss: 69.7664\n","Epoch 72/1000\n","71/72 [============================>.] - ETA: 0s - loss: 11.5166\n","Epoch 72: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 11.2916 - val_loss: 9.1967\n","Epoch 73/1000\n","71/72 [============================>.] - ETA: 0s - loss: 4.2660\n","Epoch 73: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 4.2111 - val_loss: 9.3427\n","Epoch 74/1000\n","71/72 [============================>.] - ETA: 0s - loss: 4.0818\n","Epoch 74: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 4.0106 - val_loss: 10.2582\n","Epoch 75/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.1246\n","Epoch 75: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 2.1068 - val_loss: 10.1784\n","Epoch 76/1000\n","71/72 [============================>.] - ETA: 0s - loss: 5.2507\n","Epoch 76: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 5.1563 - val_loss: 6.8552\n","Epoch 77/1000\n","71/72 [============================>.] - ETA: 0s - loss: 5.9654\n","Epoch 77: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 6.0157 - val_loss: 13.4402\n","Epoch 78/1000\n","71/72 [============================>.] - ETA: 0s - loss: 5.2611\n","Epoch 78: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 5.2305 - val_loss: 12.9529\n","Epoch 79/1000\n","71/72 [============================>.] - ETA: 0s - loss: 4.7443\n","Epoch 79: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 4.8832 - val_loss: 15.4662\n","Epoch 80/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 2.1403\n","Epoch 80: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 12ms/step - loss: 2.2131 - val_loss: 6.0356\n","Epoch 81/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.9775\n","Epoch 81: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 1.9569 - val_loss: 0.7733\n","Epoch 82/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.9249\n","Epoch 82: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 1.9235 - val_loss: 14.7811\n","Epoch 83/1000\n","72/72 [============================>.] - ETA: 0s - loss: 2.6945\n","Epoch 83: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 2.6795 - val_loss: 1.2996\n","Epoch 84/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.6317\n","Epoch 84: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 1.6163 - val_loss: 0.7735\n","Epoch 85/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8988\n","Epoch 85: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.9017 - val_loss: 0.5939\n","Epoch 86/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8253\n","Epoch 86: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.8185 - val_loss: 0.3397\n","Epoch 87/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8282\n","Epoch 87: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.8256 - val_loss: 0.6201\n","Epoch 88/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.7689\n","Epoch 88: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.7639 - val_loss: 0.2665\n","Epoch 89/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.6061\n","Epoch 89: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.6120 - val_loss: 0.8337\n","Epoch 90/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5967\n","Epoch 90: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.6039 - val_loss: 1.0296\n","Epoch 91/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5653\n","Epoch 91: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5757 - val_loss: 2.2649\n","Epoch 92/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.6545\n","Epoch 92: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.6567 - val_loss: 2.2753\n","Epoch 93/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.6511\n","Epoch 93: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.6651 - val_loss: 3.2945\n","Epoch 94/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8311\n","Epoch 94: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.8618 - val_loss: 4.3082\n","Epoch 95/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8209\n","Epoch 95: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.8560 - val_loss: 5.1482\n","Epoch 96/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.7324\n","Epoch 96: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.7365 - val_loss: 2.8547\n","Epoch 97/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5685\n","Epoch 97: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5831 - val_loss: 1.1208\n","Epoch 98/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5397\n","Epoch 98: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5440 - val_loss: 0.5435\n","Epoch 99/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5794\n","Epoch 99: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5790 - val_loss: 0.2075\n","Epoch 100/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5853\n","Epoch 100: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5998 - val_loss: 0.7720\n","Epoch 101/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5307\n","Epoch 101: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5310 - val_loss: 0.8890\n","Epoch 102/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4879\n","Epoch 102: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4935 - val_loss: 1.3567\n","Epoch 103/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4499\n","Epoch 103: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4609 - val_loss: 2.0930\n","Epoch 104/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5693\n","Epoch 104: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5799 - val_loss: 1.9546\n","Epoch 105/1000\n","69/72 [===========================>..] - ETA: 0s - loss: 0.5567\n","Epoch 105: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 12ms/step - loss: 0.5549 - val_loss: 1.9582\n","Epoch 106/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5955\n","Epoch 106: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 12ms/step - loss: 0.5952 - val_loss: 0.9777\n","Epoch 107/1000\n","72/72 [============================>.] - ETA: 0s - loss: 0.4745\n","Epoch 107: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4735 - val_loss: 0.7837\n","Epoch 108/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4496\n","Epoch 108: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4520 - val_loss: 0.4082\n","Epoch 109/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4515\n","Epoch 109: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4560 - val_loss: 0.4501\n","Epoch 110/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4310\n","Epoch 110: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4314 - val_loss: 0.6851\n","Epoch 111/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4471\n","Epoch 111: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4440 - val_loss: 0.5643\n","Epoch 112/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3911\n","Epoch 112: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3944 - val_loss: 0.5665\n","Epoch 113/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3925\n","Epoch 113: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3967 - val_loss: 0.6262\n","Epoch 114/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4219\n","Epoch 114: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4293 - val_loss: 0.5318\n","Epoch 115/1000\n","72/72 [============================>.] - ETA: 0s - loss: 0.4041\n","Epoch 115: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4043 - val_loss: 0.5682\n","Epoch 116/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4077\n","Epoch 116: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4114 - val_loss: 0.4488\n","Epoch 117/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4519\n","Epoch 117: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4494 - val_loss: 0.6236\n","Epoch 118/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3861\n","Epoch 118: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3984 - val_loss: 0.6310\n","Epoch 119/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3548\n","Epoch 119: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3580 - val_loss: 0.6718\n","Epoch 120/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4237\n","Epoch 120: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4229 - val_loss: 0.5416\n","Epoch 121/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3747\n","Epoch 121: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3829 - val_loss: 0.6003\n","Epoch 122/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3834\n","Epoch 122: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3839 - val_loss: 0.7046\n","Epoch 123/1000\n","72/72 [============================>.] - ETA: 0s - loss: 0.3860\n","Epoch 123: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3896 - val_loss: 0.4937\n","Epoch 124/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3496\n","Epoch 124: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3532 - val_loss: 0.4445\n","Epoch 125/1000\n","72/72 [============================>.] - ETA: 0s - loss: 0.3034\n","Epoch 125: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3045 - val_loss: 0.6713\n","Epoch 126/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3600\n","Epoch 126: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3649 - val_loss: 0.6176\n","Epoch 127/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3483\n","Epoch 127: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3512 - val_loss: 0.6101\n","Epoch 128/1000\n","72/72 [============================>.] - ETA: 0s - loss: 0.3347\n","Epoch 128: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3350 - val_loss: 0.3936\n","Epoch 129/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3908\n","Epoch 129: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3931 - val_loss: 0.4823\n","Epoch 130/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3182\n","Epoch 130: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3218 - val_loss: 0.6137\n","Epoch 131/1000\n","72/72 [============================>.] - ETA: 0s - loss: 0.3311\n","Epoch 131: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3342 - val_loss: 0.7068\n","Epoch 132/1000\n","72/72 [============================>.] - ETA: 0s - loss: 0.3226\n","Epoch 132: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3224 - val_loss: 0.5799\n","Epoch 133/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3325\n","Epoch 133: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3353 - val_loss: 0.6012\n","Epoch 134/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3258\n","Epoch 134: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3296 - val_loss: 0.6799\n","Epoch 135/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 0.3357\n","Epoch 135: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3497 - val_loss: 0.9115\n","Epoch 136/1000\n","72/72 [============================>.] - ETA: 0s - loss: 0.3860\n","Epoch 136: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3877 - val_loss: 0.4699\n","Epoch 137/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3759\n","Epoch 137: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3763 - val_loss: 0.8334\n","Epoch 138/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3707\n","Epoch 138: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3711 - val_loss: 0.7148\n","Epoch 139/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3691\n","Epoch 139: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3750 - val_loss: 0.5838\n","Epoch 140/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3562\n","Epoch 140: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3586 - val_loss: 0.3635\n","Epoch 141/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2975\n","Epoch 141: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3016 - val_loss: 0.6488\n","Epoch 142/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3168\n","Epoch 142: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3189 - val_loss: 0.5637\n","Epoch 143/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3165\n","Epoch 143: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3262 - val_loss: 0.8109\n","Epoch 144/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3044\n","Epoch 144: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3095 - val_loss: 0.4338\n","Epoch 145/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3223\n","Epoch 145: val_loss did not improve from 0.11876\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3235 - val_loss: 0.5506\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1238/1238 [00:00<00:00, 35330.19it/s]\n","100%|██████████| 1210/1210 [00:00<00:00, 35652.07it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/1000\n","72/72 [============================>.] - ETA: 0s - loss: 23.6900\n","Epoch 1: val_loss improved from inf to 5.89312, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-33-1000-15.h5\n","72/72 [==============================] - 4s 17ms/step - loss: 23.5482 - val_loss: 5.8931\n","Epoch 2/1000\n","71/72 [============================>.] - ETA: 0s - loss: 12.6832\n","Epoch 2: val_loss improved from 5.89312 to 4.05885, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-33-1000-15.h5\n","72/72 [==============================] - 1s 12ms/step - loss: 12.6901 - val_loss: 4.0588\n","Epoch 3/1000\n","71/72 [============================>.] - ETA: 0s - loss: 7.3358\n","Epoch 3: val_loss improved from 4.05885 to 1.06935, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-33-1000-15.h5\n","72/72 [==============================] - 1s 12ms/step - loss: 7.3251 - val_loss: 1.0694\n","Epoch 4/1000\n","71/72 [============================>.] - ETA: 0s - loss: 4.1478\n","Epoch 4: val_loss did not improve from 1.06935\n","72/72 [==============================] - 1s 11ms/step - loss: 4.1372 - val_loss: 1.2144\n","Epoch 5/1000\n","71/72 [============================>.] - ETA: 0s - loss: 3.4201\n","Epoch 5: val_loss improved from 1.06935 to 1.06673, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-33-1000-15.h5\n","72/72 [==============================] - 1s 12ms/step - loss: 3.4123 - val_loss: 1.0667\n","Epoch 6/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.9886\n","Epoch 6: val_loss improved from 1.06673 to 0.87991, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-33-1000-15.h5\n","72/72 [==============================] - 1s 12ms/step - loss: 3.0125 - val_loss: 0.8799\n","Epoch 7/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 3.3117\n","Epoch 7: val_loss did not improve from 0.87991\n","72/72 [==============================] - 1s 11ms/step - loss: 3.3980 - val_loss: 1.1069\n","Epoch 8/1000\n","71/72 [============================>.] - ETA: 0s - loss: 3.0260\n","Epoch 8: val_loss improved from 0.87991 to 0.28753, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-33-1000-15.h5\n","72/72 [==============================] - 1s 13ms/step - loss: 3.0127 - val_loss: 0.2875\n","Epoch 9/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.9957\n","Epoch 9: val_loss did not improve from 0.28753\n","72/72 [==============================] - 1s 11ms/step - loss: 2.9812 - val_loss: 5.9653\n","Epoch 10/1000\n","71/72 [============================>.] - ETA: 0s - loss: 4.6962\n","Epoch 10: val_loss did not improve from 0.28753\n","72/72 [==============================] - 1s 11ms/step - loss: 4.6839 - val_loss: 3.8714\n","Epoch 11/1000\n","71/72 [============================>.] - ETA: 0s - loss: 3.8154\n","Epoch 11: val_loss did not improve from 0.28753\n","72/72 [==============================] - 1s 11ms/step - loss: 3.9771 - val_loss: 3.0371\n","Epoch 12/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.4938\n","Epoch 12: val_loss did not improve from 0.28753\n","72/72 [==============================] - 1s 11ms/step - loss: 2.4922 - val_loss: 0.3165\n","Epoch 13/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.6242\n","Epoch 13: val_loss did not improve from 0.28753\n","72/72 [==============================] - 1s 11ms/step - loss: 2.6271 - val_loss: 1.7610\n","Epoch 14/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.8607\n","Epoch 14: val_loss did not improve from 0.28753\n","72/72 [==============================] - 1s 11ms/step - loss: 1.8805 - val_loss: 0.5666\n","Epoch 15/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.4782\n","Epoch 15: val_loss improved from 0.28753 to 0.21396, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-33-1000-15.h5\n","72/72 [==============================] - 1s 12ms/step - loss: 1.4867 - val_loss: 0.2140\n","Epoch 16/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.5382\n","Epoch 16: val_loss did not improve from 0.21396\n","72/72 [==============================] - 1s 11ms/step - loss: 1.5382 - val_loss: 0.2215\n","Epoch 17/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.4685\n","Epoch 17: val_loss did not improve from 0.21396\n","72/72 [==============================] - 1s 11ms/step - loss: 1.4724 - val_loss: 1.1730\n","Epoch 18/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.9605\n","Epoch 18: val_loss did not improve from 0.21396\n","72/72 [==============================] - 1s 11ms/step - loss: 1.9547 - val_loss: 0.6253\n","Epoch 19/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.3232\n","Epoch 19: val_loss did not improve from 0.21396\n","72/72 [==============================] - 1s 11ms/step - loss: 2.3097 - val_loss: 0.5681\n","Epoch 20/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.9169\n","Epoch 20: val_loss improved from 0.21396 to 0.19734, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-33-1000-15.h5\n","72/72 [==============================] - 1s 12ms/step - loss: 2.8985 - val_loss: 0.1973\n","Epoch 21/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.7732\n","Epoch 21: val_loss did not improve from 0.19734\n","72/72 [==============================] - 1s 11ms/step - loss: 1.7728 - val_loss: 0.4216\n","Epoch 22/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.9413\n","Epoch 22: val_loss did not improve from 0.19734\n","72/72 [==============================] - 1s 11ms/step - loss: 1.9681 - val_loss: 1.7696\n","Epoch 23/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.9490\n","Epoch 23: val_loss did not improve from 0.19734\n","72/72 [==============================] - 1s 11ms/step - loss: 1.9351 - val_loss: 1.5882\n","Epoch 24/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.5394\n","Epoch 24: val_loss did not improve from 0.19734\n","72/72 [==============================] - 1s 11ms/step - loss: 1.5439 - val_loss: 0.7903\n","Epoch 25/1000\n","71/72 [============================>.] - ETA: 0s - loss: 3.2959\n","Epoch 25: val_loss did not improve from 0.19734\n","72/72 [==============================] - 1s 11ms/step - loss: 3.2659 - val_loss: 1.6831\n","Epoch 26/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.4284\n","Epoch 26: val_loss did not improve from 0.19734\n","72/72 [==============================] - 1s 12ms/step - loss: 2.4373 - val_loss: 1.7747\n","Epoch 27/1000\n","71/72 [============================>.] - ETA: 0s - loss: 3.4011\n","Epoch 27: val_loss did not improve from 0.19734\n","72/72 [==============================] - 1s 11ms/step - loss: 3.4047 - val_loss: 5.1192\n","Epoch 28/1000\n","71/72 [============================>.] - ETA: 0s - loss: 3.5098\n","Epoch 28: val_loss did not improve from 0.19734\n","72/72 [==============================] - 1s 11ms/step - loss: 3.4610 - val_loss: 1.6612\n","Epoch 29/1000\n","71/72 [============================>.] - ETA: 0s - loss: 3.2527\n","Epoch 29: val_loss did not improve from 0.19734\n","72/72 [==============================] - 1s 12ms/step - loss: 3.2496 - val_loss: 1.1216\n","Epoch 30/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.5701\n","Epoch 30: val_loss did not improve from 0.19734\n","72/72 [==============================] - 1s 12ms/step - loss: 1.5694 - val_loss: 0.6149\n","Epoch 31/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.1304\n","Epoch 31: val_loss improved from 0.19734 to 0.16069, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-33-1000-15.h5\n","72/72 [==============================] - 1s 12ms/step - loss: 1.1573 - val_loss: 0.1607\n","Epoch 32/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.5219\n","Epoch 32: val_loss did not improve from 0.16069\n","72/72 [==============================] - 1s 11ms/step - loss: 1.5330 - val_loss: 0.2896\n","Epoch 33/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.4693\n","Epoch 33: val_loss did not improve from 0.16069\n","72/72 [==============================] - 1s 11ms/step - loss: 1.4677 - val_loss: 0.6517\n","Epoch 34/1000\n","72/72 [============================>.] - ETA: 0s - loss: 1.3457\n","Epoch 34: val_loss did not improve from 0.16069\n","72/72 [==============================] - 1s 11ms/step - loss: 1.3560 - val_loss: 1.7174\n","Epoch 35/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.5048\n","Epoch 35: val_loss did not improve from 0.16069\n","72/72 [==============================] - 1s 11ms/step - loss: 1.4911 - val_loss: 0.3609\n","Epoch 36/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.1881\n","Epoch 36: val_loss did not improve from 0.16069\n","72/72 [==============================] - 1s 11ms/step - loss: 1.1817 - val_loss: 0.4353\n","Epoch 37/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.9197\n","Epoch 37: val_loss improved from 0.16069 to 0.09843, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-33-1000-15.h5\n","72/72 [==============================] - 1s 12ms/step - loss: 1.9054 - val_loss: 0.0984\n","Epoch 38/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.8577\n","Epoch 38: val_loss did not improve from 0.09843\n","72/72 [==============================] - 1s 11ms/step - loss: 1.8618 - val_loss: 1.2223\n","Epoch 39/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.7361\n","Epoch 39: val_loss did not improve from 0.09843\n","72/72 [==============================] - 1s 11ms/step - loss: 1.7208 - val_loss: 0.6930\n","Epoch 40/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.4235\n","Epoch 40: val_loss did not improve from 0.09843\n","72/72 [==============================] - 1s 12ms/step - loss: 1.4306 - val_loss: 0.1490\n","Epoch 41/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.6271\n","Epoch 41: val_loss did not improve from 0.09843\n","72/72 [==============================] - 1s 12ms/step - loss: 1.6507 - val_loss: 0.1177\n","Epoch 42/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.5896\n","Epoch 42: val_loss did not improve from 0.09843\n","72/72 [==============================] - 1s 12ms/step - loss: 1.5809 - val_loss: 1.3559\n","Epoch 43/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.4820\n","Epoch 43: val_loss did not improve from 0.09843\n","72/72 [==============================] - 1s 12ms/step - loss: 1.4808 - val_loss: 0.6421\n","Epoch 44/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.9892\n","Epoch 44: val_loss did not improve from 0.09843\n","72/72 [==============================] - 1s 12ms/step - loss: 0.9928 - val_loss: 0.3247\n","Epoch 45/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.9478\n","Epoch 45: val_loss did not improve from 0.09843\n","72/72 [==============================] - 1s 11ms/step - loss: 0.9406 - val_loss: 0.1419\n","Epoch 46/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.2470\n","Epoch 46: val_loss did not improve from 0.09843\n","72/72 [==============================] - 1s 11ms/step - loss: 1.2311 - val_loss: 0.1382\n","Epoch 47/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.4482\n","Epoch 47: val_loss did not improve from 0.09843\n","72/72 [==============================] - 1s 11ms/step - loss: 1.4433 - val_loss: 0.8497\n","Epoch 48/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.1372\n","Epoch 48: val_loss did not improve from 0.09843\n","72/72 [==============================] - 1s 12ms/step - loss: 1.1481 - val_loss: 2.4660\n","Epoch 49/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.9406\n","Epoch 49: val_loss did not improve from 0.09843\n","72/72 [==============================] - 1s 11ms/step - loss: 0.9427 - val_loss: 0.1968\n","Epoch 50/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 1.0238\n","Epoch 50: val_loss improved from 0.09843 to 0.09120, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-33-1000-15.h5\n","72/72 [==============================] - 1s 18ms/step - loss: 1.0246 - val_loss: 0.0912\n","Epoch 51/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.9877\n","Epoch 51: val_loss did not improve from 0.09120\n","72/72 [==============================] - 1s 11ms/step - loss: 0.9952 - val_loss: 0.1230\n","Epoch 52/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.0296\n","Epoch 52: val_loss did not improve from 0.09120\n","72/72 [==============================] - 1s 11ms/step - loss: 1.0279 - val_loss: 0.3358\n","Epoch 53/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.0490\n","Epoch 53: val_loss did not improve from 0.09120\n","72/72 [==============================] - 1s 11ms/step - loss: 1.0358 - val_loss: 0.7095\n","Epoch 54/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8878\n","Epoch 54: val_loss did not improve from 0.09120\n","72/72 [==============================] - 1s 11ms/step - loss: 0.8882 - val_loss: 0.2320\n","Epoch 55/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.0975\n","Epoch 55: val_loss improved from 0.09120 to 0.09014, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-33-1000-15.h5\n","72/72 [==============================] - 1s 12ms/step - loss: 1.0913 - val_loss: 0.0901\n","Epoch 56/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.1804\n","Epoch 56: val_loss did not improve from 0.09014\n","72/72 [==============================] - 1s 12ms/step - loss: 1.1876 - val_loss: 0.3206\n","Epoch 57/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.0308\n","Epoch 57: val_loss did not improve from 0.09014\n","72/72 [==============================] - 1s 11ms/step - loss: 1.0310 - val_loss: 3.6210\n","Epoch 58/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.2138\n","Epoch 58: val_loss did not improve from 0.09014\n","72/72 [==============================] - 1s 11ms/step - loss: 1.2024 - val_loss: 0.2014\n","Epoch 59/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.2102\n","Epoch 59: val_loss did not improve from 0.09014\n","72/72 [==============================] - 1s 11ms/step - loss: 1.2031 - val_loss: 0.5357\n","Epoch 60/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.9378\n","Epoch 60: val_loss did not improve from 0.09014\n","72/72 [==============================] - 1s 11ms/step - loss: 0.9464 - val_loss: 0.5994\n","Epoch 61/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.6266\n","Epoch 61: val_loss did not improve from 0.09014\n","72/72 [==============================] - 1s 11ms/step - loss: 1.6079 - val_loss: 1.1497\n","Epoch 62/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.2074\n","Epoch 62: val_loss did not improve from 0.09014\n","72/72 [==============================] - 1s 11ms/step - loss: 1.2045 - val_loss: 0.2817\n","Epoch 63/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.3793\n","Epoch 63: val_loss did not improve from 0.09014\n","72/72 [==============================] - 1s 11ms/step - loss: 1.3677 - val_loss: 1.2844\n","Epoch 64/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.2718\n","Epoch 64: val_loss did not improve from 0.09014\n","72/72 [==============================] - 1s 11ms/step - loss: 1.2692 - val_loss: 2.2913\n","Epoch 65/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.5544\n","Epoch 65: val_loss did not improve from 0.09014\n","72/72 [==============================] - 1s 11ms/step - loss: 1.5582 - val_loss: 1.1880\n","Epoch 66/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.8525\n","Epoch 66: val_loss did not improve from 0.09014\n","72/72 [==============================] - 1s 11ms/step - loss: 1.8714 - val_loss: 0.6206\n","Epoch 67/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.3125\n","Epoch 67: val_loss did not improve from 0.09014\n","72/72 [==============================] - 1s 11ms/step - loss: 1.3004 - val_loss: 4.5084\n","Epoch 68/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.2095\n","Epoch 68: val_loss did not improve from 0.09014\n","72/72 [==============================] - 1s 11ms/step - loss: 1.2178 - val_loss: 0.3734\n","Epoch 69/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.8187\n","Epoch 69: val_loss did not improve from 0.09014\n","72/72 [==============================] - 1s 11ms/step - loss: 1.8092 - val_loss: 2.2758\n","Epoch 70/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.0847\n","Epoch 70: val_loss did not improve from 0.09014\n","72/72 [==============================] - 1s 11ms/step - loss: 2.0703 - val_loss: 2.1715\n","Epoch 71/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.6731\n","Epoch 71: val_loss did not improve from 0.09014\n","72/72 [==============================] - 1s 11ms/step - loss: 2.6301 - val_loss: 2.3551\n","Epoch 72/1000\n","72/72 [============================>.] - ETA: 0s - loss: 1.3476\n","Epoch 72: val_loss did not improve from 0.09014\n","72/72 [==============================] - 1s 11ms/step - loss: 1.3438 - val_loss: 1.7339\n","Epoch 73/1000\n","72/72 [============================>.] - ETA: 0s - loss: 1.6694\n","Epoch 73: val_loss did not improve from 0.09014\n","72/72 [==============================] - 1s 11ms/step - loss: 1.6639 - val_loss: 0.7922\n","Epoch 74/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.6837\n","Epoch 74: val_loss did not improve from 0.09014\n","72/72 [==============================] - 1s 11ms/step - loss: 1.6794 - val_loss: 0.7081\n","Epoch 75/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.6325\n","Epoch 75: val_loss did not improve from 0.09014\n","72/72 [==============================] - 1s 11ms/step - loss: 2.6129 - val_loss: 1.4266\n","Epoch 76/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.3974\n","Epoch 76: val_loss did not improve from 0.09014\n","72/72 [==============================] - 1s 11ms/step - loss: 2.4078 - val_loss: 2.1780\n","Epoch 77/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.5159\n","Epoch 77: val_loss did not improve from 0.09014\n","72/72 [==============================] - 1s 11ms/step - loss: 1.4997 - val_loss: 1.3657\n","Epoch 78/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.2072\n","Epoch 78: val_loss did not improve from 0.09014\n","72/72 [==============================] - 1s 11ms/step - loss: 1.1923 - val_loss: 0.2187\n","Epoch 79/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.2462\n","Epoch 79: val_loss did not improve from 0.09014\n","72/72 [==============================] - 1s 11ms/step - loss: 1.2326 - val_loss: 0.6976\n","Epoch 80/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.9773\n","Epoch 80: val_loss improved from 0.09014 to 0.07877, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-33-1000-15.h5\n","72/72 [==============================] - 1s 17ms/step - loss: 0.9845 - val_loss: 0.0788\n","Epoch 81/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.1638\n","Epoch 81: val_loss did not improve from 0.07877\n","72/72 [==============================] - 1s 11ms/step - loss: 1.1630 - val_loss: 0.7626\n","Epoch 82/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.0858\n","Epoch 82: val_loss did not improve from 0.07877\n","72/72 [==============================] - 1s 11ms/step - loss: 1.0828 - val_loss: 1.1798\n","Epoch 83/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.0157\n","Epoch 83: val_loss did not improve from 0.07877\n","72/72 [==============================] - 1s 11ms/step - loss: 1.0186 - val_loss: 1.1424\n","Epoch 84/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8596\n","Epoch 84: val_loss did not improve from 0.07877\n","72/72 [==============================] - 1s 11ms/step - loss: 0.8837 - val_loss: 0.2253\n","Epoch 85/1000\n","72/72 [============================>.] - ETA: 0s - loss: 1.1244\n","Epoch 85: val_loss did not improve from 0.07877\n","72/72 [==============================] - 1s 11ms/step - loss: 1.1216 - val_loss: 0.5396\n","Epoch 86/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 1.0327\n","Epoch 86: val_loss did not improve from 0.07877\n","72/72 [==============================] - 1s 11ms/step - loss: 1.0265 - val_loss: 0.2056\n","Epoch 87/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8378\n","Epoch 87: val_loss did not improve from 0.07877\n","72/72 [==============================] - 1s 11ms/step - loss: 0.8482 - val_loss: 1.0828\n","Epoch 88/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.9072\n","Epoch 88: val_loss did not improve from 0.07877\n","72/72 [==============================] - 1s 11ms/step - loss: 0.9127 - val_loss: 1.5474\n","Epoch 89/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8437\n","Epoch 89: val_loss did not improve from 0.07877\n","72/72 [==============================] - 1s 11ms/step - loss: 0.8379 - val_loss: 0.2041\n","Epoch 90/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8599\n","Epoch 90: val_loss did not improve from 0.07877\n","72/72 [==============================] - 1s 11ms/step - loss: 0.8612 - val_loss: 0.2573\n","Epoch 91/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.0612\n","Epoch 91: val_loss did not improve from 0.07877\n","72/72 [==============================] - 1s 11ms/step - loss: 1.0678 - val_loss: 0.1036\n","Epoch 92/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.0764\n","Epoch 92: val_loss did not improve from 0.07877\n","72/72 [==============================] - 1s 11ms/step - loss: 1.0809 - val_loss: 0.3988\n","Epoch 93/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.0029\n","Epoch 93: val_loss did not improve from 0.07877\n","72/72 [==============================] - 1s 11ms/step - loss: 1.0332 - val_loss: 3.2495\n","Epoch 94/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.9345\n","Epoch 94: val_loss did not improve from 0.07877\n","72/72 [==============================] - 1s 11ms/step - loss: 0.9300 - val_loss: 0.2010\n","Epoch 95/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.7654\n","Epoch 95: val_loss improved from 0.07877 to 0.05959, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-33-1000-15.h5\n","72/72 [==============================] - 1s 16ms/step - loss: 0.7691 - val_loss: 0.0596\n","Epoch 96/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.7774\n","Epoch 96: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 0.7829 - val_loss: 0.1360\n","Epoch 97/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.0092\n","Epoch 97: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 1.0143 - val_loss: 0.4140\n","Epoch 98/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8891\n","Epoch 98: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 0.8969 - val_loss: 1.6419\n","Epoch 99/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.7756\n","Epoch 99: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 0.7824 - val_loss: 0.1240\n","Epoch 100/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8443\n","Epoch 100: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 0.8404 - val_loss: 0.0642\n","Epoch 101/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.7779\n","Epoch 101: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 0.7780 - val_loss: 0.1066\n","Epoch 102/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.9450\n","Epoch 102: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 12ms/step - loss: 0.9408 - val_loss: 0.3058\n","Epoch 103/1000\n","72/72 [============================>.] - ETA: 0s - loss: 1.1792\n","Epoch 103: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 1.1767 - val_loss: 2.6667\n","Epoch 104/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.3255\n","Epoch 104: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 1.3438 - val_loss: 3.6488\n","Epoch 105/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.9690\n","Epoch 105: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 0.9619 - val_loss: 0.8713\n","Epoch 106/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.9367\n","Epoch 106: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 1.9587 - val_loss: 11.1581\n","Epoch 107/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.1675\n","Epoch 107: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 2.1871 - val_loss: 20.5880\n","Epoch 108/1000\n","71/72 [============================>.] - ETA: 0s - loss: 5.7301\n","Epoch 108: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 5.7555 - val_loss: 1.8678\n","Epoch 109/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.3801\n","Epoch 109: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 2.3698 - val_loss: 0.8400\n","Epoch 110/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.5041\n","Epoch 110: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 1.5296 - val_loss: 1.9472\n","Epoch 111/1000\n","72/72 [============================>.] - ETA: 0s - loss: 1.9383\n","Epoch 111: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 1.9366 - val_loss: 0.2533\n","Epoch 112/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.6149\n","Epoch 112: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 1.6044 - val_loss: 0.1733\n","Epoch 113/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.0198\n","Epoch 113: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 1.0113 - val_loss: 0.5737\n","Epoch 114/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.9291\n","Epoch 114: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 0.9303 - val_loss: 1.2724\n","Epoch 115/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8385\n","Epoch 115: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 0.8443 - val_loss: 0.3549\n","Epoch 116/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8066\n","Epoch 116: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 0.8046 - val_loss: 0.2174\n","Epoch 117/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.4632\n","Epoch 117: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 1.4579 - val_loss: 1.5601\n","Epoch 118/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.0220\n","Epoch 118: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 1.0229 - val_loss: 0.9460\n","Epoch 119/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.0519\n","Epoch 119: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 1.0505 - val_loss: 0.3187\n","Epoch 120/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.2645\n","Epoch 120: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 1.2593 - val_loss: 1.1895\n","Epoch 121/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.9286\n","Epoch 121: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 0.9171 - val_loss: 0.9820\n","Epoch 122/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.6765\n","Epoch 122: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 0.6850 - val_loss: 0.2835\n","Epoch 123/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.0674\n","Epoch 123: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 1.0785 - val_loss: 0.7815\n","Epoch 124/1000\n","72/72 [============================>.] - ETA: 0s - loss: 0.9689\n","Epoch 124: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 0.9642 - val_loss: 1.1288\n","Epoch 125/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.7954\n","Epoch 125: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 0.7977 - val_loss: 0.4560\n","Epoch 126/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.1714\n","Epoch 126: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 1.1733 - val_loss: 0.3432\n","Epoch 127/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.7766\n","Epoch 127: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 0.7784 - val_loss: 1.1160\n","Epoch 128/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8313\n","Epoch 128: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 0.8256 - val_loss: 0.0758\n","Epoch 129/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.7506\n","Epoch 129: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 0.7560 - val_loss: 0.2638\n","Epoch 130/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.7598\n","Epoch 130: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 0.7553 - val_loss: 0.5643\n","Epoch 131/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.6537\n","Epoch 131: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 0.6604 - val_loss: 0.1257\n","Epoch 132/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.9099\n","Epoch 132: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 0.9241 - val_loss: 0.1259\n","Epoch 133/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.7706\n","Epoch 133: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 0.7650 - val_loss: 0.9149\n","Epoch 134/1000\n","72/72 [============================>.] - ETA: 0s - loss: 0.7738\n","Epoch 134: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 0.7716 - val_loss: 0.0995\n","Epoch 135/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.7971\n","Epoch 135: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 0.7979 - val_loss: 0.1155\n","Epoch 136/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8372\n","Epoch 136: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 0.8266 - val_loss: 0.2150\n","Epoch 137/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.6875\n","Epoch 137: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 0.6880 - val_loss: 0.5450\n","Epoch 138/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.6688\n","Epoch 138: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 0.6715 - val_loss: 0.1033\n","Epoch 139/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.7312\n","Epoch 139: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 12ms/step - loss: 0.7371 - val_loss: 0.2512\n","Epoch 140/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.7212\n","Epoch 140: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 0.7266 - val_loss: 0.3620\n","Epoch 141/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5698\n","Epoch 141: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5728 - val_loss: 0.0916\n","Epoch 142/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.6750\n","Epoch 142: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 0.6712 - val_loss: 0.1145\n","Epoch 143/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.6821\n","Epoch 143: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 0.6854 - val_loss: 0.1175\n","Epoch 144/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.6370\n","Epoch 144: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 0.6384 - val_loss: 0.5014\n","Epoch 145/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.7824\n","Epoch 145: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 0.7990 - val_loss: 1.6190\n","Epoch 146/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8397\n","Epoch 146: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 0.8453 - val_loss: 0.9980\n","Epoch 147/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.7994\n","Epoch 147: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 12ms/step - loss: 0.8103 - val_loss: 4.1208\n","Epoch 148/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.1532\n","Epoch 148: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 1.1371 - val_loss: 0.1237\n","Epoch 149/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.7257\n","Epoch 149: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 0.7256 - val_loss: 0.7608\n","Epoch 150/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.4766\n","Epoch 150: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 1.4691 - val_loss: 0.2831\n","Epoch 151/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.4038\n","Epoch 151: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 1.3943 - val_loss: 0.8417\n","Epoch 152/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.6044\n","Epoch 152: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 2.6557 - val_loss: 2.4451\n","Epoch 153/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.4198\n","Epoch 153: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 1.4271 - val_loss: 2.0913\n","Epoch 154/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.1408\n","Epoch 154: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 1.1334 - val_loss: 2.1548\n","Epoch 155/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.2182\n","Epoch 155: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 2.1854 - val_loss: 2.3567\n","Epoch 156/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.4057\n","Epoch 156: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 1.3911 - val_loss: 1.5949\n","Epoch 157/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.2317\n","Epoch 157: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 1.2393 - val_loss: 2.1889\n","Epoch 158/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.3653\n","Epoch 158: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 1.3474 - val_loss: 1.4030\n","Epoch 159/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.1767\n","Epoch 159: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 1.1665 - val_loss: 1.1857\n","Epoch 160/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.9733\n","Epoch 160: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 0.9603 - val_loss: 0.0897\n","Epoch 161/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8972\n","Epoch 161: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 0.9047 - val_loss: 1.1212\n","Epoch 162/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.0475\n","Epoch 162: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 1.0839 - val_loss: 0.1805\n","Epoch 163/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.1013\n","Epoch 163: val_loss did not improve from 0.05959\n","72/72 [==============================] - 1s 11ms/step - loss: 1.1324 - val_loss: 0.4787\n","Epoch 164/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.7652\n","Epoch 164: val_loss improved from 0.05959 to 0.04341, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-33-1000-15.h5\n","72/72 [==============================] - 1s 16ms/step - loss: 0.7758 - val_loss: 0.0434\n","Epoch 165/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.6899\n","Epoch 165: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.7023 - val_loss: 0.2981\n","Epoch 166/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8370\n","Epoch 166: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.8442 - val_loss: 1.0284\n","Epoch 167/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8516\n","Epoch 167: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.8444 - val_loss: 0.6667\n","Epoch 168/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.6046\n","Epoch 168: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.6152 - val_loss: 0.0835\n","Epoch 169/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.7279\n","Epoch 169: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 12ms/step - loss: 0.7231 - val_loss: 0.2105\n","Epoch 170/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8997\n","Epoch 170: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.9015 - val_loss: 0.3766\n","Epoch 171/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.6657\n","Epoch 171: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.6616 - val_loss: 0.4549\n","Epoch 172/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5628\n","Epoch 172: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5623 - val_loss: 0.0780\n","Epoch 173/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.6373\n","Epoch 173: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.6323 - val_loss: 0.5142\n","Epoch 174/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.6400\n","Epoch 174: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.6406 - val_loss: 0.2382\n","Epoch 175/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5834\n","Epoch 175: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5834 - val_loss: 0.3681\n","Epoch 176/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.6443\n","Epoch 176: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.6470 - val_loss: 0.1048\n","Epoch 177/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 0.8118\n","Epoch 177: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 12ms/step - loss: 0.8137 - val_loss: 0.3714\n","Epoch 178/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.6687\n","Epoch 178: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.6678 - val_loss: 1.1802\n","Epoch 179/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5997\n","Epoch 179: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.6034 - val_loss: 0.2683\n","Epoch 180/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.7741\n","Epoch 180: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.7725 - val_loss: 0.2108\n","Epoch 181/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.1620\n","Epoch 181: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 1.1575 - val_loss: 0.6045\n","Epoch 182/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.7504\n","Epoch 182: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.7476 - val_loss: 0.7388\n","Epoch 183/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 0.5930\n","Epoch 183: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 12ms/step - loss: 0.5931 - val_loss: 0.0508\n","Epoch 184/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5707\n","Epoch 184: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5706 - val_loss: 0.1937\n","Epoch 185/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5399\n","Epoch 185: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5424 - val_loss: 0.1768\n","Epoch 186/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5957\n","Epoch 186: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5935 - val_loss: 0.2833\n","Epoch 187/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5333\n","Epoch 187: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5350 - val_loss: 0.0836\n","Epoch 188/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.6884\n","Epoch 188: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.6874 - val_loss: 0.1652\n","Epoch 189/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5884\n","Epoch 189: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5879 - val_loss: 0.5420\n","Epoch 190/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5894\n","Epoch 190: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5963 - val_loss: 0.1255\n","Epoch 191/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.7271\n","Epoch 191: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.7186 - val_loss: 0.1432\n","Epoch 192/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5673\n","Epoch 192: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5720 - val_loss: 0.3451\n","Epoch 193/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5561\n","Epoch 193: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5512 - val_loss: 0.4631\n","Epoch 194/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5337\n","Epoch 194: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5268 - val_loss: 0.0452\n","Epoch 195/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5484\n","Epoch 195: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5420 - val_loss: 0.1868\n","Epoch 196/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.6056\n","Epoch 196: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.6114 - val_loss: 0.3589\n","Epoch 197/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4663\n","Epoch 197: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4672 - val_loss: 0.0830\n","Epoch 198/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5003\n","Epoch 198: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5038 - val_loss: 0.0873\n","Epoch 199/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5496\n","Epoch 199: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5491 - val_loss: 0.1477\n","Epoch 200/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 0.5979\n","Epoch 200: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5927 - val_loss: 0.2452\n","Epoch 201/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.6365\n","Epoch 201: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.6320 - val_loss: 0.7829\n","Epoch 202/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.6020\n","Epoch 202: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.6019 - val_loss: 0.1235\n","Epoch 203/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.6197\n","Epoch 203: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.6136 - val_loss: 0.1157\n","Epoch 204/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4990\n","Epoch 204: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5001 - val_loss: 0.3313\n","Epoch 205/1000\n","72/72 [============================>.] - ETA: 0s - loss: 0.4418\n","Epoch 205: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4408 - val_loss: 0.1691\n","Epoch 206/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4869\n","Epoch 206: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4854 - val_loss: 0.0499\n","Epoch 207/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5493\n","Epoch 207: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5462 - val_loss: 0.1150\n","Epoch 208/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5318\n","Epoch 208: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5284 - val_loss: 0.6309\n","Epoch 209/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.6491\n","Epoch 209: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.6410 - val_loss: 0.3541\n","Epoch 210/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5624\n","Epoch 210: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5618 - val_loss: 0.6804\n","Epoch 211/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.6361\n","Epoch 211: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.6359 - val_loss: 0.6293\n","Epoch 212/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8944\n","Epoch 212: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.8879 - val_loss: 1.1081\n","Epoch 213/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.9471\n","Epoch 213: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.9468 - val_loss: 0.0887\n","Epoch 214/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.0512\n","Epoch 214: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 12ms/step - loss: 1.0522 - val_loss: 0.1884\n","Epoch 215/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.6869\n","Epoch 215: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.6926 - val_loss: 0.5956\n","Epoch 216/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.7376\n","Epoch 216: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.7351 - val_loss: 1.4631\n","Epoch 217/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.2320\n","Epoch 217: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 1.2168 - val_loss: 0.2565\n","Epoch 218/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.1232\n","Epoch 218: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 1.1130 - val_loss: 0.6774\n","Epoch 219/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 0.8489\n","Epoch 219: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 12ms/step - loss: 0.8442 - val_loss: 0.3703\n","Epoch 220/1000\n","69/72 [===========================>..] - ETA: 0s - loss: 0.8231\n","Epoch 220: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 12ms/step - loss: 0.8902 - val_loss: 0.2570\n","Epoch 221/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.6512\n","Epoch 221: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.6609 - val_loss: 1.0296\n","Epoch 222/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5392\n","Epoch 222: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5460 - val_loss: 0.0824\n","Epoch 223/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5774\n","Epoch 223: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5695 - val_loss: 0.1702\n","Epoch 224/1000\n","69/72 [===========================>..] - ETA: 0s - loss: 0.4616\n","Epoch 224: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 12ms/step - loss: 0.4717 - val_loss: 0.1258\n","Epoch 225/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4828\n","Epoch 225: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4808 - val_loss: 0.0880\n","Epoch 226/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4466\n","Epoch 226: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4462 - val_loss: 0.1064\n","Epoch 227/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4229\n","Epoch 227: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4404 - val_loss: 0.0887\n","Epoch 228/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4960\n","Epoch 228: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4934 - val_loss: 0.0946\n","Epoch 229/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.6511\n","Epoch 229: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.6477 - val_loss: 0.5110\n","Epoch 230/1000\n","73/72 [==============================] - ETA: 0s - loss: 0.5322\n","Epoch 230: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5322 - val_loss: 0.1866\n","Epoch 231/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4428\n","Epoch 231: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4485 - val_loss: 0.2623\n","Epoch 232/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4912\n","Epoch 232: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4895 - val_loss: 0.1139\n","Epoch 233/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4868\n","Epoch 233: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4917 - val_loss: 0.2165\n","Epoch 234/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4657\n","Epoch 234: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4722 - val_loss: 0.1357\n","Epoch 235/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5587\n","Epoch 235: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5600 - val_loss: 0.1356\n","Epoch 236/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5961\n","Epoch 236: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.6005 - val_loss: 0.1882\n","Epoch 237/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5553\n","Epoch 237: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5570 - val_loss: 0.6690\n","Epoch 238/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4877\n","Epoch 238: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4885 - val_loss: 0.6371\n","Epoch 239/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.6113\n","Epoch 239: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.6083 - val_loss: 0.1519\n","Epoch 240/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4586\n","Epoch 240: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4636 - val_loss: 0.1128\n","Epoch 241/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4639\n","Epoch 241: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4611 - val_loss: 0.1435\n","Epoch 242/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 0.5486\n","Epoch 242: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 12ms/step - loss: 0.5443 - val_loss: 0.2091\n","Epoch 243/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4182\n","Epoch 243: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4283 - val_loss: 0.0893\n","Epoch 244/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4048\n","Epoch 244: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 12ms/step - loss: 0.4086 - val_loss: 0.0819\n","Epoch 245/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4819\n","Epoch 245: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4857 - val_loss: 0.1144\n","Epoch 246/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4247\n","Epoch 246: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4203 - val_loss: 0.1311\n","Epoch 247/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4629\n","Epoch 247: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4719 - val_loss: 0.0898\n","Epoch 248/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4362\n","Epoch 248: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4322 - val_loss: 0.2955\n","Epoch 249/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5330\n","Epoch 249: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5295 - val_loss: 0.3202\n","Epoch 250/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4850\n","Epoch 250: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4859 - val_loss: 0.2838\n","Epoch 251/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 0.4929\n","Epoch 251: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 12ms/step - loss: 0.4929 - val_loss: 0.1579\n","Epoch 252/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4737\n","Epoch 252: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4728 - val_loss: 0.2214\n","Epoch 253/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4140\n","Epoch 253: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4206 - val_loss: 0.2027\n","Epoch 254/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4351\n","Epoch 254: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4369 - val_loss: 0.0930\n","Epoch 255/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4682\n","Epoch 255: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4624 - val_loss: 0.1294\n","Epoch 256/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 0.4416\n","Epoch 256: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 12ms/step - loss: 0.4344 - val_loss: 0.0860\n","Epoch 257/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4669\n","Epoch 257: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4677 - val_loss: 0.1070\n","Epoch 258/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3968\n","Epoch 258: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4038 - val_loss: 0.1824\n","Epoch 259/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4776\n","Epoch 259: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4727 - val_loss: 0.2189\n","Epoch 260/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4049\n","Epoch 260: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 12ms/step - loss: 0.4041 - val_loss: 0.1700\n","Epoch 261/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4346\n","Epoch 261: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4361 - val_loss: 0.4822\n","Epoch 262/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4595\n","Epoch 262: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4553 - val_loss: 0.4282\n","Epoch 263/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5948\n","Epoch 263: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5939 - val_loss: 0.2956\n","Epoch 264/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4682\n","Epoch 264: val_loss did not improve from 0.04341\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4686 - val_loss: 0.3034\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1238/1238 [00:00<00:00, 36681.68it/s]\n","100%|██████████| 1210/1210 [00:00<00:00, 35141.07it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/1000\n","71/72 [============================>.] - ETA: 0s - loss: 29.0016\n","Epoch 1: val_loss improved from inf to 11.49974, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-34-1000-15.h5\n","72/72 [==============================] - 4s 17ms/step - loss: 28.5228 - val_loss: 11.4997\n","Epoch 2/1000\n","72/72 [============================>.] - ETA: 0s - loss: 8.0818\n","Epoch 2: val_loss improved from 11.49974 to 0.58774, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-34-1000-15.h5\n","72/72 [==============================] - 1s 12ms/step - loss: 8.0533 - val_loss: 0.5877\n","Epoch 3/1000\n","71/72 [============================>.] - ETA: 0s - loss: 4.4298\n","Epoch 3: val_loss did not improve from 0.58774\n","72/72 [==============================] - 1s 11ms/step - loss: 4.4168 - val_loss: 2.8504\n","Epoch 4/1000\n","71/72 [============================>.] - ETA: 0s - loss: 3.5078\n","Epoch 4: val_loss did not improve from 0.58774\n","72/72 [==============================] - 1s 11ms/step - loss: 3.4824 - val_loss: 1.4050\n","Epoch 5/1000\n","71/72 [============================>.] - ETA: 0s - loss: 3.8434\n","Epoch 5: val_loss did not improve from 0.58774\n","72/72 [==============================] - 1s 11ms/step - loss: 3.8236 - val_loss: 8.0755\n","Epoch 6/1000\n","72/72 [============================>.] - ETA: 0s - loss: 5.5802\n","Epoch 6: val_loss did not improve from 0.58774\n","72/72 [==============================] - 1s 11ms/step - loss: 5.5598 - val_loss: 4.3807\n","Epoch 7/1000\n","72/72 [============================>.] - ETA: 0s - loss: 3.3432\n","Epoch 7: val_loss improved from 0.58774 to 0.20466, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-34-1000-15.h5\n","72/72 [==============================] - 1s 12ms/step - loss: 3.3460 - val_loss: 0.2047\n","Epoch 8/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 3.0543\n","Epoch 8: val_loss did not improve from 0.20466\n","72/72 [==============================] - 1s 12ms/step - loss: 3.1406 - val_loss: 3.1351\n","Epoch 9/1000\n","71/72 [============================>.] - ETA: 0s - loss: 3.4153\n","Epoch 9: val_loss did not improve from 0.20466\n","72/72 [==============================] - 1s 11ms/step - loss: 3.3991 - val_loss: 2.0809\n","Epoch 10/1000\n","71/72 [============================>.] - ETA: 0s - loss: 3.8335\n","Epoch 10: val_loss did not improve from 0.20466\n","72/72 [==============================] - 1s 11ms/step - loss: 3.7906 - val_loss: 3.7867\n","Epoch 11/1000\n","71/72 [============================>.] - ETA: 0s - loss: 3.2011\n","Epoch 11: val_loss did not improve from 0.20466\n","72/72 [==============================] - 1s 11ms/step - loss: 3.2413 - val_loss: 1.6853\n","Epoch 12/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.4786\n","Epoch 12: val_loss did not improve from 0.20466\n","72/72 [==============================] - 1s 11ms/step - loss: 2.5086 - val_loss: 1.2011\n","Epoch 13/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.8220\n","Epoch 13: val_loss did not improve from 0.20466\n","72/72 [==============================] - 1s 11ms/step - loss: 1.8103 - val_loss: 1.1073\n","Epoch 14/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.3989\n","Epoch 14: val_loss did not improve from 0.20466\n","72/72 [==============================] - 1s 11ms/step - loss: 1.3990 - val_loss: 0.6071\n","Epoch 15/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.4673\n","Epoch 15: val_loss did not improve from 0.20466\n","72/72 [==============================] - 1s 11ms/step - loss: 1.4657 - val_loss: 0.3841\n","Epoch 16/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.4868\n","Epoch 16: val_loss did not improve from 0.20466\n","72/72 [==============================] - 1s 11ms/step - loss: 1.4839 - val_loss: 0.4680\n","Epoch 17/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.8505\n","Epoch 17: val_loss did not improve from 0.20466\n","72/72 [==============================] - 1s 12ms/step - loss: 1.8404 - val_loss: 1.2331\n","Epoch 18/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.9166\n","Epoch 18: val_loss did not improve from 0.20466\n","72/72 [==============================] - 1s 11ms/step - loss: 1.9268 - val_loss: 0.5918\n","Epoch 19/1000\n","71/72 [============================>.] - ETA: 0s - loss: 3.4205\n","Epoch 19: val_loss did not improve from 0.20466\n","72/72 [==============================] - 1s 11ms/step - loss: 3.3669 - val_loss: 2.5325\n","Epoch 20/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.2482\n","Epoch 20: val_loss did not improve from 0.20466\n","72/72 [==============================] - 1s 11ms/step - loss: 2.2338 - val_loss: 3.1491\n","Epoch 21/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.2800\n","Epoch 21: val_loss did not improve from 0.20466\n","72/72 [==============================] - 1s 11ms/step - loss: 2.2627 - val_loss: 1.9670\n","Epoch 22/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.3282\n","Epoch 22: val_loss did not improve from 0.20466\n","72/72 [==============================] - 1s 11ms/step - loss: 2.3193 - val_loss: 0.5458\n","Epoch 23/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.4589\n","Epoch 23: val_loss did not improve from 0.20466\n","72/72 [==============================] - 1s 11ms/step - loss: 1.4704 - val_loss: 0.4505\n","Epoch 24/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.2284\n","Epoch 24: val_loss did not improve from 0.20466\n","72/72 [==============================] - 1s 11ms/step - loss: 1.2231 - val_loss: 0.3579\n","Epoch 25/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.3625\n","Epoch 25: val_loss did not improve from 0.20466\n","72/72 [==============================] - 1s 11ms/step - loss: 1.3544 - val_loss: 0.5645\n","Epoch 26/1000\n","69/72 [===========================>..] - ETA: 0s - loss: 1.8458\n","Epoch 26: val_loss did not improve from 0.20466\n","72/72 [==============================] - 1s 11ms/step - loss: 1.8678 - val_loss: 1.1546\n","Epoch 27/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.2599\n","Epoch 27: val_loss did not improve from 0.20466\n","72/72 [==============================] - 1s 11ms/step - loss: 1.2699 - val_loss: 0.8248\n","Epoch 28/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.1237\n","Epoch 28: val_loss did not improve from 0.20466\n","72/72 [==============================] - 1s 11ms/step - loss: 1.1137 - val_loss: 0.3747\n","Epoch 29/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.1721\n","Epoch 29: val_loss did not improve from 0.20466\n","72/72 [==============================] - 1s 11ms/step - loss: 1.1584 - val_loss: 0.4661\n","Epoch 30/1000\n","72/72 [============================>.] - ETA: 0s - loss: 1.0663\n","Epoch 30: val_loss did not improve from 0.20466\n","72/72 [==============================] - 1s 11ms/step - loss: 1.0618 - val_loss: 0.5388\n","Epoch 31/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.9443\n","Epoch 31: val_loss did not improve from 0.20466\n","72/72 [==============================] - 1s 11ms/step - loss: 0.9471 - val_loss: 0.9386\n","Epoch 32/1000\n","72/72 [============================>.] - ETA: 0s - loss: 0.9447\n","Epoch 32: val_loss did not improve from 0.20466\n","72/72 [==============================] - 1s 11ms/step - loss: 0.9442 - val_loss: 0.6569\n","Epoch 33/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.9710\n","Epoch 33: val_loss did not improve from 0.20466\n","72/72 [==============================] - 1s 11ms/step - loss: 0.9712 - val_loss: 0.9923\n","Epoch 34/1000\n","73/72 [==============================] - ETA: 0s - loss: 1.2397\n","Epoch 34: val_loss did not improve from 0.20466\n","72/72 [==============================] - 1s 11ms/step - loss: 1.2397 - val_loss: 0.4424\n","Epoch 35/1000\n","72/72 [============================>.] - ETA: 0s - loss: 1.7185\n","Epoch 35: val_loss did not improve from 0.20466\n","72/72 [==============================] - 1s 11ms/step - loss: 1.7117 - val_loss: 1.4823\n","Epoch 36/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.8174\n","Epoch 36: val_loss did not improve from 0.20466\n","72/72 [==============================] - 1s 11ms/step - loss: 1.7957 - val_loss: 0.5931\n","Epoch 37/1000\n","71/72 [============================>.] - ETA: 0s - loss: 5.8476\n","Epoch 37: val_loss did not improve from 0.20466\n","72/72 [==============================] - 1s 11ms/step - loss: 5.7378 - val_loss: 7.8213\n","Epoch 38/1000\n","71/72 [============================>.] - ETA: 0s - loss: 3.4311\n","Epoch 38: val_loss did not improve from 0.20466\n","72/72 [==============================] - 1s 11ms/step - loss: 3.3826 - val_loss: 2.1581\n","Epoch 39/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.4763\n","Epoch 39: val_loss did not improve from 0.20466\n","72/72 [==============================] - 1s 11ms/step - loss: 2.6554 - val_loss: 2.6085\n","Epoch 40/1000\n","71/72 [============================>.] - ETA: 0s - loss: 5.2790\n","Epoch 40: val_loss did not improve from 0.20466\n","72/72 [==============================] - 1s 11ms/step - loss: 5.2325 - val_loss: 3.3362\n","Epoch 41/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.6742\n","Epoch 41: val_loss did not improve from 0.20466\n","72/72 [==============================] - 1s 11ms/step - loss: 1.6621 - val_loss: 1.3632\n","Epoch 42/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.2127\n","Epoch 42: val_loss did not improve from 0.20466\n","72/72 [==============================] - 1s 11ms/step - loss: 1.2196 - val_loss: 1.5846\n","Epoch 43/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 1.0136\n","Epoch 43: val_loss did not improve from 0.20466\n","72/72 [==============================] - 1s 11ms/step - loss: 1.0032 - val_loss: 0.8199\n","Epoch 44/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8823\n","Epoch 44: val_loss did not improve from 0.20466\n","72/72 [==============================] - 1s 11ms/step - loss: 0.8805 - val_loss: 0.9454\n","Epoch 45/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.9734\n","Epoch 45: val_loss did not improve from 0.20466\n","72/72 [==============================] - 1s 11ms/step - loss: 0.9711 - val_loss: 1.2720\n","Epoch 46/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 0.9128\n","Epoch 46: val_loss improved from 0.20466 to 0.15799, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-34-1000-15.h5\n","72/72 [==============================] - 1s 17ms/step - loss: 0.9130 - val_loss: 0.1580\n","Epoch 47/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.9299\n","Epoch 47: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.9284 - val_loss: 0.3598\n","Epoch 48/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.9441\n","Epoch 48: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.9357 - val_loss: 0.3112\n","Epoch 49/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8334\n","Epoch 49: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.8317 - val_loss: 0.2685\n","Epoch 50/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.7905\n","Epoch 50: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.7881 - val_loss: 0.8970\n","Epoch 51/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8145\n","Epoch 51: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.8135 - val_loss: 1.0327\n","Epoch 52/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.9312\n","Epoch 52: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.9369 - val_loss: 0.1992\n","Epoch 53/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.9153\n","Epoch 53: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.9189 - val_loss: 0.1716\n","Epoch 54/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8026\n","Epoch 54: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 12ms/step - loss: 0.8000 - val_loss: 0.5096\n","Epoch 55/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8001\n","Epoch 55: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.8017 - val_loss: 0.2730\n","Epoch 56/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.9098\n","Epoch 56: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.9055 - val_loss: 0.3529\n","Epoch 57/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8029\n","Epoch 57: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.8035 - val_loss: 0.2956\n","Epoch 58/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8130\n","Epoch 58: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.8105 - val_loss: 0.4885\n","Epoch 59/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8920\n","Epoch 59: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.8838 - val_loss: 0.5888\n","Epoch 60/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.1423\n","Epoch 60: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 1.1286 - val_loss: 0.5409\n","Epoch 61/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.9964\n","Epoch 61: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.9954 - val_loss: 0.5578\n","Epoch 62/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.3093\n","Epoch 62: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 1.3350 - val_loss: 0.2678\n","Epoch 63/1000\n","71/72 [============================>.] - ETA: 0s - loss: 3.6695\n","Epoch 63: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 3.6174 - val_loss: 0.4216\n","Epoch 64/1000\n","71/72 [============================>.] - ETA: 0s - loss: 5.0286\n","Epoch 64: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 5.2490 - val_loss: 8.9157\n","Epoch 65/1000\n","71/72 [============================>.] - ETA: 0s - loss: 5.1275\n","Epoch 65: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 5.1503 - val_loss: 2.6482\n","Epoch 66/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.1118\n","Epoch 66: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 2.0866 - val_loss: 5.1705\n","Epoch 67/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.8145\n","Epoch 67: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 1.8644 - val_loss: 2.7023\n","Epoch 68/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.5989\n","Epoch 68: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 1.6044 - val_loss: 1.3089\n","Epoch 69/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.0544\n","Epoch 69: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 1.0505 - val_loss: 1.5483\n","Epoch 70/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.1028\n","Epoch 70: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 1.0902 - val_loss: 0.4894\n","Epoch 71/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8905\n","Epoch 71: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 12ms/step - loss: 0.8835 - val_loss: 0.8648\n","Epoch 72/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8586\n","Epoch 72: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.8542 - val_loss: 0.3271\n","Epoch 73/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.9577\n","Epoch 73: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.9611 - val_loss: 0.6399\n","Epoch 74/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8796\n","Epoch 74: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.8834 - val_loss: 0.4862\n","Epoch 75/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8959\n","Epoch 75: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.8909 - val_loss: 0.3645\n","Epoch 76/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.6793\n","Epoch 76: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.6808 - val_loss: 0.7350\n","Epoch 77/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.6817\n","Epoch 77: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.6763 - val_loss: 0.9313\n","Epoch 78/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8137\n","Epoch 78: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.8070 - val_loss: 0.6423\n","Epoch 79/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.7457\n","Epoch 79: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.7473 - val_loss: 0.6501\n","Epoch 80/1000\n","72/72 [============================>.] - ETA: 0s - loss: 0.7120\n","Epoch 80: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.7087 - val_loss: 0.7846\n","Epoch 81/1000\n","72/72 [============================>.] - ETA: 0s - loss: 0.7657\n","Epoch 81: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.7641 - val_loss: 0.9370\n","Epoch 82/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.7574\n","Epoch 82: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.7529 - val_loss: 0.2114\n","Epoch 83/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.6983\n","Epoch 83: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.7061 - val_loss: 0.2163\n","Epoch 84/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.6961\n","Epoch 84: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.6893 - val_loss: 0.5483\n","Epoch 85/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.7313\n","Epoch 85: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.7289 - val_loss: 0.3091\n","Epoch 86/1000\n","72/72 [============================>.] - ETA: 0s - loss: 0.8348\n","Epoch 86: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.8313 - val_loss: 0.4390\n","Epoch 87/1000\n","72/72 [============================>.] - ETA: 0s - loss: 0.7786\n","Epoch 87: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.7745 - val_loss: 0.2815\n","Epoch 88/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.7447\n","Epoch 88: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.7444 - val_loss: 0.4536\n","Epoch 89/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.7549\n","Epoch 89: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.7479 - val_loss: 0.2816\n","Epoch 90/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.7681\n","Epoch 90: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.7623 - val_loss: 0.4452\n","Epoch 91/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8547\n","Epoch 91: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.8583 - val_loss: 0.7923\n","Epoch 92/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.6874\n","Epoch 92: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 12ms/step - loss: 0.6881 - val_loss: 0.8626\n","Epoch 93/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.7987\n","Epoch 93: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.7898 - val_loss: 2.3457\n","Epoch 94/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8410\n","Epoch 94: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.8325 - val_loss: 1.2960\n","Epoch 95/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.7396\n","Epoch 95: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.7334 - val_loss: 0.4904\n","Epoch 96/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.7738\n","Epoch 96: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.7659 - val_loss: 1.2403\n","Epoch 97/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.7302\n","Epoch 97: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 12ms/step - loss: 0.7299 - val_loss: 0.1961\n","Epoch 98/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.7573\n","Epoch 98: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 12ms/step - loss: 0.7508 - val_loss: 0.4255\n","Epoch 99/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.0939\n","Epoch 99: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 1.0804 - val_loss: 0.3371\n","Epoch 100/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.2509\n","Epoch 100: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 1.2401 - val_loss: 1.4971\n","Epoch 101/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.6123\n","Epoch 101: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 1.5913 - val_loss: 2.3276\n","Epoch 102/1000\n","71/72 [============================>.] - ETA: 0s - loss: 6.5551\n","Epoch 102: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 6.4281 - val_loss: 4.3040\n","Epoch 103/1000\n","71/72 [============================>.] - ETA: 0s - loss: 3.9654\n","Epoch 103: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 3.9891 - val_loss: 10.1884\n","Epoch 104/1000\n","72/72 [============================>.] - ETA: 0s - loss: 2.8738\n","Epoch 104: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 2.8651 - val_loss: 2.7321\n","Epoch 105/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.2487\n","Epoch 105: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 2.2143 - val_loss: 2.8459\n","Epoch 106/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.3822\n","Epoch 106: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 1.3667 - val_loss: 0.5125\n","Epoch 107/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.9087\n","Epoch 107: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 12ms/step - loss: 0.9018 - val_loss: 0.5320\n","Epoch 108/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.7027\n","Epoch 108: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 12ms/step - loss: 0.6970 - val_loss: 0.2276\n","Epoch 109/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.7133\n","Epoch 109: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.7219 - val_loss: 0.3365\n","Epoch 110/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.6641\n","Epoch 110: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.6575 - val_loss: 0.3251\n","Epoch 111/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.6529\n","Epoch 111: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.6497 - val_loss: 0.1960\n","Epoch 112/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.6311\n","Epoch 112: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.6318 - val_loss: 0.3836\n","Epoch 113/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.6589\n","Epoch 113: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.6570 - val_loss: 0.2883\n","Epoch 114/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.9247\n","Epoch 114: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.9156 - val_loss: 1.0329\n","Epoch 115/1000\n","69/72 [===========================>..] - ETA: 0s - loss: 0.7822\n","Epoch 115: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 12ms/step - loss: 0.7645 - val_loss: 0.1807\n","Epoch 116/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.7105\n","Epoch 116: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.7078 - val_loss: 0.6077\n","Epoch 117/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5518\n","Epoch 117: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5480 - val_loss: 0.3463\n","Epoch 118/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5866\n","Epoch 118: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5831 - val_loss: 0.3772\n","Epoch 119/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5897\n","Epoch 119: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5900 - val_loss: 0.2575\n","Epoch 120/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.6609\n","Epoch 120: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.6553 - val_loss: 0.2615\n","Epoch 121/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.6248\n","Epoch 121: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.6183 - val_loss: 0.2236\n","Epoch 122/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8397\n","Epoch 122: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.8295 - val_loss: 0.7972\n","Epoch 123/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.6548\n","Epoch 123: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.6521 - val_loss: 0.4068\n","Epoch 124/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5227\n","Epoch 124: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5252 - val_loss: 0.3274\n","Epoch 125/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5479\n","Epoch 125: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5448 - val_loss: 0.3688\n","Epoch 126/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5105\n","Epoch 126: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5080 - val_loss: 0.4416\n","Epoch 127/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5084\n","Epoch 127: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5070 - val_loss: 0.3768\n","Epoch 128/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4836\n","Epoch 128: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4817 - val_loss: 0.7863\n","Epoch 129/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5006\n","Epoch 129: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4967 - val_loss: 0.4781\n","Epoch 130/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4626\n","Epoch 130: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4601 - val_loss: 0.4963\n","Epoch 131/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.6844\n","Epoch 131: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.6963 - val_loss: 0.8301\n","Epoch 132/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.7211\n","Epoch 132: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.7138 - val_loss: 0.6961\n","Epoch 133/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8236\n","Epoch 133: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.8169 - val_loss: 0.6099\n","Epoch 134/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.6073\n","Epoch 134: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.6041 - val_loss: 0.1882\n","Epoch 135/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5536\n","Epoch 135: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5585 - val_loss: 0.6702\n","Epoch 136/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.6014\n","Epoch 136: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5980 - val_loss: 0.2412\n","Epoch 137/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4967\n","Epoch 137: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4948 - val_loss: 0.3221\n","Epoch 138/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5265\n","Epoch 138: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 12ms/step - loss: 0.5241 - val_loss: 0.1934\n","Epoch 139/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5812\n","Epoch 139: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5769 - val_loss: 0.9022\n","Epoch 140/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5005\n","Epoch 140: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4955 - val_loss: 0.5164\n","Epoch 141/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4502\n","Epoch 141: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4474 - val_loss: 0.3462\n","Epoch 142/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4459\n","Epoch 142: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4452 - val_loss: 0.2072\n","Epoch 143/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 0.4527\n","Epoch 143: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 12ms/step - loss: 0.4507 - val_loss: 0.4008\n","Epoch 144/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5121\n","Epoch 144: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 12ms/step - loss: 0.5066 - val_loss: 0.6736\n","Epoch 145/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 0.4037\n","Epoch 145: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 12ms/step - loss: 0.4035 - val_loss: 0.1863\n","Epoch 146/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5591\n","Epoch 146: val_loss did not improve from 0.15799\n","72/72 [==============================] - 1s 12ms/step - loss: 0.5526 - val_loss: 0.3176\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1238/1238 [00:00<00:00, 36165.35it/s]\n","100%|██████████| 1210/1210 [00:00<00:00, 34456.80it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/1000\n","71/72 [============================>.] - ETA: 0s - loss: 26.7429\n","Epoch 1: val_loss improved from inf to 0.63416, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-35-1000-15.h5\n","72/72 [==============================] - 4s 18ms/step - loss: 26.3219 - val_loss: 0.6342\n","Epoch 2/1000\n","71/72 [============================>.] - ETA: 0s - loss: 6.1228\n","Epoch 2: val_loss did not improve from 0.63416\n","72/72 [==============================] - 1s 11ms/step - loss: 6.0782 - val_loss: 0.7561\n","Epoch 3/1000\n","71/72 [============================>.] - ETA: 0s - loss: 5.6005\n","Epoch 3: val_loss did not improve from 0.63416\n","72/72 [==============================] - 1s 12ms/step - loss: 5.5734 - val_loss: 2.1305\n","Epoch 4/1000\n","71/72 [============================>.] - ETA: 0s - loss: 4.9084\n","Epoch 4: val_loss improved from 0.63416 to 0.52989, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-35-1000-15.h5\n","72/72 [==============================] - 1s 13ms/step - loss: 4.9619 - val_loss: 0.5299\n","Epoch 5/1000\n","71/72 [============================>.] - ETA: 0s - loss: 4.0415\n","Epoch 5: val_loss did not improve from 0.52989\n","72/72 [==============================] - 1s 11ms/step - loss: 4.0333 - val_loss: 2.2909\n","Epoch 6/1000\n","71/72 [============================>.] - ETA: 0s - loss: 3.1936\n","Epoch 6: val_loss improved from 0.52989 to 0.52450, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-35-1000-15.h5\n","72/72 [==============================] - 1s 12ms/step - loss: 3.2694 - val_loss: 0.5245\n","Epoch 7/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 3.1037\n","Epoch 7: val_loss did not improve from 0.52450\n","72/72 [==============================] - 1s 11ms/step - loss: 3.1117 - val_loss: 1.4158\n","Epoch 8/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.3920\n","Epoch 8: val_loss did not improve from 0.52450\n","72/72 [==============================] - 1s 11ms/step - loss: 2.4074 - val_loss: 0.7850\n","Epoch 9/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.5844\n","Epoch 9: val_loss did not improve from 0.52450\n","72/72 [==============================] - 1s 11ms/step - loss: 2.6881 - val_loss: 2.5575\n","Epoch 10/1000\n","71/72 [============================>.] - ETA: 0s - loss: 3.4496\n","Epoch 10: val_loss did not improve from 0.52450\n","72/72 [==============================] - 1s 11ms/step - loss: 3.4788 - val_loss: 1.4449\n","Epoch 11/1000\n","71/72 [============================>.] - ETA: 0s - loss: 6.3390\n","Epoch 11: val_loss did not improve from 0.52450\n","72/72 [==============================] - 1s 11ms/step - loss: 6.3961 - val_loss: 1.8942\n","Epoch 12/1000\n","71/72 [============================>.] - ETA: 0s - loss: 3.6320\n","Epoch 12: val_loss improved from 0.52450 to 0.23900, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-35-1000-15.h5\n","72/72 [==============================] - 1s 12ms/step - loss: 3.5865 - val_loss: 0.2390\n","Epoch 13/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.8101\n","Epoch 13: val_loss did not improve from 0.23900\n","72/72 [==============================] - 1s 11ms/step - loss: 1.8294 - val_loss: 0.3059\n","Epoch 14/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.6848\n","Epoch 14: val_loss improved from 0.23900 to 0.19550, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-35-1000-15.h5\n","72/72 [==============================] - 1s 13ms/step - loss: 1.6724 - val_loss: 0.1955\n","Epoch 15/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.4811\n","Epoch 15: val_loss did not improve from 0.19550\n","72/72 [==============================] - 1s 12ms/step - loss: 1.4792 - val_loss: 0.3032\n","Epoch 16/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.5053\n","Epoch 16: val_loss did not improve from 0.19550\n","72/72 [==============================] - 1s 11ms/step - loss: 1.4980 - val_loss: 0.2875\n","Epoch 17/1000\n","72/72 [============================>.] - ETA: 0s - loss: 1.4997\n","Epoch 17: val_loss did not improve from 0.19550\n","72/72 [==============================] - 1s 11ms/step - loss: 1.4950 - val_loss: 0.2290\n","Epoch 18/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.8403\n","Epoch 18: val_loss did not improve from 0.19550\n","72/72 [==============================] - 1s 11ms/step - loss: 1.8756 - val_loss: 0.9096\n","Epoch 19/1000\n","71/72 [============================>.] - ETA: 0s - loss: 3.4915\n","Epoch 19: val_loss did not improve from 0.19550\n","72/72 [==============================] - 1s 11ms/step - loss: 3.4895 - val_loss: 0.5227\n","Epoch 20/1000\n","71/72 [============================>.] - ETA: 0s - loss: 3.1316\n","Epoch 20: val_loss did not improve from 0.19550\n","72/72 [==============================] - 1s 11ms/step - loss: 3.1519 - val_loss: 0.3292\n","Epoch 21/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.8747\n","Epoch 21: val_loss improved from 0.19550 to 0.18105, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-35-1000-15.h5\n","72/72 [==============================] - 1s 12ms/step - loss: 1.8622 - val_loss: 0.1811\n","Epoch 22/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.5571\n","Epoch 22: val_loss did not improve from 0.18105\n","72/72 [==============================] - 1s 11ms/step - loss: 1.5826 - val_loss: 0.4934\n","Epoch 23/1000\n","71/72 [============================>.] - ETA: 0s - loss: 3.3473\n","Epoch 23: val_loss did not improve from 0.18105\n","72/72 [==============================] - 1s 11ms/step - loss: 3.2955 - val_loss: 0.2684\n","Epoch 24/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.3582\n","Epoch 24: val_loss did not improve from 0.18105\n","72/72 [==============================] - 1s 11ms/step - loss: 2.3715 - val_loss: 0.3807\n","Epoch 25/1000\n","71/72 [============================>.] - ETA: 0s - loss: 3.4632\n","Epoch 25: val_loss did not improve from 0.18105\n","72/72 [==============================] - 1s 11ms/step - loss: 3.4173 - val_loss: 0.2356\n","Epoch 26/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.7805\n","Epoch 26: val_loss did not improve from 0.18105\n","72/72 [==============================] - 1s 11ms/step - loss: 1.7953 - val_loss: 0.7467\n","Epoch 27/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.2552\n","Epoch 27: val_loss improved from 0.18105 to 0.09281, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-35-1000-15.h5\n","72/72 [==============================] - 1s 12ms/step - loss: 1.2418 - val_loss: 0.0928\n","Epoch 28/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.1526\n","Epoch 28: val_loss did not improve from 0.09281\n","72/72 [==============================] - 1s 11ms/step - loss: 1.1616 - val_loss: 0.7331\n","Epoch 29/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.1558\n","Epoch 29: val_loss did not improve from 0.09281\n","72/72 [==============================] - 1s 11ms/step - loss: 1.1466 - val_loss: 0.1541\n","Epoch 30/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.0706\n","Epoch 30: val_loss did not improve from 0.09281\n","72/72 [==============================] - 1s 11ms/step - loss: 1.0792 - val_loss: 0.4150\n","Epoch 31/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.1073\n","Epoch 31: val_loss did not improve from 0.09281\n","72/72 [==============================] - 1s 11ms/step - loss: 1.1055 - val_loss: 0.3052\n","Epoch 32/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.2950\n","Epoch 32: val_loss did not improve from 0.09281\n","72/72 [==============================] - 1s 11ms/step - loss: 1.3061 - val_loss: 1.2372\n","Epoch 33/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 1.1223\n","Epoch 33: val_loss did not improve from 0.09281\n","72/72 [==============================] - 1s 12ms/step - loss: 1.1237 - val_loss: 0.9596\n","Epoch 34/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.0648\n","Epoch 34: val_loss did not improve from 0.09281\n","72/72 [==============================] - 1s 11ms/step - loss: 1.0684 - val_loss: 0.2317\n","Epoch 35/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.3144\n","Epoch 35: val_loss did not improve from 0.09281\n","72/72 [==============================] - 1s 11ms/step - loss: 1.3601 - val_loss: 0.6969\n","Epoch 36/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.9843\n","Epoch 36: val_loss did not improve from 0.09281\n","72/72 [==============================] - 1s 11ms/step - loss: 1.9953 - val_loss: 1.4593\n","Epoch 37/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 2.6184\n","Epoch 37: val_loss did not improve from 0.09281\n","72/72 [==============================] - 1s 11ms/step - loss: 2.6340 - val_loss: 1.1712\n","Epoch 38/1000\n","71/72 [============================>.] - ETA: 0s - loss: 3.3712\n","Epoch 38: val_loss did not improve from 0.09281\n","72/72 [==============================] - 1s 11ms/step - loss: 3.3142 - val_loss: 0.2510\n","Epoch 39/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.0667\n","Epoch 39: val_loss did not improve from 0.09281\n","72/72 [==============================] - 1s 11ms/step - loss: 1.0646 - val_loss: 0.3349\n","Epoch 40/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.1011\n","Epoch 40: val_loss did not improve from 0.09281\n","72/72 [==============================] - 1s 11ms/step - loss: 1.1008 - val_loss: 0.2107\n","Epoch 41/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.2192\n","Epoch 41: val_loss did not improve from 0.09281\n","72/72 [==============================] - 1s 11ms/step - loss: 1.2116 - val_loss: 0.3749\n","Epoch 42/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.9526\n","Epoch 42: val_loss did not improve from 0.09281\n","72/72 [==============================] - 1s 11ms/step - loss: 0.9596 - val_loss: 0.2470\n","Epoch 43/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.2351\n","Epoch 43: val_loss improved from 0.09281 to 0.07487, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-35-1000-15.h5\n","72/72 [==============================] - 1s 16ms/step - loss: 1.2275 - val_loss: 0.0749\n","Epoch 44/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.9908\n","Epoch 44: val_loss did not improve from 0.07487\n","72/72 [==============================] - 1s 11ms/step - loss: 0.9975 - val_loss: 0.2719\n","Epoch 45/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.4214\n","Epoch 45: val_loss did not improve from 0.07487\n","72/72 [==============================] - 1s 11ms/step - loss: 1.4056 - val_loss: 0.5033\n","Epoch 46/1000\n","71/72 [============================>.] - ETA: 0s - loss: 3.0301\n","Epoch 46: val_loss did not improve from 0.07487\n","72/72 [==============================] - 1s 11ms/step - loss: 3.0005 - val_loss: 1.2243\n","Epoch 47/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.2042\n","Epoch 47: val_loss did not improve from 0.07487\n","72/72 [==============================] - 1s 11ms/step - loss: 1.2167 - val_loss: 1.0992\n","Epoch 48/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.3228\n","Epoch 48: val_loss did not improve from 0.07487\n","72/72 [==============================] - 1s 11ms/step - loss: 1.3301 - val_loss: 0.7885\n","Epoch 49/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.8508\n","Epoch 49: val_loss did not improve from 0.07487\n","72/72 [==============================] - 1s 11ms/step - loss: 1.8416 - val_loss: 2.8110\n","Epoch 50/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.1096\n","Epoch 50: val_loss did not improve from 0.07487\n","72/72 [==============================] - 1s 11ms/step - loss: 2.0937 - val_loss: 1.9568\n","Epoch 51/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.8927\n","Epoch 51: val_loss did not improve from 0.07487\n","72/72 [==============================] - 1s 12ms/step - loss: 1.8838 - val_loss: 1.8973\n","Epoch 52/1000\n","71/72 [============================>.] - ETA: 0s - loss: 3.4822\n","Epoch 52: val_loss did not improve from 0.07487\n","72/72 [==============================] - 1s 12ms/step - loss: 3.4455 - val_loss: 2.3177\n","Epoch 53/1000\n","71/72 [============================>.] - ETA: 0s - loss: 4.3810\n","Epoch 53: val_loss did not improve from 0.07487\n","72/72 [==============================] - 1s 11ms/step - loss: 4.4120 - val_loss: 1.9595\n","Epoch 54/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.6213\n","Epoch 54: val_loss did not improve from 0.07487\n","72/72 [==============================] - 1s 11ms/step - loss: 2.5924 - val_loss: 0.7794\n","Epoch 55/1000\n","71/72 [============================>.] - ETA: 0s - loss: 3.4881\n","Epoch 55: val_loss did not improve from 0.07487\n","72/72 [==============================] - 1s 11ms/step - loss: 3.4238 - val_loss: 1.6620\n","Epoch 56/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.6661\n","Epoch 56: val_loss did not improve from 0.07487\n","72/72 [==============================] - 1s 11ms/step - loss: 1.6479 - val_loss: 0.4188\n","Epoch 57/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.1381\n","Epoch 57: val_loss did not improve from 0.07487\n","72/72 [==============================] - 1s 11ms/step - loss: 1.1393 - val_loss: 0.6688\n","Epoch 58/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.1181\n","Epoch 58: val_loss did not improve from 0.07487\n","72/72 [==============================] - 1s 11ms/step - loss: 1.1087 - val_loss: 0.2792\n","Epoch 59/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.1043\n","Epoch 59: val_loss did not improve from 0.07487\n","72/72 [==============================] - 1s 11ms/step - loss: 1.1098 - val_loss: 0.1595\n","Epoch 60/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8795\n","Epoch 60: val_loss did not improve from 0.07487\n","72/72 [==============================] - 1s 11ms/step - loss: 0.8797 - val_loss: 0.2232\n","Epoch 61/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8218\n","Epoch 61: val_loss did not improve from 0.07487\n","72/72 [==============================] - 1s 11ms/step - loss: 0.8294 - val_loss: 0.7345\n","Epoch 62/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.9690\n","Epoch 62: val_loss did not improve from 0.07487\n","72/72 [==============================] - 1s 12ms/step - loss: 0.9727 - val_loss: 0.5459\n","Epoch 63/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8856\n","Epoch 63: val_loss did not improve from 0.07487\n","72/72 [==============================] - 1s 11ms/step - loss: 0.8905 - val_loss: 1.1371\n","Epoch 64/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.5033\n","Epoch 64: val_loss did not improve from 0.07487\n","72/72 [==============================] - 1s 11ms/step - loss: 1.5053 - val_loss: 0.9129\n","Epoch 65/1000\n","71/72 [============================>.] - ETA: 0s - loss: 4.1735\n","Epoch 65: val_loss did not improve from 0.07487\n","72/72 [==============================] - 1s 12ms/step - loss: 4.2244 - val_loss: 9.7254\n","Epoch 66/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.2104\n","Epoch 66: val_loss did not improve from 0.07487\n","72/72 [==============================] - 1s 11ms/step - loss: 2.1898 - val_loss: 1.0079\n","Epoch 67/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 1.5789\n","Epoch 67: val_loss did not improve from 0.07487\n","72/72 [==============================] - 1s 12ms/step - loss: 1.5804 - val_loss: 0.5408\n","Epoch 68/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.2006\n","Epoch 68: val_loss did not improve from 0.07487\n","72/72 [==============================] - 1s 11ms/step - loss: 1.2256 - val_loss: 2.4713\n","Epoch 69/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.4606\n","Epoch 69: val_loss did not improve from 0.07487\n","72/72 [==============================] - 1s 11ms/step - loss: 1.4605 - val_loss: 3.0788\n","Epoch 70/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.9312\n","Epoch 70: val_loss did not improve from 0.07487\n","72/72 [==============================] - 1s 11ms/step - loss: 0.9290 - val_loss: 0.8859\n","Epoch 71/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8276\n","Epoch 71: val_loss did not improve from 0.07487\n","72/72 [==============================] - 1s 11ms/step - loss: 0.8393 - val_loss: 0.4689\n","Epoch 72/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.7331\n","Epoch 72: val_loss did not improve from 0.07487\n","72/72 [==============================] - 1s 11ms/step - loss: 0.7378 - val_loss: 0.5178\n","Epoch 73/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.6422\n","Epoch 73: val_loss did not improve from 0.07487\n","72/72 [==============================] - 1s 12ms/step - loss: 0.6432 - val_loss: 0.5352\n","Epoch 74/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5983\n","Epoch 74: val_loss did not improve from 0.07487\n","72/72 [==============================] - 1s 11ms/step - loss: 0.6015 - val_loss: 0.5770\n","Epoch 75/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5346\n","Epoch 75: val_loss did not improve from 0.07487\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5395 - val_loss: 0.2276\n","Epoch 76/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4664\n","Epoch 76: val_loss did not improve from 0.07487\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4855 - val_loss: 0.1947\n","Epoch 77/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4491\n","Epoch 77: val_loss did not improve from 0.07487\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4683 - val_loss: 0.0898\n","Epoch 78/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3984\n","Epoch 78: val_loss improved from 0.07487 to 0.06464, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-35-1000-15.h5\n","72/72 [==============================] - 1s 17ms/step - loss: 0.4013 - val_loss: 0.0646\n","Epoch 79/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4047\n","Epoch 79: val_loss did not improve from 0.06464\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4059 - val_loss: 0.0657\n","Epoch 80/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4004\n","Epoch 80: val_loss did not improve from 0.06464\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4030 - val_loss: 0.0765\n","Epoch 81/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3840\n","Epoch 81: val_loss did not improve from 0.06464\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3861 - val_loss: 0.0651\n","Epoch 82/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3693\n","Epoch 82: val_loss improved from 0.06464 to 0.05651, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-35-1000-15.h5\n","72/72 [==============================] - 1s 13ms/step - loss: 0.3721 - val_loss: 0.0565\n","Epoch 83/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3536\n","Epoch 83: val_loss did not improve from 0.05651\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3528 - val_loss: 0.0834\n","Epoch 84/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3464\n","Epoch 84: val_loss improved from 0.05651 to 0.05538, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-35-1000-15.h5\n","72/72 [==============================] - 1s 13ms/step - loss: 0.3464 - val_loss: 0.0554\n","Epoch 85/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3536\n","Epoch 85: val_loss improved from 0.05538 to 0.04605, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-35-1000-15.h5\n","72/72 [==============================] - 1s 13ms/step - loss: 0.3561 - val_loss: 0.0461\n","Epoch 86/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3329\n","Epoch 86: val_loss did not improve from 0.04605\n","72/72 [==============================] - 1s 12ms/step - loss: 0.3331 - val_loss: 0.0514\n","Epoch 87/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3329\n","Epoch 87: val_loss did not improve from 0.04605\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3350 - val_loss: 0.0513\n","Epoch 88/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3254\n","Epoch 88: val_loss improved from 0.04605 to 0.04590, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-35-1000-15.h5\n","72/72 [==============================] - 1s 13ms/step - loss: 0.3265 - val_loss: 0.0459\n","Epoch 89/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3125\n","Epoch 89: val_loss improved from 0.04590 to 0.04541, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-35-1000-15.h5\n","72/72 [==============================] - 1s 13ms/step - loss: 0.3121 - val_loss: 0.0454\n","Epoch 90/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3152\n","Epoch 90: val_loss did not improve from 0.04541\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3170 - val_loss: 0.0467\n","Epoch 91/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3299\n","Epoch 91: val_loss improved from 0.04541 to 0.04381, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-35-1000-15.h5\n","72/72 [==============================] - 1s 13ms/step - loss: 0.3299 - val_loss: 0.0438\n","Epoch 92/1000\n","72/72 [============================>.] - ETA: 0s - loss: 0.3132\n","Epoch 92: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3133 - val_loss: 0.0468\n","Epoch 93/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3337\n","Epoch 93: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3336 - val_loss: 0.0600\n","Epoch 94/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3200\n","Epoch 94: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3207 - val_loss: 0.0582\n","Epoch 95/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2874\n","Epoch 95: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2896 - val_loss: 0.0635\n","Epoch 96/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2915\n","Epoch 96: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2905 - val_loss: 0.0568\n","Epoch 97/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3013\n","Epoch 97: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3003 - val_loss: 0.0458\n","Epoch 98/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2969\n","Epoch 98: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2978 - val_loss: 0.0492\n","Epoch 99/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3251\n","Epoch 99: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3249 - val_loss: 0.0668\n","Epoch 100/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2862\n","Epoch 100: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2868 - val_loss: 0.0717\n","Epoch 101/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2909\n","Epoch 101: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2925 - val_loss: 0.0594\n","Epoch 102/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2995\n","Epoch 102: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3019 - val_loss: 0.0690\n","Epoch 103/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3001\n","Epoch 103: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3016 - val_loss: 0.0611\n","Epoch 104/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 0.2832\n","Epoch 104: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2841 - val_loss: 0.0537\n","Epoch 105/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2965\n","Epoch 105: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2951 - val_loss: 0.0615\n","Epoch 106/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3151\n","Epoch 106: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 12ms/step - loss: 0.3146 - val_loss: 0.0532\n","Epoch 107/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2823\n","Epoch 107: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2840 - val_loss: 0.0743\n","Epoch 108/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2732\n","Epoch 108: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2722 - val_loss: 0.0487\n","Epoch 109/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2707\n","Epoch 109: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2703 - val_loss: 0.0941\n","Epoch 110/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2656\n","Epoch 110: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2659 - val_loss: 0.1953\n","Epoch 111/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2917\n","Epoch 111: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2932 - val_loss: 0.0967\n","Epoch 112/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2935\n","Epoch 112: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2935 - val_loss: 0.0557\n","Epoch 113/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2748\n","Epoch 113: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2747 - val_loss: 0.0648\n","Epoch 114/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2971\n","Epoch 114: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2965 - val_loss: 0.1295\n","Epoch 115/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3120\n","Epoch 115: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3129 - val_loss: 0.0663\n","Epoch 116/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2743\n","Epoch 116: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2750 - val_loss: 0.0843\n","Epoch 117/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2808\n","Epoch 117: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2786 - val_loss: 0.0584\n","Epoch 118/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2692\n","Epoch 118: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2688 - val_loss: 0.0622\n","Epoch 119/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2899\n","Epoch 119: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2905 - val_loss: 0.0712\n","Epoch 120/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2592\n","Epoch 120: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2575 - val_loss: 0.1049\n","Epoch 121/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2747\n","Epoch 121: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2756 - val_loss: 0.1183\n","Epoch 122/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2644\n","Epoch 122: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2630 - val_loss: 0.1917\n","Epoch 123/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2837\n","Epoch 123: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2815 - val_loss: 0.1240\n","Epoch 124/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2604\n","Epoch 124: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2588 - val_loss: 0.1368\n","Epoch 125/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2740\n","Epoch 125: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2747 - val_loss: 0.1913\n","Epoch 126/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2863\n","Epoch 126: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2853 - val_loss: 0.0663\n","Epoch 127/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2658\n","Epoch 127: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2642 - val_loss: 0.0695\n","Epoch 128/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3061\n","Epoch 128: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3034 - val_loss: 0.4818\n","Epoch 129/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3617\n","Epoch 129: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3591 - val_loss: 0.0775\n","Epoch 130/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2846\n","Epoch 130: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2840 - val_loss: 0.0699\n","Epoch 131/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2808\n","Epoch 131: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2784 - val_loss: 0.0657\n","Epoch 132/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2895\n","Epoch 132: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2885 - val_loss: 0.3847\n","Epoch 133/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3143\n","Epoch 133: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 12ms/step - loss: 0.3128 - val_loss: 0.1616\n","Epoch 134/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2974\n","Epoch 134: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2960 - val_loss: 0.2269\n","Epoch 135/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3191\n","Epoch 135: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3183 - val_loss: 0.0825\n","Epoch 136/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3021\n","Epoch 136: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2994 - val_loss: 0.1628\n","Epoch 137/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2661\n","Epoch 137: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2660 - val_loss: 0.2014\n","Epoch 138/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 0.2611\n","Epoch 138: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2670 - val_loss: 0.0815\n","Epoch 139/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2782\n","Epoch 139: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2770 - val_loss: 0.0668\n","Epoch 140/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3487\n","Epoch 140: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3440 - val_loss: 0.2146\n","Epoch 141/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2846\n","Epoch 141: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2827 - val_loss: 0.0472\n","Epoch 142/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2828\n","Epoch 142: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2857 - val_loss: 0.0539\n","Epoch 143/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2962\n","Epoch 143: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2964 - val_loss: 0.1625\n","Epoch 144/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2586\n","Epoch 144: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2576 - val_loss: 0.2004\n","Epoch 145/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3030\n","Epoch 145: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3017 - val_loss: 0.1259\n","Epoch 146/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2775\n","Epoch 146: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2759 - val_loss: 0.1057\n","Epoch 147/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2717\n","Epoch 147: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2698 - val_loss: 0.0585\n","Epoch 148/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2972\n","Epoch 148: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2958 - val_loss: 0.1953\n","Epoch 149/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2764\n","Epoch 149: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2751 - val_loss: 0.1036\n","Epoch 150/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2556\n","Epoch 150: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2536 - val_loss: 0.0994\n","Epoch 151/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2671\n","Epoch 151: val_loss did not improve from 0.04381\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2644 - val_loss: 0.0889\n","Epoch 152/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2747\n","Epoch 152: val_loss improved from 0.04381 to 0.04270, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-35-1000-15.h5\n","72/72 [==============================] - 1s 16ms/step - loss: 0.2743 - val_loss: 0.0427\n","Epoch 153/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2655\n","Epoch 153: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2630 - val_loss: 0.0856\n","Epoch 154/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2538\n","Epoch 154: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2525 - val_loss: 0.1900\n","Epoch 155/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2762\n","Epoch 155: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2749 - val_loss: 0.0621\n","Epoch 156/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2589\n","Epoch 156: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2577 - val_loss: 0.0856\n","Epoch 157/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 0.3163\n","Epoch 157: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3121 - val_loss: 0.1168\n","Epoch 158/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2903\n","Epoch 158: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2889 - val_loss: 0.1189\n","Epoch 159/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2834\n","Epoch 159: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2820 - val_loss: 0.0606\n","Epoch 160/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3062\n","Epoch 160: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3057 - val_loss: 0.0592\n","Epoch 161/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2933\n","Epoch 161: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2909 - val_loss: 0.0628\n","Epoch 162/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2508\n","Epoch 162: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2512 - val_loss: 0.0878\n","Epoch 163/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2458\n","Epoch 163: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2438 - val_loss: 0.1594\n","Epoch 164/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2628\n","Epoch 164: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2613 - val_loss: 0.3679\n","Epoch 165/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3405\n","Epoch 165: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3400 - val_loss: 0.0885\n","Epoch 166/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2589\n","Epoch 166: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2569 - val_loss: 0.0845\n","Epoch 167/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3310\n","Epoch 167: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 12ms/step - loss: 0.3286 - val_loss: 0.1562\n","Epoch 168/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2631\n","Epoch 168: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2601 - val_loss: 0.1666\n","Epoch 169/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2198\n","Epoch 169: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2168 - val_loss: 0.1101\n","Epoch 170/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2462\n","Epoch 170: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2461 - val_loss: 0.0638\n","Epoch 171/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2473\n","Epoch 171: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2439 - val_loss: 0.3120\n","Epoch 172/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2519\n","Epoch 172: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2518 - val_loss: 0.0530\n","Epoch 173/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2738\n","Epoch 173: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2713 - val_loss: 0.0813\n","Epoch 174/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2962\n","Epoch 174: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2965 - val_loss: 0.1490\n","Epoch 175/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2845\n","Epoch 175: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2821 - val_loss: 0.2086\n","Epoch 176/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2495\n","Epoch 176: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2486 - val_loss: 0.0720\n","Epoch 177/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2202\n","Epoch 177: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2191 - val_loss: 0.1076\n","Epoch 178/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2454\n","Epoch 178: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2450 - val_loss: 0.1069\n","Epoch 179/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2402\n","Epoch 179: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2377 - val_loss: 0.1473\n","Epoch 180/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2462\n","Epoch 180: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2453 - val_loss: 0.0817\n","Epoch 181/1000\n","73/72 [==============================] - ETA: 0s - loss: 0.2204\n","Epoch 181: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2204 - val_loss: 0.0678\n","Epoch 182/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2809\n","Epoch 182: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2808 - val_loss: 0.0849\n","Epoch 183/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 0.2788\n","Epoch 183: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2818 - val_loss: 0.1477\n","Epoch 184/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2797\n","Epoch 184: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2785 - val_loss: 0.1042\n","Epoch 185/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2396\n","Epoch 185: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2443 - val_loss: 0.1735\n","Epoch 186/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2476\n","Epoch 186: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2461 - val_loss: 0.0812\n","Epoch 187/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3065\n","Epoch 187: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3059 - val_loss: 0.1288\n","Epoch 188/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 0.3442\n","Epoch 188: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3395 - val_loss: 0.4431\n","Epoch 189/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2625\n","Epoch 189: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2612 - val_loss: 0.0975\n","Epoch 190/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2541\n","Epoch 190: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2529 - val_loss: 0.0920\n","Epoch 191/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 0.2388\n","Epoch 191: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2388 - val_loss: 0.1427\n","Epoch 192/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2242\n","Epoch 192: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2257 - val_loss: 0.2138\n","Epoch 193/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2236\n","Epoch 193: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2225 - val_loss: 0.0852\n","Epoch 194/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 0.2176\n","Epoch 194: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2163 - val_loss: 0.1698\n","Epoch 195/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2407\n","Epoch 195: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2398 - val_loss: 0.1196\n","Epoch 196/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2306\n","Epoch 196: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2297 - val_loss: 0.0994\n","Epoch 197/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2115\n","Epoch 197: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2092 - val_loss: 0.0808\n","Epoch 198/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 0.2263\n","Epoch 198: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2244 - val_loss: 0.0623\n","Epoch 199/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2300\n","Epoch 199: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2283 - val_loss: 0.1514\n","Epoch 200/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2420\n","Epoch 200: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2416 - val_loss: 0.1126\n","Epoch 201/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2457\n","Epoch 201: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2454 - val_loss: 0.1683\n","Epoch 202/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2624\n","Epoch 202: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2598 - val_loss: 0.1605\n","Epoch 203/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2262\n","Epoch 203: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2264 - val_loss: 0.0549\n","Epoch 204/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2359\n","Epoch 204: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2352 - val_loss: 0.2746\n","Epoch 205/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2272\n","Epoch 205: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2247 - val_loss: 0.0815\n","Epoch 206/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2288\n","Epoch 206: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2281 - val_loss: 0.1396\n","Epoch 207/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3163\n","Epoch 207: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 12ms/step - loss: 0.3122 - val_loss: 0.1288\n","Epoch 208/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2746\n","Epoch 208: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2724 - val_loss: 0.3620\n","Epoch 209/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2335\n","Epoch 209: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2319 - val_loss: 0.1025\n","Epoch 210/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2457\n","Epoch 210: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2440 - val_loss: 0.1153\n","Epoch 211/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2506\n","Epoch 211: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2478 - val_loss: 0.0567\n","Epoch 212/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2347\n","Epoch 212: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2354 - val_loss: 0.1186\n","Epoch 213/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2238\n","Epoch 213: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2210 - val_loss: 0.1251\n","Epoch 214/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2320\n","Epoch 214: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2319 - val_loss: 0.1747\n","Epoch 215/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3124\n","Epoch 215: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3164 - val_loss: 0.9108\n","Epoch 216/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3856\n","Epoch 216: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 12ms/step - loss: 0.3792 - val_loss: 0.1358\n","Epoch 217/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3492\n","Epoch 217: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3476 - val_loss: 0.0455\n","Epoch 218/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2557\n","Epoch 218: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2531 - val_loss: 0.1081\n","Epoch 219/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2531\n","Epoch 219: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2521 - val_loss: 0.0891\n","Epoch 220/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2202\n","Epoch 220: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2202 - val_loss: 0.0687\n","Epoch 221/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2525\n","Epoch 221: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2512 - val_loss: 0.0879\n","Epoch 222/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2606\n","Epoch 222: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2595 - val_loss: 0.1285\n","Epoch 223/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2068\n","Epoch 223: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2063 - val_loss: 0.1913\n","Epoch 224/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2490\n","Epoch 224: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2486 - val_loss: 0.0951\n","Epoch 225/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2372\n","Epoch 225: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2361 - val_loss: 0.2098\n","Epoch 226/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2298\n","Epoch 226: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2288 - val_loss: 0.0899\n","Epoch 227/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2188\n","Epoch 227: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2172 - val_loss: 0.1151\n","Epoch 228/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2151\n","Epoch 228: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2152 - val_loss: 0.0864\n","Epoch 229/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2636\n","Epoch 229: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2601 - val_loss: 0.0992\n","Epoch 230/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2176\n","Epoch 230: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2154 - val_loss: 0.1301\n","Epoch 231/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2559\n","Epoch 231: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2540 - val_loss: 0.1745\n","Epoch 232/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2034\n","Epoch 232: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2032 - val_loss: 0.0711\n","Epoch 233/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2254\n","Epoch 233: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2229 - val_loss: 0.3233\n","Epoch 234/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2157\n","Epoch 234: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2151 - val_loss: 0.0646\n","Epoch 235/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2182\n","Epoch 235: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2157 - val_loss: 0.0661\n","Epoch 236/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 0.2156\n","Epoch 236: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2158 - val_loss: 0.1427\n","Epoch 237/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2122\n","Epoch 237: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2132 - val_loss: 0.1399\n","Epoch 238/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2084\n","Epoch 238: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2074 - val_loss: 0.1629\n","Epoch 239/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2182\n","Epoch 239: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2193 - val_loss: 0.0581\n","Epoch 240/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2001\n","Epoch 240: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2008 - val_loss: 0.1171\n","Epoch 241/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2121\n","Epoch 241: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2116 - val_loss: 0.0644\n","Epoch 242/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2260\n","Epoch 242: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2265 - val_loss: 0.0555\n","Epoch 243/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2340\n","Epoch 243: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2315 - val_loss: 0.0786\n","Epoch 244/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2382\n","Epoch 244: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2370 - val_loss: 0.1825\n","Epoch 245/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2211\n","Epoch 245: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2196 - val_loss: 0.0585\n","Epoch 246/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2126\n","Epoch 246: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2119 - val_loss: 0.1210\n","Epoch 247/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.1896\n","Epoch 247: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.1905 - val_loss: 0.0525\n","Epoch 248/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.1858\n","Epoch 248: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.1846 - val_loss: 0.0689\n","Epoch 249/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2208\n","Epoch 249: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2201 - val_loss: 0.2019\n","Epoch 250/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2265\n","Epoch 250: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2257 - val_loss: 0.1347\n","Epoch 251/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2109\n","Epoch 251: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2106 - val_loss: 0.1399\n","Epoch 252/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2334\n","Epoch 252: val_loss did not improve from 0.04270\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2321 - val_loss: 0.1687\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1238/1238 [00:00<00:00, 38024.21it/s]\n","100%|██████████| 1210/1210 [00:00<00:00, 36446.02it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/1000\n","71/72 [============================>.] - ETA: 0s - loss: 26.9686\n","Epoch 1: val_loss improved from inf to 0.58949, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-36-1000-15.h5\n","72/72 [==============================] - 4s 17ms/step - loss: 26.5483 - val_loss: 0.5895\n","Epoch 2/1000\n","71/72 [============================>.] - ETA: 0s - loss: 6.1169\n","Epoch 2: val_loss did not improve from 0.58949\n","72/72 [==============================] - 1s 11ms/step - loss: 6.1043 - val_loss: 1.3343\n","Epoch 3/1000\n","71/72 [============================>.] - ETA: 0s - loss: 4.2300\n","Epoch 3: val_loss did not improve from 0.58949\n","72/72 [==============================] - 1s 11ms/step - loss: 4.2274 - val_loss: 1.2112\n","Epoch 4/1000\n","71/72 [============================>.] - ETA: 0s - loss: 5.2382\n","Epoch 4: val_loss did not improve from 0.58949\n","72/72 [==============================] - 1s 11ms/step - loss: 5.1909 - val_loss: 2.3382\n","Epoch 5/1000\n","71/72 [============================>.] - ETA: 0s - loss: 7.2391\n","Epoch 5: val_loss did not improve from 0.58949\n","72/72 [==============================] - 1s 11ms/step - loss: 7.1664 - val_loss: 1.0277\n","Epoch 6/1000\n","73/72 [==============================] - ETA: 0s - loss: 4.2719\n","Epoch 6: val_loss improved from 0.58949 to 0.15569, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-36-1000-15.h5\n","72/72 [==============================] - 1s 12ms/step - loss: 4.2719 - val_loss: 0.1557\n","Epoch 7/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.5302\n","Epoch 7: val_loss did not improve from 0.15569\n","72/72 [==============================] - 1s 11ms/step - loss: 2.5138 - val_loss: 0.1586\n","Epoch 8/1000\n","72/72 [============================>.] - ETA: 0s - loss: 2.3377\n","Epoch 8: val_loss did not improve from 0.15569\n","72/72 [==============================] - 1s 11ms/step - loss: 2.3355 - val_loss: 0.1746\n","Epoch 9/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.5070\n","Epoch 9: val_loss did not improve from 0.15569\n","72/72 [==============================] - 1s 11ms/step - loss: 2.4916 - val_loss: 0.3093\n","Epoch 10/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.3411\n","Epoch 10: val_loss did not improve from 0.15569\n","72/72 [==============================] - 1s 11ms/step - loss: 2.3181 - val_loss: 0.4751\n","Epoch 11/1000\n","71/72 [============================>.] - ETA: 0s - loss: 3.7376\n","Epoch 11: val_loss did not improve from 0.15569\n","72/72 [==============================] - 1s 11ms/step - loss: 3.6928 - val_loss: 1.6353\n","Epoch 12/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.6389\n","Epoch 12: val_loss did not improve from 0.15569\n","72/72 [==============================] - 1s 11ms/step - loss: 2.6126 - val_loss: 0.3759\n","Epoch 13/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.4196\n","Epoch 13: val_loss did not improve from 0.15569\n","72/72 [==============================] - 1s 11ms/step - loss: 2.4080 - val_loss: 0.3332\n","Epoch 14/1000\n","69/72 [===========================>..] - ETA: 0s - loss: 2.5956\n","Epoch 14: val_loss did not improve from 0.15569\n","72/72 [==============================] - 1s 12ms/step - loss: 2.6261 - val_loss: 0.2152\n","Epoch 15/1000\n","71/72 [============================>.] - ETA: 0s - loss: 3.3054\n","Epoch 15: val_loss did not improve from 0.15569\n","72/72 [==============================] - 1s 11ms/step - loss: 3.5764 - val_loss: 2.2648\n","Epoch 16/1000\n","71/72 [============================>.] - ETA: 0s - loss: 8.3343\n","Epoch 16: val_loss did not improve from 0.15569\n","72/72 [==============================] - 1s 11ms/step - loss: 8.2161 - val_loss: 12.5002\n","Epoch 17/1000\n","71/72 [============================>.] - ETA: 0s - loss: 5.3497\n","Epoch 17: val_loss did not improve from 0.15569\n","72/72 [==============================] - 1s 11ms/step - loss: 5.3995 - val_loss: 6.0885\n","Epoch 18/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.3320\n","Epoch 18: val_loss did not improve from 0.15569\n","72/72 [==============================] - 1s 12ms/step - loss: 2.3461 - val_loss: 1.8868\n","Epoch 19/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 1.9420\n","Epoch 19: val_loss did not improve from 0.15569\n","72/72 [==============================] - 1s 12ms/step - loss: 1.9265 - val_loss: 1.9109\n","Epoch 20/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.8724\n","Epoch 20: val_loss did not improve from 0.15569\n","72/72 [==============================] - 1s 12ms/step - loss: 1.8507 - val_loss: 1.4194\n","Epoch 21/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.0118\n","Epoch 21: val_loss did not improve from 0.15569\n","72/72 [==============================] - 1s 11ms/step - loss: 1.9896 - val_loss: 0.4933\n","Epoch 22/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.6328\n","Epoch 22: val_loss did not improve from 0.15569\n","72/72 [==============================] - 1s 11ms/step - loss: 1.6343 - val_loss: 0.4138\n","Epoch 23/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 2.3298\n","Epoch 23: val_loss did not improve from 0.15569\n","72/72 [==============================] - 1s 12ms/step - loss: 2.3027 - val_loss: 2.3696\n","Epoch 24/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.6554\n","Epoch 24: val_loss did not improve from 0.15569\n","72/72 [==============================] - 1s 11ms/step - loss: 2.6364 - val_loss: 1.3962\n","Epoch 25/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.0139\n","Epoch 25: val_loss did not improve from 0.15569\n","72/72 [==============================] - 1s 11ms/step - loss: 1.9957 - val_loss: 0.2067\n","Epoch 26/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.9490\n","Epoch 26: val_loss did not improve from 0.15569\n","72/72 [==============================] - 1s 11ms/step - loss: 1.9642 - val_loss: 2.7404\n","Epoch 27/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.1735\n","Epoch 27: val_loss did not improve from 0.15569\n","72/72 [==============================] - 1s 11ms/step - loss: 2.1724 - val_loss: 1.0051\n","Epoch 28/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.7011\n","Epoch 28: val_loss did not improve from 0.15569\n","72/72 [==============================] - 1s 11ms/step - loss: 1.6831 - val_loss: 0.3006\n","Epoch 29/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 1.4685\n","Epoch 29: val_loss did not improve from 0.15569\n","72/72 [==============================] - 1s 11ms/step - loss: 1.4693 - val_loss: 0.8475\n","Epoch 30/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.5910\n","Epoch 30: val_loss did not improve from 0.15569\n","72/72 [==============================] - 1s 11ms/step - loss: 1.5815 - val_loss: 0.2729\n","Epoch 31/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.7285\n","Epoch 31: val_loss did not improve from 0.15569\n","72/72 [==============================] - 1s 11ms/step - loss: 1.7060 - val_loss: 0.4771\n","Epoch 32/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.5795\n","Epoch 32: val_loss did not improve from 0.15569\n","72/72 [==============================] - 1s 11ms/step - loss: 1.5739 - val_loss: 0.6540\n","Epoch 33/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.4520\n","Epoch 33: val_loss did not improve from 0.15569\n","72/72 [==============================] - 1s 11ms/step - loss: 1.4642 - val_loss: 0.4844\n","Epoch 34/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.3188\n","Epoch 34: val_loss did not improve from 0.15569\n","72/72 [==============================] - 1s 11ms/step - loss: 1.3160 - val_loss: 0.3224\n","Epoch 35/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.1559\n","Epoch 35: val_loss improved from 0.15569 to 0.15300, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-36-1000-15.h5\n","72/72 [==============================] - 1s 16ms/step - loss: 1.1507 - val_loss: 0.1530\n","Epoch 36/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.1769\n","Epoch 36: val_loss did not improve from 0.15300\n","72/72 [==============================] - 1s 11ms/step - loss: 1.1663 - val_loss: 0.4169\n","Epoch 37/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.9198\n","Epoch 37: val_loss did not improve from 0.15300\n","72/72 [==============================] - 1s 12ms/step - loss: 1.9140 - val_loss: 0.3667\n","Epoch 38/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.3662\n","Epoch 38: val_loss did not improve from 0.15300\n","72/72 [==============================] - 1s 11ms/step - loss: 1.3665 - val_loss: 0.4013\n","Epoch 39/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.4028\n","Epoch 39: val_loss did not improve from 0.15300\n","72/72 [==============================] - 1s 11ms/step - loss: 1.3898 - val_loss: 1.2879\n","Epoch 40/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.2676\n","Epoch 40: val_loss did not improve from 0.15300\n","72/72 [==============================] - 1s 11ms/step - loss: 1.2867 - val_loss: 0.7550\n","Epoch 41/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.6511\n","Epoch 41: val_loss did not improve from 0.15300\n","72/72 [==============================] - 1s 11ms/step - loss: 1.7010 - val_loss: 1.4448\n","Epoch 42/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.1540\n","Epoch 42: val_loss did not improve from 0.15300\n","72/72 [==============================] - 1s 12ms/step - loss: 2.1639 - val_loss: 1.3847\n","Epoch 43/1000\n","71/72 [============================>.] - ETA: 0s - loss: 2.0982\n","Epoch 43: val_loss did not improve from 0.15300\n","72/72 [==============================] - 1s 11ms/step - loss: 2.0914 - val_loss: 1.1148\n","Epoch 44/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.5391\n","Epoch 44: val_loss did not improve from 0.15300\n","72/72 [==============================] - 1s 11ms/step - loss: 1.5783 - val_loss: 0.7350\n","Epoch 45/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.3403\n","Epoch 45: val_loss did not improve from 0.15300\n","72/72 [==============================] - 1s 11ms/step - loss: 1.3231 - val_loss: 0.7752\n","Epoch 46/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.1008\n","Epoch 46: val_loss did not improve from 0.15300\n","72/72 [==============================] - 1s 11ms/step - loss: 1.1474 - val_loss: 0.4100\n","Epoch 47/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.4467\n","Epoch 47: val_loss did not improve from 0.15300\n","72/72 [==============================] - 1s 11ms/step - loss: 1.4256 - val_loss: 0.4711\n","Epoch 48/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.0186\n","Epoch 48: val_loss did not improve from 0.15300\n","72/72 [==============================] - 1s 11ms/step - loss: 1.0354 - val_loss: 0.5291\n","Epoch 49/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.9311\n","Epoch 49: val_loss did not improve from 0.15300\n","72/72 [==============================] - 1s 11ms/step - loss: 0.9572 - val_loss: 0.3993\n","Epoch 50/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.0493\n","Epoch 50: val_loss did not improve from 0.15300\n","72/72 [==============================] - 1s 12ms/step - loss: 1.0430 - val_loss: 0.2786\n","Epoch 51/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 0.8161\n","Epoch 51: val_loss did not improve from 0.15300\n","72/72 [==============================] - 1s 12ms/step - loss: 0.8287 - val_loss: 0.5472\n","Epoch 52/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.1334\n","Epoch 52: val_loss did not improve from 0.15300\n","72/72 [==============================] - 1s 11ms/step - loss: 1.1753 - val_loss: 0.3180\n","Epoch 53/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.0003\n","Epoch 53: val_loss did not improve from 0.15300\n","72/72 [==============================] - 1s 11ms/step - loss: 0.9927 - val_loss: 0.4005\n","Epoch 54/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.9261\n","Epoch 54: val_loss did not improve from 0.15300\n","72/72 [==============================] - 1s 11ms/step - loss: 0.9577 - val_loss: 0.8258\n","Epoch 55/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.8494\n","Epoch 55: val_loss did not improve from 0.15300\n","72/72 [==============================] - 1s 11ms/step - loss: 0.8443 - val_loss: 0.6105\n","Epoch 56/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.0767\n","Epoch 56: val_loss did not improve from 0.15300\n","72/72 [==============================] - 1s 11ms/step - loss: 1.1512 - val_loss: 0.3639\n","Epoch 57/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.5118\n","Epoch 57: val_loss did not improve from 0.15300\n","72/72 [==============================] - 1s 11ms/step - loss: 1.5270 - val_loss: 0.8088\n","Epoch 58/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.9372\n","Epoch 58: val_loss did not improve from 0.15300\n","72/72 [==============================] - 1s 11ms/step - loss: 0.9824 - val_loss: 1.0693\n","Epoch 59/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.9522\n","Epoch 59: val_loss did not improve from 0.15300\n","72/72 [==============================] - 1s 11ms/step - loss: 1.0110 - val_loss: 1.0951\n","Epoch 60/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.2532\n","Epoch 60: val_loss did not improve from 0.15300\n","72/72 [==============================] - 1s 11ms/step - loss: 1.3049 - val_loss: 1.2394\n","Epoch 61/1000\n","71/72 [============================>.] - ETA: 0s - loss: 1.0542\n","Epoch 61: val_loss did not improve from 0.15300\n","72/72 [==============================] - 1s 11ms/step - loss: 1.0448 - val_loss: 0.5951\n","Epoch 62/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.7315\n","Epoch 62: val_loss did not improve from 0.15300\n","72/72 [==============================] - 1s 11ms/step - loss: 0.7244 - val_loss: 0.3373\n","Epoch 63/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5814\n","Epoch 63: val_loss improved from 0.15300 to 0.11750, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-36-1000-15.h5\n","72/72 [==============================] - 1s 17ms/step - loss: 0.5777 - val_loss: 0.1175\n","Epoch 64/1000\n","72/72 [============================>.] - ETA: 0s - loss: 0.4358\n","Epoch 64: val_loss did not improve from 0.11750\n","72/72 [==============================] - 1s 12ms/step - loss: 0.4360 - val_loss: 0.2129\n","Epoch 65/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4329\n","Epoch 65: val_loss did not improve from 0.11750\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4321 - val_loss: 0.1468\n","Epoch 66/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4490\n","Epoch 66: val_loss did not improve from 0.11750\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4464 - val_loss: 0.1186\n","Epoch 67/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4121\n","Epoch 67: val_loss did not improve from 0.11750\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4069 - val_loss: 0.1550\n","Epoch 68/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3900\n","Epoch 68: val_loss did not improve from 0.11750\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3913 - val_loss: 0.1719\n","Epoch 69/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3856\n","Epoch 69: val_loss did not improve from 0.11750\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3902 - val_loss: 0.2654\n","Epoch 70/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3917\n","Epoch 70: val_loss did not improve from 0.11750\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3915 - val_loss: 0.2832\n","Epoch 71/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3949\n","Epoch 71: val_loss did not improve from 0.11750\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3903 - val_loss: 0.2440\n","Epoch 72/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3574\n","Epoch 72: val_loss did not improve from 0.11750\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3667 - val_loss: 0.4011\n","Epoch 73/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4039\n","Epoch 73: val_loss did not improve from 0.11750\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4063 - val_loss: 0.2155\n","Epoch 74/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3645\n","Epoch 74: val_loss did not improve from 0.11750\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3674 - val_loss: 0.1856\n","Epoch 75/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3627\n","Epoch 75: val_loss did not improve from 0.11750\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3636 - val_loss: 0.2760\n","Epoch 76/1000\n","72/72 [============================>.] - ETA: 0s - loss: 0.3735\n","Epoch 76: val_loss did not improve from 0.11750\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3714 - val_loss: 0.2151\n","Epoch 77/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3813\n","Epoch 77: val_loss did not improve from 0.11750\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3860 - val_loss: 0.1356\n","Epoch 78/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3440\n","Epoch 78: val_loss did not improve from 0.11750\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3421 - val_loss: 0.1543\n","Epoch 79/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3434\n","Epoch 79: val_loss did not improve from 0.11750\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3448 - val_loss: 0.2066\n","Epoch 80/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3678\n","Epoch 80: val_loss did not improve from 0.11750\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3698 - val_loss: 0.4230\n","Epoch 81/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3659\n","Epoch 81: val_loss did not improve from 0.11750\n","72/72 [==============================] - 1s 12ms/step - loss: 0.3644 - val_loss: 0.2059\n","Epoch 82/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3686\n","Epoch 82: val_loss did not improve from 0.11750\n","72/72 [==============================] - 1s 12ms/step - loss: 0.3696 - val_loss: 0.2955\n","Epoch 83/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3629\n","Epoch 83: val_loss did not improve from 0.11750\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3590 - val_loss: 0.1456\n","Epoch 84/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3486\n","Epoch 84: val_loss did not improve from 0.11750\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3510 - val_loss: 0.1334\n","Epoch 85/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3444\n","Epoch 85: val_loss did not improve from 0.11750\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3426 - val_loss: 0.1318\n","Epoch 86/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3503\n","Epoch 86: val_loss improved from 0.11750 to 0.06822, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-36-1000-15.h5\n","72/72 [==============================] - 1s 16ms/step - loss: 0.3504 - val_loss: 0.0682\n","Epoch 87/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3530\n","Epoch 87: val_loss did not improve from 0.06822\n","72/72 [==============================] - 1s 12ms/step - loss: 0.3526 - val_loss: 0.1058\n","Epoch 88/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3597\n","Epoch 88: val_loss did not improve from 0.06822\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3623 - val_loss: 0.1357\n","Epoch 89/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3442\n","Epoch 89: val_loss did not improve from 0.06822\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3441 - val_loss: 0.1601\n","Epoch 90/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3699\n","Epoch 90: val_loss did not improve from 0.06822\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3775 - val_loss: 0.2641\n","Epoch 91/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3377\n","Epoch 91: val_loss did not improve from 0.06822\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3367 - val_loss: 0.1334\n","Epoch 92/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3405\n","Epoch 92: val_loss did not improve from 0.06822\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3378 - val_loss: 0.1342\n","Epoch 93/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3350\n","Epoch 93: val_loss did not improve from 0.06822\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3384 - val_loss: 0.1912\n","Epoch 94/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3273\n","Epoch 94: val_loss did not improve from 0.06822\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3288 - val_loss: 0.2567\n","Epoch 95/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3578\n","Epoch 95: val_loss improved from 0.06822 to 0.05856, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-36-1000-15.h5\n","72/72 [==============================] - 1s 16ms/step - loss: 0.3574 - val_loss: 0.0586\n","Epoch 96/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3352\n","Epoch 96: val_loss did not improve from 0.05856\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3385 - val_loss: 0.0973\n","Epoch 97/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3282\n","Epoch 97: val_loss did not improve from 0.05856\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3304 - val_loss: 0.1509\n","Epoch 98/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3858\n","Epoch 98: val_loss did not improve from 0.05856\n","72/72 [==============================] - 1s 12ms/step - loss: 0.3851 - val_loss: 0.1768\n","Epoch 99/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4019\n","Epoch 99: val_loss did not improve from 0.05856\n","72/72 [==============================] - 1s 12ms/step - loss: 0.4044 - val_loss: 0.1134\n","Epoch 100/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4447\n","Epoch 100: val_loss did not improve from 0.05856\n","72/72 [==============================] - 1s 12ms/step - loss: 0.4424 - val_loss: 0.2547\n","Epoch 101/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3721\n","Epoch 101: val_loss did not improve from 0.05856\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3712 - val_loss: 0.1160\n","Epoch 102/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3533\n","Epoch 102: val_loss did not improve from 0.05856\n","72/72 [==============================] - 1s 12ms/step - loss: 0.3561 - val_loss: 0.1224\n","Epoch 103/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3607\n","Epoch 103: val_loss did not improve from 0.05856\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3628 - val_loss: 0.1229\n","Epoch 104/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3349\n","Epoch 104: val_loss did not improve from 0.05856\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3316 - val_loss: 0.0986\n","Epoch 105/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3341\n","Epoch 105: val_loss did not improve from 0.05856\n","72/72 [==============================] - 1s 12ms/step - loss: 0.3377 - val_loss: 0.1462\n","Epoch 106/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3189\n","Epoch 106: val_loss did not improve from 0.05856\n","72/72 [==============================] - 1s 12ms/step - loss: 0.3211 - val_loss: 0.1840\n","Epoch 107/1000\n","69/72 [===========================>..] - ETA: 0s - loss: 0.3418\n","Epoch 107: val_loss did not improve from 0.05856\n","72/72 [==============================] - 1s 12ms/step - loss: 0.3401 - val_loss: 0.0964\n","Epoch 108/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3501\n","Epoch 108: val_loss did not improve from 0.05856\n","72/72 [==============================] - 1s 12ms/step - loss: 0.3485 - val_loss: 0.2147\n","Epoch 109/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4136\n","Epoch 109: val_loss did not improve from 0.05856\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4126 - val_loss: 0.1307\n","Epoch 110/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3504\n","Epoch 110: val_loss did not improve from 0.05856\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3496 - val_loss: 0.0750\n","Epoch 111/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3068\n","Epoch 111: val_loss did not improve from 0.05856\n","72/72 [==============================] - 1s 12ms/step - loss: 0.3097 - val_loss: 0.2549\n","Epoch 112/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3506\n","Epoch 112: val_loss did not improve from 0.05856\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3504 - val_loss: 0.0705\n","Epoch 113/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3079\n","Epoch 113: val_loss did not improve from 0.05856\n","72/72 [==============================] - 1s 12ms/step - loss: 0.3089 - val_loss: 0.0782\n","Epoch 114/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3002\n","Epoch 114: val_loss did not improve from 0.05856\n","72/72 [==============================] - 1s 12ms/step - loss: 0.3004 - val_loss: 0.1153\n","Epoch 115/1000\n","69/72 [===========================>..] - ETA: 0s - loss: 0.2991\n","Epoch 115: val_loss did not improve from 0.05856\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2996 - val_loss: 0.1449\n","Epoch 116/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3362\n","Epoch 116: val_loss did not improve from 0.05856\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3362 - val_loss: 0.0643\n","Epoch 117/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3333\n","Epoch 117: val_loss did not improve from 0.05856\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3338 - val_loss: 0.1274\n","Epoch 118/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3402\n","Epoch 118: val_loss did not improve from 0.05856\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3396 - val_loss: 0.0780\n","Epoch 119/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3288\n","Epoch 119: val_loss did not improve from 0.05856\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3288 - val_loss: 0.0765\n","Epoch 120/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2916\n","Epoch 120: val_loss did not improve from 0.05856\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2908 - val_loss: 0.1030\n","Epoch 121/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3224\n","Epoch 121: val_loss did not improve from 0.05856\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3242 - val_loss: 0.3183\n","Epoch 122/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3236\n","Epoch 122: val_loss did not improve from 0.05856\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3276 - val_loss: 0.1564\n","Epoch 123/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3102\n","Epoch 123: val_loss did not improve from 0.05856\n","72/72 [==============================] - 1s 12ms/step - loss: 0.3133 - val_loss: 0.1568\n","Epoch 124/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3197\n","Epoch 124: val_loss did not improve from 0.05856\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3173 - val_loss: 0.1547\n","Epoch 125/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3147\n","Epoch 125: val_loss improved from 0.05856 to 0.05446, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-36-1000-15.h5\n","72/72 [==============================] - 1s 17ms/step - loss: 0.3124 - val_loss: 0.0545\n","Epoch 126/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2941\n","Epoch 126: val_loss did not improve from 0.05446\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3270 - val_loss: 0.2605\n","Epoch 127/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4373\n","Epoch 127: val_loss did not improve from 0.05446\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4316 - val_loss: 0.0771\n","Epoch 128/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3499\n","Epoch 128: val_loss did not improve from 0.05446\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3667 - val_loss: 0.3072\n","Epoch 129/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.5410\n","Epoch 129: val_loss did not improve from 0.05446\n","72/72 [==============================] - 1s 11ms/step - loss: 0.5387 - val_loss: 0.9944\n","Epoch 130/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.4002\n","Epoch 130: val_loss did not improve from 0.05446\n","72/72 [==============================] - 1s 12ms/step - loss: 0.3988 - val_loss: 0.0829\n","Epoch 131/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3693\n","Epoch 131: val_loss did not improve from 0.05446\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3690 - val_loss: 0.0944\n","Epoch 132/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3412\n","Epoch 132: val_loss did not improve from 0.05446\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3394 - val_loss: 0.1400\n","Epoch 133/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3205\n","Epoch 133: val_loss did not improve from 0.05446\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3284 - val_loss: 0.2993\n","Epoch 134/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3328\n","Epoch 134: val_loss did not improve from 0.05446\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3317 - val_loss: 0.0679\n","Epoch 135/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2861\n","Epoch 135: val_loss did not improve from 0.05446\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2847 - val_loss: 0.0937\n","Epoch 136/1000\n","72/72 [============================>.] - ETA: 0s - loss: 0.3402\n","Epoch 136: val_loss did not improve from 0.05446\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3397 - val_loss: 0.2169\n","Epoch 137/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3161\n","Epoch 137: val_loss did not improve from 0.05446\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3160 - val_loss: 0.0906\n","Epoch 138/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 0.3178\n","Epoch 138: val_loss did not improve from 0.05446\n","72/72 [==============================] - 1s 12ms/step - loss: 0.3180 - val_loss: 0.1390\n","Epoch 139/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3348\n","Epoch 139: val_loss did not improve from 0.05446\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3346 - val_loss: 0.2441\n","Epoch 140/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3190\n","Epoch 140: val_loss did not improve from 0.05446\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3199 - val_loss: 0.0808\n","Epoch 141/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3214\n","Epoch 141: val_loss did not improve from 0.05446\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3207 - val_loss: 0.1758\n","Epoch 142/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 0.2977\n","Epoch 142: val_loss improved from 0.05446 to 0.05332, saving model to /content/drive/MyDrive/농산물예측/aT_data/check13/transformer-36-1000-15.h5\n","72/72 [==============================] - 1s 16ms/step - loss: 0.2979 - val_loss: 0.0533\n","Epoch 143/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2910\n","Epoch 143: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2903 - val_loss: 0.0809\n","Epoch 144/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3062\n","Epoch 144: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3361 - val_loss: 0.4530\n","Epoch 145/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3413\n","Epoch 145: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3421 - val_loss: 0.3860\n","Epoch 146/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3425\n","Epoch 146: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3427 - val_loss: 0.0538\n","Epoch 147/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3282\n","Epoch 147: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3316 - val_loss: 0.4382\n","Epoch 148/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3314\n","Epoch 148: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3332 - val_loss: 0.1125\n","Epoch 149/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3426\n","Epoch 149: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3464 - val_loss: 0.1437\n","Epoch 150/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3129\n","Epoch 150: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3259 - val_loss: 0.1405\n","Epoch 151/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3499\n","Epoch 151: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3493 - val_loss: 0.1230\n","Epoch 152/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2987\n","Epoch 152: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3020 - val_loss: 0.4522\n","Epoch 153/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3133\n","Epoch 153: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3179 - val_loss: 0.1293\n","Epoch 154/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3226\n","Epoch 154: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3213 - val_loss: 0.4329\n","Epoch 155/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3065\n","Epoch 155: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3111 - val_loss: 0.1679\n","Epoch 156/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3055\n","Epoch 156: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3026 - val_loss: 0.0693\n","Epoch 157/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3071\n","Epoch 157: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3124 - val_loss: 0.1065\n","Epoch 158/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3251\n","Epoch 158: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3248 - val_loss: 0.1041\n","Epoch 159/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 0.3320\n","Epoch 159: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 12ms/step - loss: 0.3289 - val_loss: 0.0549\n","Epoch 160/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3163\n","Epoch 160: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3206 - val_loss: 0.1890\n","Epoch 161/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3246\n","Epoch 161: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3275 - val_loss: 0.1122\n","Epoch 162/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3315\n","Epoch 162: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3327 - val_loss: 0.0929\n","Epoch 163/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3576\n","Epoch 163: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3565 - val_loss: 0.1331\n","Epoch 164/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3022\n","Epoch 164: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3185 - val_loss: 0.3117\n","Epoch 165/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3123\n","Epoch 165: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3190 - val_loss: 0.2594\n","Epoch 166/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3020\n","Epoch 166: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3021 - val_loss: 0.1893\n","Epoch 167/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2843\n","Epoch 167: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2843 - val_loss: 0.2278\n","Epoch 168/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3220\n","Epoch 168: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3223 - val_loss: 0.1250\n","Epoch 169/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3045\n","Epoch 169: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3030 - val_loss: 0.1839\n","Epoch 170/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2897\n","Epoch 170: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2957 - val_loss: 0.4725\n","Epoch 171/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2825\n","Epoch 171: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2826 - val_loss: 0.2905\n","Epoch 172/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2765\n","Epoch 172: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2823 - val_loss: 0.2206\n","Epoch 173/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2872\n","Epoch 173: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2876 - val_loss: 0.2689\n","Epoch 174/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2763\n","Epoch 174: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2741 - val_loss: 0.1525\n","Epoch 175/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2725\n","Epoch 175: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2789 - val_loss: 0.4176\n","Epoch 176/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2940\n","Epoch 176: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2924 - val_loss: 0.1554\n","Epoch 177/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2808\n","Epoch 177: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2824 - val_loss: 0.2721\n","Epoch 178/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2951\n","Epoch 178: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2963 - val_loss: 0.3051\n","Epoch 179/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 0.2838\n","Epoch 179: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2825 - val_loss: 0.2164\n","Epoch 180/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2630\n","Epoch 180: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2708 - val_loss: 0.5002\n","Epoch 181/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3058\n","Epoch 181: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 12ms/step - loss: 0.3277 - val_loss: 0.3005\n","Epoch 182/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3384\n","Epoch 182: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3344 - val_loss: 0.0874\n","Epoch 183/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2848\n","Epoch 183: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2909 - val_loss: 0.2500\n","Epoch 184/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2907\n","Epoch 184: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2992 - val_loss: 0.7565\n","Epoch 185/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3180\n","Epoch 185: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3195 - val_loss: 0.3383\n","Epoch 186/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 0.2734\n","Epoch 186: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2729 - val_loss: 0.5761\n","Epoch 187/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2755\n","Epoch 187: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2780 - val_loss: 0.1404\n","Epoch 188/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2955\n","Epoch 188: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2970 - val_loss: 0.8881\n","Epoch 189/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2723\n","Epoch 189: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2703 - val_loss: 0.0582\n","Epoch 190/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2749\n","Epoch 190: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2734 - val_loss: 0.0755\n","Epoch 191/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2502\n","Epoch 191: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2492 - val_loss: 0.1522\n","Epoch 192/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2881\n","Epoch 192: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2998 - val_loss: 0.5101\n","Epoch 193/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2778\n","Epoch 193: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2791 - val_loss: 0.3039\n","Epoch 194/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2850\n","Epoch 194: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2823 - val_loss: 0.1625\n","Epoch 195/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2730\n","Epoch 195: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2734 - val_loss: 0.1366\n","Epoch 196/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2560\n","Epoch 196: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2574 - val_loss: 0.2705\n","Epoch 197/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 0.2875\n","Epoch 197: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2833 - val_loss: 0.4191\n","Epoch 198/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2699\n","Epoch 198: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2701 - val_loss: 0.2512\n","Epoch 199/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2550\n","Epoch 199: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2575 - val_loss: 0.3500\n","Epoch 200/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2889\n","Epoch 200: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2996 - val_loss: 0.2344\n","Epoch 201/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2776\n","Epoch 201: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2792 - val_loss: 0.2184\n","Epoch 202/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 0.2692\n","Epoch 202: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2663 - val_loss: 0.2030\n","Epoch 203/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2617\n","Epoch 203: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2605 - val_loss: 0.2214\n","Epoch 204/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2649\n","Epoch 204: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2657 - val_loss: 0.3742\n","Epoch 205/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2921\n","Epoch 205: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2991 - val_loss: 0.1534\n","Epoch 206/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.3036\n","Epoch 206: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.3083 - val_loss: 0.3519\n","Epoch 207/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2761\n","Epoch 207: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2736 - val_loss: 0.1913\n","Epoch 208/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2587\n","Epoch 208: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2621 - val_loss: 0.2581\n","Epoch 209/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2657\n","Epoch 209: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2747 - val_loss: 0.1996\n","Epoch 210/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2397\n","Epoch 210: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2382 - val_loss: 0.1882\n","Epoch 211/1000\n","69/72 [===========================>..] - ETA: 0s - loss: 0.2670\n","Epoch 211: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 13ms/step - loss: 0.2624 - val_loss: 0.6739\n","Epoch 212/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2490\n","Epoch 212: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2492 - val_loss: 0.4868\n","Epoch 213/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2474\n","Epoch 213: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2469 - val_loss: 0.3138\n","Epoch 214/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2575\n","Epoch 214: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2630 - val_loss: 0.1410\n","Epoch 215/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2451\n","Epoch 215: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2435 - val_loss: 0.0768\n","Epoch 216/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2590\n","Epoch 216: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2593 - val_loss: 0.3190\n","Epoch 217/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2922\n","Epoch 217: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2972 - val_loss: 0.1164\n","Epoch 218/1000\n","69/72 [===========================>..] - ETA: 0s - loss: 0.3571\n","Epoch 218: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 12ms/step - loss: 0.3488 - val_loss: 0.1548\n","Epoch 219/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2792\n","Epoch 219: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2773 - val_loss: 0.2100\n","Epoch 220/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 0.2846\n","Epoch 220: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2857 - val_loss: 0.3316\n","Epoch 221/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2570\n","Epoch 221: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2601 - val_loss: 0.3013\n","Epoch 222/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2677\n","Epoch 222: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2673 - val_loss: 0.1737\n","Epoch 223/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2506\n","Epoch 223: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2510 - val_loss: 0.1579\n","Epoch 224/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2290\n","Epoch 224: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2353 - val_loss: 0.4421\n","Epoch 225/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2562\n","Epoch 225: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2625 - val_loss: 0.3106\n","Epoch 226/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2936\n","Epoch 226: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2926 - val_loss: 0.2278\n","Epoch 227/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2557\n","Epoch 227: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2633 - val_loss: 0.1660\n","Epoch 228/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2376\n","Epoch 228: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2393 - val_loss: 0.4553\n","Epoch 229/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2696\n","Epoch 229: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2679 - val_loss: 0.5146\n","Epoch 230/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 0.2561\n","Epoch 230: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2543 - val_loss: 0.2619\n","Epoch 231/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 0.2674\n","Epoch 231: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2679 - val_loss: 0.1445\n","Epoch 232/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2764\n","Epoch 232: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2763 - val_loss: 0.1851\n","Epoch 233/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 0.2654\n","Epoch 233: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2658 - val_loss: 0.2705\n","Epoch 234/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2920\n","Epoch 234: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2919 - val_loss: 0.3121\n","Epoch 235/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2690\n","Epoch 235: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2704 - val_loss: 0.3054\n","Epoch 236/1000\n","70/72 [===========================>..] - ETA: 0s - loss: 0.2598\n","Epoch 236: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2569 - val_loss: 0.2179\n","Epoch 237/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2433\n","Epoch 237: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2442 - val_loss: 0.2587\n","Epoch 238/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2432\n","Epoch 238: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2399 - val_loss: 0.2127\n","Epoch 239/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2224\n","Epoch 239: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2235 - val_loss: 0.2197\n","Epoch 240/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2279\n","Epoch 240: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2276 - val_loss: 0.2127\n","Epoch 241/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2518\n","Epoch 241: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 12ms/step - loss: 0.2525 - val_loss: 0.3764\n","Epoch 242/1000\n","71/72 [============================>.] - ETA: 0s - loss: 0.2486\n","Epoch 242: val_loss did not improve from 0.05332\n","72/72 [==============================] - 1s 11ms/step - loss: 0.2567 - val_loss: 0.5091\n"]}]},{"cell_type":"code","source":["for i in tqdm(range(10)):\n","  \n","  make_csv(f'/content/drive/MyDrive/농산물예측/aT_data/aT_test_raw/sep_{i}/','test') \n","  \n","  for j in range(37):\n","    # print(f'sep:{i}, 품목:{j}')\n","    # print(len(globals()[f'test_total_sep_{i}_{j}'])) # df row수 - 0이 있나 확인하기 위해서\n","\n","    # 사용할 열 선택 - 정규식 적용후, train 데이터랑 똑같은 컬럼만 남기기\n","\n","    # 정규 표현식 적용\n","    globals()[f'test_total_sep_{i}_{j}'].columns = globals()[f'test_total_sep_{i}_{j}'].columns.str.replace(r'\\([^)]*\\)','',regex= True)\n","    \n","    # 스케일링 & 타겟값 변환\n","    #globals()[f'train_scaled_{i}'] = scaling_df(globals()[f'total_{i}'])\n","    globals()[f'test_total_sep_{i}_{j}'] = scaling_df(globals()[f'test_total_sep_{i}_{j}'])\n","    \n","    \n","    \n","\n","    # train 데이터랑 똑같이 selection\n","    globals()[f'test_total_sep_{i}_{j}'] = globals()[f'test_total_sep_{i}_{j}'][list(globals()[f'train_scaled_{j}'].columns)]\n","\n","\n","    \n","    # 해당일자평균가격 테스트 데이터에서는 타겟값 없애고 돌리네\n","    globals()[f'test_total_sep_{i}_{j}'].drop('log_target',axis=1, inplace=True)\n","\n","    \n","\n","\n","    file_number = j\n","\n","    # nan 처리\n","    globals()[f'test_total_sep_{i}_{j}'].fillna(0, inplace = True)\n","    # 형상 맞추기 코드는 아직 넣지 않았다. (안넣어도 되지 않을까?)\n","\n","    # x_test 생성\n","    df_test = astype_data(globals()[f'test_total_sep_{i}_{j}'].values.reshape(1, globals()[f'test_total_sep_{i}_{j}'].values.shape[0], globals()[f'test_total_sep_{i}_{j}'].values.shape[1]))\n","\n","\n","\n","    # model test\n","    if os.path.exists(f'/content/drive/MyDrive/농산물예측/aT_data/model_output{try_cnt}') == False:\n","            os.mkdir(f'/content/drive/MyDrive/농산물예측/aT_data/model_output{try_cnt}')\n","\n","    if os.path.exists(f'/content/drive/MyDrive/농산물예측/aT_data/model_output{try_cnt}/set_{i}') == False:\n","            os.mkdir(f'/content/drive/MyDrive/농산물예측/aT_data/model_output{try_cnt}/set_{i}')\n","\n","    # 해당하는 모델 불러오기\n","    model_test = tf.keras.models.load_model(f'/content/drive/MyDrive/농산물예측/aT_data/model{try_cnt}/transformer-{file_number}-{epoch}-{batch}.h5')\n","    pred = model_test.predict(df_test)\n","\n","\n","    # 여기서 다시 형변환 해주자!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!np.expm1\n","    pred = np.expm1(pred)\n","\n","    \n","\n","    # 결과 저장\n","    save_df = pd.DataFrame(pred).T\n","    save_df.to_csv(f'/content/drive/MyDrive/농산물예측/aT_data/model_output{try_cnt}/set_{i}/predict_{file_number}.csv', index=False)\n"],"metadata":{"id":"qLNGMdofZTPK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664373111253,"user_tz":-540,"elapsed":661861,"user":{"displayName":"이태영","userId":"15551978220746215296"}},"outputId":"debe3ddb-7748-4178-e47b-aea48264aa4f"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 0/10 [00:00<?, ?it/s]\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","100%|██████████| 37/37 [00:00<00:00, 259.23it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:00<00:00, 178.15it/s]\u001b[A\n","100%|██████████| 37/37 [00:00<00:00, 142.49it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:01, 19.66it/s]\u001b[A\n"," 11%|█         | 4/37 [00:00<00:02, 16.24it/s]\u001b[A\n"," 19%|█▉        | 7/37 [00:00<00:01, 18.22it/s]\u001b[A\n"," 24%|██▍       | 9/37 [00:00<00:01, 18.48it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:00<00:01, 18.72it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:00<00:01, 19.16it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:00<00:01, 19.37it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:00<00:00, 19.71it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:01<00:00, 19.73it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:01<00:00, 20.12it/s]\u001b[A\n"," 73%|███████▎  | 27/37 [00:01<00:00, 20.50it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:01<00:00, 20.53it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:01<00:00, 20.41it/s]\u001b[A\n","100%|██████████| 37/37 [00:01<00:00, 19.72it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:01, 19.56it/s]\u001b[A\n"," 11%|█         | 4/37 [00:00<00:01, 18.93it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:00<00:01, 18.97it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:00<00:01, 19.13it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:00<00:01, 18.99it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:00<00:01, 18.48it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:00<00:01, 18.39it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:00<00:01, 18.06it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:00<00:01, 18.26it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:01<00:00, 18.40it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:01<00:00, 18.04it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:01<00:00, 18.43it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:01<00:00, 18.17it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:01<00:00, 18.24it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:01<00:00, 18.51it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:01<00:00, 18.49it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:01<00:00, 18.50it/s]\u001b[A\n","100%|██████████| 37/37 [00:02<00:00, 18.44it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n"," 11%|█         | 4/37 [00:00<00:00, 34.07it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:00<00:00, 34.69it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:00<00:00, 33.63it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:00<00:00, 33.33it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:00<00:00, 33.14it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:00<00:00, 32.70it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:00<00:00, 32.60it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:00<00:00, 32.61it/s]\u001b[A\n","100%|██████████| 37/37 [00:01<00:00, 32.76it/s]\n","WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f076efd70e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f076f8c37a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"," 10%|█         | 1/10 [01:05<09:51, 65.77s/it]\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:02<01:27,  2.44s/it]\u001b[A\n","  5%|▌         | 2/37 [00:02<00:42,  1.23s/it]\u001b[A\n","  8%|▊         | 3/37 [00:03<00:26,  1.26it/s]\u001b[A\n"," 11%|█         | 4/37 [00:03<00:19,  1.68it/s]\u001b[A\n"," 14%|█▎        | 5/37 [00:03<00:15,  2.03it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:03<00:13,  2.38it/s]\u001b[A\n"," 19%|█▉        | 7/37 [00:04<00:10,  2.73it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:04<00:05,  4.78it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:04<00:05,  4.43it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:05<00:05,  4.20it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:05<00:06,  3.93it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:05<00:06,  3.79it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:05<00:04,  4.58it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:06<00:04,  4.10it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:06<00:04,  3.90it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:06<00:04,  3.91it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:07<00:04,  3.43it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:07<00:05,  3.16it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:07<00:04,  3.17it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:08<00:04,  3.24it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:08<00:02,  5.24it/s]\u001b[A\n"," 78%|███████▊  | 29/37 [00:08<00:01,  6.64it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:09<00:01,  5.16it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:09<00:01,  4.67it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:09<00:00,  5.30it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:10<00:00,  4.42it/s]\u001b[A\n"," 95%|█████████▍| 35/37 [00:10<00:00,  3.65it/s]\u001b[A\n"," 97%|█████████▋| 36/37 [00:10<00:00,  3.41it/s]\u001b[A\n","100%|██████████| 37/37 [00:11<00:00,  3.28it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:00<00:09,  3.63it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:09,  3.58it/s]\u001b[A\n","  8%|▊         | 3/37 [00:00<00:09,  3.55it/s]\u001b[A\n"," 11%|█         | 4/37 [00:01<00:10,  3.20it/s]\u001b[A\n"," 14%|█▎        | 5/37 [00:01<00:09,  3.34it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:01<00:09,  3.36it/s]\u001b[A\n"," 19%|█▉        | 7/37 [00:02<00:08,  3.43it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:02<00:08,  3.40it/s]\u001b[A\n"," 24%|██▍       | 9/37 [00:02<00:08,  3.37it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:02<00:07,  3.39it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:03<00:07,  3.51it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:03<00:08,  3.10it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:04<00:09,  2.44it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:04<00:08,  2.61it/s]\u001b[A\n"," 41%|████      | 15/37 [00:04<00:07,  2.76it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:05<00:07,  2.89it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:05<00:06,  3.06it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:05<00:06,  2.96it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:06<00:05,  3.07it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:06<00:05,  3.17it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:06<00:05,  2.67it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:07<00:05,  2.78it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:07<00:04,  2.98it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:07<00:04,  3.12it/s]\u001b[A\n"," 68%|██████▊   | 25/37 [00:08<00:03,  3.08it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:08<00:03,  3.18it/s]\u001b[A\n"," 73%|███████▎  | 27/37 [00:08<00:03,  3.12it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:09<00:02,  3.19it/s]\u001b[A\n"," 78%|███████▊  | 29/37 [00:09<00:02,  3.37it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:09<00:02,  3.43it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:09<00:01,  3.55it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:10<00:01,  3.18it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:10<00:01,  3.40it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:10<00:00,  3.11it/s]\u001b[A\n"," 95%|█████████▍| 35/37 [00:11<00:00,  2.87it/s]\u001b[A\n"," 97%|█████████▋| 36/37 [00:11<00:00,  3.03it/s]\u001b[A\n","100%|██████████| 37/37 [00:11<00:00,  3.11it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:00<00:11,  3.27it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:13,  2.58it/s]\u001b[A\n","  8%|▊         | 3/37 [00:01<00:13,  2.50it/s]\u001b[A\n"," 11%|█         | 4/37 [00:01<00:10,  3.06it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:01<00:05,  5.43it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:01<00:03,  7.84it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:01<00:02, 11.20it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:01<00:01, 12.90it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:01<00:01, 15.23it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:02<00:01, 16.88it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:02<00:00, 17.41it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:02<00:00, 18.34it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:02<00:00, 18.73it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:02<00:00, 18.36it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:02<00:00, 15.79it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:02<00:00, 16.65it/s]\u001b[A\n","100%|██████████| 37/37 [00:03<00:00, 11.84it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:01, 17.87it/s]\u001b[A\n"," 11%|█         | 4/37 [00:00<00:01, 17.65it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:00<00:01, 16.21it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:00<00:01, 16.22it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:00<00:01, 16.42it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:00<00:01, 17.05it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:00<00:01, 17.22it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:00<00:01, 17.05it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:01<00:01, 17.36it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:01<00:00, 17.29it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:01<00:00, 17.65it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:01<00:00, 17.81it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:01<00:00, 17.85it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:01<00:00, 17.80it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:01<00:00, 17.68it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:01<00:00, 17.79it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:01<00:00, 17.62it/s]\u001b[A\n","100%|██████████| 37/37 [00:02<00:00, 17.47it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n"," 11%|█         | 4/37 [00:00<00:01, 32.17it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:00<00:00, 33.38it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:00<00:00, 32.34it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:00<00:00, 32.39it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:00<00:00, 32.41it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:00<00:00, 32.32it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:00<00:00, 32.11it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:00<00:00, 32.11it/s]\u001b[A\n","100%|██████████| 37/37 [00:01<00:00, 32.07it/s]\n"," 20%|██        | 2/10 [02:11<08:47, 65.92s/it]\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:02<01:27,  2.42s/it]\u001b[A\n","  5%|▌         | 2/37 [00:02<00:40,  1.17s/it]\u001b[A\n","  8%|▊         | 3/37 [00:03<00:28,  1.21it/s]\u001b[A\n"," 11%|█         | 4/37 [00:03<00:20,  1.62it/s]\u001b[A\n"," 14%|█▎        | 5/37 [00:03<00:17,  1.79it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:04<00:15,  2.03it/s]\u001b[A\n"," 19%|█▉        | 7/37 [00:04<00:12,  2.44it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:04<00:06,  4.39it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:05<00:06,  4.00it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:05<00:06,  3.86it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:05<00:06,  3.77it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:06<00:06,  3.53it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:06<00:04,  4.44it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:06<00:04,  4.18it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:06<00:04,  3.89it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:07<00:05,  3.42it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:07<00:04,  3.50it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:07<00:04,  3.46it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:08<00:04,  3.51it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:08<00:04,  3.39it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:08<00:02,  5.14it/s]\u001b[A\n"," 78%|███████▊  | 29/37 [00:09<00:01,  6.66it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:09<00:01,  5.62it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:09<00:01,  4.40it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:10<00:00,  5.01it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:10<00:00,  4.63it/s]\u001b[A\n"," 95%|█████████▍| 35/37 [00:10<00:00,  4.43it/s]\u001b[A\n"," 97%|█████████▋| 36/37 [00:10<00:00,  4.24it/s]\u001b[A\n","100%|██████████| 37/37 [00:11<00:00,  3.30it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:00<00:08,  4.16it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:09,  3.84it/s]\u001b[A\n","  8%|▊         | 3/37 [00:00<00:09,  3.47it/s]\u001b[A\n"," 11%|█         | 4/37 [00:01<00:09,  3.40it/s]\u001b[A\n"," 14%|█▎        | 5/37 [00:01<00:09,  3.50it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:01<00:08,  3.63it/s]\u001b[A\n"," 19%|█▉        | 7/37 [00:01<00:08,  3.51it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:02<00:08,  3.42it/s]\u001b[A\n"," 24%|██▍       | 9/37 [00:02<00:08,  3.37it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:02<00:07,  3.47it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:03<00:07,  3.51it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:03<00:06,  3.67it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:03<00:06,  3.45it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:04<00:07,  3.23it/s]\u001b[A\n"," 41%|████      | 15/37 [00:04<00:06,  3.25it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:04<00:06,  3.36it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:04<00:05,  3.35it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:05<00:06,  3.09it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:05<00:05,  3.25it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:05<00:05,  3.31it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:06<00:05,  3.01it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:06<00:04,  3.13it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:06<00:04,  3.23it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:07<00:04,  3.24it/s]\u001b[A\n"," 68%|██████▊   | 25/37 [00:07<00:03,  3.25it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:07<00:03,  2.98it/s]\u001b[A\n"," 73%|███████▎  | 27/37 [00:08<00:04,  2.23it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:08<00:03,  2.32it/s]\u001b[A\n"," 78%|███████▊  | 29/37 [00:09<00:03,  2.60it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:09<00:02,  2.72it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:09<00:02,  2.99it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:10<00:01,  3.08it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:10<00:01,  3.10it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:10<00:00,  3.21it/s]\u001b[A\n"," 95%|█████████▍| 35/37 [00:11<00:00,  2.88it/s]\u001b[A\n"," 97%|█████████▋| 36/37 [00:11<00:00,  2.73it/s]\u001b[A\n","100%|██████████| 37/37 [00:11<00:00,  3.11it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:00<00:12,  2.96it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:11,  3.00it/s]\u001b[A\n","  8%|▊         | 3/37 [00:01<00:12,  2.79it/s]\u001b[A\n"," 11%|█         | 4/37 [00:01<00:10,  3.05it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:01<00:05,  5.51it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:01<00:03,  7.83it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:01<00:02, 11.25it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:01<00:01, 13.80it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:01<00:01, 15.82it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:02<00:02,  8.86it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:02<00:01, 11.12it/s]\u001b[A\n"," 68%|██████▊   | 25/37 [00:02<00:00, 13.18it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:02<00:00, 15.01it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:03<00:00, 16.44it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:03<00:00, 17.56it/s]\u001b[A\n","100%|██████████| 37/37 [00:03<00:00, 11.00it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:01, 19.26it/s]\u001b[A\n"," 11%|█         | 4/37 [00:00<00:01, 18.04it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:00<00:01, 18.37it/s]\u001b[A\n"," 24%|██▍       | 9/37 [00:00<00:01, 19.33it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:00<00:01, 19.32it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:00<00:01, 19.37it/s]\u001b[A\n"," 41%|████      | 15/37 [00:00<00:01, 19.51it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:00<00:01, 19.23it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:01<00:00, 18.56it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:01<00:00, 18.23it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:01<00:00, 18.57it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:01<00:00, 19.26it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:01<00:00, 19.28it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:01<00:00, 19.36it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:01<00:00, 19.11it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:01<00:00, 19.02it/s]\u001b[A\n","100%|██████████| 37/37 [00:01<00:00, 18.99it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n"," 11%|█         | 4/37 [00:00<00:00, 33.57it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:00<00:00, 34.52it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:00<00:00, 33.63it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:00<00:00, 33.16it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:00<00:00, 33.17it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:00<00:00, 32.74it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:00<00:00, 32.78it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:00<00:00, 32.67it/s]\u001b[A\n","100%|██████████| 37/37 [00:01<00:00, 32.76it/s]\n"," 30%|███       | 3/10 [03:18<07:43, 66.25s/it]\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:02<01:19,  2.22s/it]\u001b[A\n","  5%|▌         | 2/37 [00:02<00:37,  1.07s/it]\u001b[A\n","  8%|▊         | 3/37 [00:02<00:24,  1.38it/s]\u001b[A\n"," 11%|█         | 4/37 [00:03<00:20,  1.64it/s]\u001b[A\n"," 14%|█▎        | 5/37 [00:03<00:17,  1.88it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:03<00:13,  2.26it/s]\u001b[A\n"," 19%|█▉        | 7/37 [00:04<00:11,  2.54it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:04<00:05,  4.55it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:04<00:06,  4.15it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:05<00:06,  3.79it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:05<00:06,  3.80it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:05<00:06,  3.77it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:06<00:04,  4.48it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:06<00:05,  3.56it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:06<00:05,  3.58it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:06<00:04,  3.68it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:07<00:04,  3.48it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:07<00:05,  3.07it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:08<00:04,  3.20it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:08<00:04,  3.46it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:08<00:02,  4.90it/s]\u001b[A\n"," 78%|███████▊  | 29/37 [00:08<00:01,  6.42it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:09<00:01,  5.03it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:09<00:01,  4.46it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:10<00:00,  4.70it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:10<00:00,  4.40it/s]\u001b[A\n"," 95%|█████████▍| 35/37 [00:10<00:00,  4.25it/s]\u001b[A\n"," 97%|█████████▋| 36/37 [00:10<00:00,  4.10it/s]\u001b[A\n","100%|██████████| 37/37 [00:11<00:00,  3.31it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:00<00:11,  3.13it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:12,  2.91it/s]\u001b[A\n","  8%|▊         | 3/37 [00:00<00:10,  3.22it/s]\u001b[A\n"," 11%|█         | 4/37 [00:01<00:09,  3.39it/s]\u001b[A\n"," 14%|█▎        | 5/37 [00:01<00:09,  3.52it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:01<00:08,  3.52it/s]\u001b[A\n"," 19%|█▉        | 7/37 [00:02<00:13,  2.23it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:02<00:11,  2.47it/s]\u001b[A\n"," 24%|██▍       | 9/37 [00:03<00:10,  2.68it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:03<00:09,  2.84it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:03<00:09,  2.78it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:04<00:08,  2.95it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:04<00:08,  2.79it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:04<00:07,  2.95it/s]\u001b[A\n"," 41%|████      | 15/37 [00:05<00:06,  3.14it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:05<00:06,  3.31it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:05<00:06,  2.92it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:06<00:06,  3.01it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:06<00:05,  3.23it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:06<00:05,  3.21it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:06<00:04,  3.29it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:07<00:04,  3.45it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:07<00:04,  3.31it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:07<00:03,  3.40it/s]\u001b[A\n"," 68%|██████▊   | 25/37 [00:08<00:03,  3.42it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:08<00:03,  3.50it/s]\u001b[A\n"," 73%|███████▎  | 27/37 [00:08<00:03,  3.26it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:09<00:02,  3.28it/s]\u001b[A\n"," 78%|███████▊  | 29/37 [00:09<00:02,  3.43it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:09<00:02,  3.48it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:09<00:01,  3.46it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:10<00:01,  3.49it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:10<00:01,  3.52it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:10<00:00,  3.47it/s]\u001b[A\n"," 95%|█████████▍| 35/37 [00:11<00:00,  3.50it/s]\u001b[A\n"," 97%|█████████▋| 36/37 [00:11<00:00,  3.18it/s]\u001b[A\n","100%|██████████| 37/37 [00:11<00:00,  3.17it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:00<00:12,  2.91it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:13,  2.68it/s]\u001b[A\n","  8%|▊         | 3/37 [00:01<00:12,  2.65it/s]\u001b[A\n"," 11%|█         | 4/37 [00:01<00:11,  2.97it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:01<00:05,  5.38it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:01<00:03,  7.84it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:01<00:02, 10.20it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:01<00:02, 12.16it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:01<00:01, 13.95it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:02<00:01, 15.40it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:02<00:01, 16.60it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:02<00:00, 17.46it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:02<00:00, 18.65it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:02<00:00, 19.37it/s]\u001b[A\n"," 78%|███████▊  | 29/37 [00:02<00:00, 19.69it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:02<00:00, 19.92it/s]\u001b[A\n","100%|██████████| 37/37 [00:03<00:00, 12.15it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:01, 19.87it/s]\u001b[A\n"," 11%|█         | 4/37 [00:00<00:01, 19.91it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:00<00:01, 19.76it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:00<00:01, 19.70it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:00<00:01, 19.46it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:00<00:01, 18.87it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:00<00:01, 18.78it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:00<00:01, 18.58it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:00<00:01, 18.47it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:01<00:00, 18.20it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:01<00:00, 17.57it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:01<00:00, 17.90it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:01<00:00, 18.37it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:01<00:00, 18.23it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:01<00:00, 18.00it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:01<00:00, 18.16it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:01<00:00, 18.37it/s]\u001b[A\n","100%|██████████| 37/37 [00:01<00:00, 18.56it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n"," 11%|█         | 4/37 [00:00<00:00, 33.16it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:00<00:00, 34.08it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:00<00:00, 33.61it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:00<00:00, 33.06it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:00<00:00, 33.02it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:00<00:00, 32.54it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:00<00:00, 32.37it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:00<00:00, 32.52it/s]\u001b[A\n","100%|██████████| 37/37 [00:01<00:00, 32.66it/s]\n"," 40%|████      | 4/10 [04:24<06:36, 66.06s/it]\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:02<01:22,  2.30s/it]\u001b[A\n","  5%|▌         | 2/37 [00:02<00:40,  1.16s/it]\u001b[A\n","  8%|▊         | 3/37 [00:02<00:25,  1.31it/s]\u001b[A\n"," 11%|█         | 4/37 [00:03<00:18,  1.75it/s]\u001b[A\n"," 14%|█▎        | 5/37 [00:03<00:16,  1.98it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:04<00:14,  2.13it/s]\u001b[A\n"," 19%|█▉        | 7/37 [00:04<00:12,  2.31it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:04<00:06,  4.21it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:04<00:06,  4.06it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:05<00:06,  4.04it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:05<00:06,  3.80it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:05<00:06,  3.82it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:06<00:04,  4.59it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:06<00:04,  4.44it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:06<00:04,  4.27it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:06<00:04,  4.08it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:07<00:04,  3.96it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:07<00:04,  3.83it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:07<00:03,  3.76it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:07<00:03,  3.65it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:08<00:01,  5.75it/s]\u001b[A\n"," 78%|███████▊  | 29/37 [00:08<00:01,  6.93it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:08<00:01,  6.10it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:09<00:01,  5.34it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:09<00:00,  6.04it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:09<00:00,  5.06it/s]\u001b[A\n"," 95%|█████████▍| 35/37 [00:10<00:00,  4.41it/s]\u001b[A\n"," 97%|█████████▋| 36/37 [00:10<00:00,  3.72it/s]\u001b[A\n","100%|██████████| 37/37 [00:10<00:00,  3.44it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:00<00:09,  3.61it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:10,  3.35it/s]\u001b[A\n","  8%|▊         | 3/37 [00:00<00:11,  3.07it/s]\u001b[A\n"," 11%|█         | 4/37 [00:01<00:12,  2.61it/s]\u001b[A\n"," 14%|█▎        | 5/37 [00:01<00:11,  2.83it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:02<00:14,  2.16it/s]\u001b[A\n"," 19%|█▉        | 7/37 [00:02<00:13,  2.27it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:03<00:12,  2.34it/s]\u001b[A\n"," 24%|██▍       | 9/37 [00:03<00:11,  2.51it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:03<00:10,  2.64it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:04<00:09,  2.77it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:04<00:08,  3.02it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:04<00:07,  3.04it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:05<00:07,  3.23it/s]\u001b[A\n"," 41%|████      | 15/37 [00:05<00:06,  3.36it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:05<00:06,  3.43it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:05<00:05,  3.53it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:06<00:06,  3.11it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:06<00:06,  2.89it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:07<00:06,  2.78it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:07<00:06,  2.58it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:07<00:05,  2.52it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:08<00:05,  2.74it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:08<00:04,  2.90it/s]\u001b[A\n"," 68%|██████▊   | 25/37 [00:08<00:04,  2.99it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:09<00:03,  3.03it/s]\u001b[A\n"," 73%|███████▎  | 27/37 [00:09<00:03,  3.24it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:09<00:02,  3.40it/s]\u001b[A\n"," 78%|███████▊  | 29/37 [00:10<00:02,  3.10it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:10<00:02,  2.90it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:10<00:01,  3.09it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:10<00:01,  3.23it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:11<00:01,  3.40it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:11<00:00,  3.30it/s]\u001b[A\n"," 95%|█████████▍| 35/37 [00:11<00:00,  3.27it/s]\u001b[A\n"," 97%|█████████▋| 36/37 [00:12<00:00,  3.01it/s]\u001b[A\n","100%|██████████| 37/37 [00:12<00:00,  2.94it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:00<00:12,  2.97it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:11,  3.00it/s]\u001b[A\n","  8%|▊         | 3/37 [00:01<00:11,  2.86it/s]\u001b[A\n"," 14%|█▎        | 5/37 [00:01<00:07,  4.55it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:01<00:06,  5.12it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:01<00:03,  7.67it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:01<00:02, 10.09it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:01<00:02, 12.26it/s]\u001b[A\n"," 41%|████      | 15/37 [00:01<00:01, 14.87it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:02<00:01, 16.70it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:02<00:00, 17.96it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:02<00:00, 18.14it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:02<00:00, 18.83it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:02<00:00, 19.03it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:02<00:00, 19.12it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:02<00:00, 19.18it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:02<00:00, 19.30it/s]\u001b[A\n","100%|██████████| 37/37 [00:02<00:00, 12.44it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:01, 19.73it/s]\u001b[A\n"," 11%|█         | 4/37 [00:00<00:01, 19.31it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:00<00:01, 19.24it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:00<00:01, 19.14it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:00<00:01, 18.91it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:00<00:01, 18.92it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:00<00:01, 18.82it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:00<00:01, 19.00it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:00<00:00, 19.07it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:01<00:00, 19.00it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:01<00:00, 19.14it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:01<00:00, 18.72it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:01<00:00, 18.84it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:01<00:00, 17.30it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:01<00:00, 16.71it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:01<00:00, 16.92it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:01<00:00, 17.39it/s]\u001b[A\n","100%|██████████| 37/37 [00:02<00:00, 18.13it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n"," 11%|█         | 4/37 [00:00<00:00, 33.51it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:00<00:00, 33.04it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:00<00:00, 32.08it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:00<00:00, 31.71it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:00<00:00, 32.04it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:00<00:00, 32.54it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:00<00:00, 32.36it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:00<00:00, 32.33it/s]\u001b[A\n","100%|██████████| 37/37 [00:01<00:00, 32.30it/s]\n"," 50%|█████     | 5/10 [05:30<05:31, 66.22s/it]\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:01<01:11,  1.99s/it]\u001b[A\n","  5%|▌         | 2/37 [00:02<00:34,  1.02it/s]\u001b[A\n","  8%|▊         | 3/37 [00:02<00:22,  1.51it/s]\u001b[A\n"," 11%|█         | 4/37 [00:02<00:17,  1.92it/s]\u001b[A\n"," 14%|█▎        | 5/37 [00:03<00:14,  2.28it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:03<00:11,  2.68it/s]\u001b[A\n"," 19%|█▉        | 7/37 [00:03<00:10,  2.80it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:04<00:06,  4.24it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:04<00:06,  3.82it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:04<00:07,  3.40it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:05<00:07,  3.19it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:05<00:07,  2.95it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:05<00:05,  3.90it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:06<00:05,  3.88it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:06<00:04,  3.85it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:06<00:05,  3.53it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:07<00:04,  3.57it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:07<00:04,  3.33it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:07<00:04,  3.50it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:08<00:04,  3.12it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:08<00:02,  4.63it/s]\u001b[A\n"," 78%|███████▊  | 29/37 [00:08<00:01,  5.64it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:09<00:01,  4.58it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:09<00:01,  4.46it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:09<00:00,  5.14it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:10<00:00,  4.60it/s]\u001b[A\n"," 95%|█████████▍| 35/37 [00:10<00:00,  4.07it/s]\u001b[A\n"," 97%|█████████▋| 36/37 [00:10<00:00,  3.76it/s]\u001b[A\n","100%|██████████| 37/37 [00:11<00:00,  3.32it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:00<00:10,  3.47it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:09,  3.54it/s]\u001b[A\n","  8%|▊         | 3/37 [00:01<00:14,  2.34it/s]\u001b[A\n"," 11%|█         | 4/37 [00:01<00:13,  2.42it/s]\u001b[A\n"," 14%|█▎        | 5/37 [00:01<00:11,  2.77it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:02<00:10,  2.94it/s]\u001b[A\n"," 19%|█▉        | 7/37 [00:02<00:09,  3.13it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:02<00:09,  3.12it/s]\u001b[A\n"," 24%|██▍       | 9/37 [00:03<00:08,  3.22it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:03<00:09,  2.96it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:03<00:08,  3.10it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:04<00:08,  2.88it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:04<00:07,  3.04it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:04<00:07,  3.20it/s]\u001b[A\n"," 41%|████      | 15/37 [00:05<00:07,  2.89it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:05<00:07,  2.98it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:05<00:06,  2.90it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:06<00:06,  2.74it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:06<00:06,  2.99it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:06<00:05,  3.10it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:07<00:05,  3.16it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:07<00:05,  2.97it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:07<00:04,  2.86it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:08<00:04,  2.73it/s]\u001b[A\n"," 68%|██████▊   | 25/37 [00:08<00:04,  2.67it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:08<00:03,  2.86it/s]\u001b[A\n"," 73%|███████▎  | 27/37 [00:09<00:03,  3.02it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:09<00:04,  2.18it/s]\u001b[A\n"," 78%|███████▊  | 29/37 [00:10<00:03,  2.13it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:10<00:02,  2.37it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:11<00:02,  2.36it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:11<00:02,  2.41it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:11<00:01,  2.63it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:12<00:01,  2.84it/s]\u001b[A\n"," 95%|█████████▍| 35/37 [00:12<00:00,  3.02it/s]\u001b[A\n"," 97%|█████████▋| 36/37 [00:12<00:00,  3.13it/s]\u001b[A\n","100%|██████████| 37/37 [00:13<00:00,  2.84it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:00<00:12,  2.95it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:11,  3.05it/s]\u001b[A\n","  8%|▊         | 3/37 [00:01<00:12,  2.83it/s]\u001b[A\n"," 11%|█         | 4/37 [00:01<00:12,  2.61it/s]\u001b[A\n"," 19%|█▉        | 7/37 [00:01<00:05,  5.79it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:01<00:03,  8.80it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:01<00:02, 11.42it/s]\u001b[A\n"," 41%|████      | 15/37 [00:02<00:01, 12.84it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:02<00:01, 14.28it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:02<00:01, 15.55it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:02<00:00, 16.62it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:02<00:00, 17.42it/s]\u001b[A\n"," 68%|██████▊   | 25/37 [00:02<00:00, 17.89it/s]\u001b[A\n"," 73%|███████▎  | 27/37 [00:02<00:00, 18.32it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:02<00:00, 19.21it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:02<00:00, 19.30it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:02<00:00, 19.33it/s]\u001b[A\n","100%|██████████| 37/37 [00:03<00:00, 11.84it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  8%|▊         | 3/37 [00:00<00:01, 20.17it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:00<00:01, 19.36it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:00<00:01, 18.67it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:00<00:01, 18.66it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:00<00:01, 18.92it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:00<00:01, 18.96it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:00<00:01, 19.26it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:00<00:00, 19.38it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:01<00:00, 19.21it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:01<00:00, 18.94it/s]\u001b[A\n"," 68%|██████▊   | 25/37 [00:01<00:00, 18.90it/s]\u001b[A\n"," 73%|███████▎  | 27/37 [00:01<00:00, 18.85it/s]\u001b[A\n"," 78%|███████▊  | 29/37 [00:01<00:00, 18.80it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:01<00:00, 18.95it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:01<00:00, 18.83it/s]\u001b[A\n"," 95%|█████████▍| 35/37 [00:01<00:00, 18.39it/s]\u001b[A\n","100%|██████████| 37/37 [00:01<00:00, 18.81it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n"," 11%|█         | 4/37 [00:00<00:01, 29.93it/s]\u001b[A\n"," 19%|█▉        | 7/37 [00:00<00:01, 28.08it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:00<00:00, 30.52it/s]\u001b[A\n"," 41%|████      | 15/37 [00:00<00:00, 31.52it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:00<00:00, 32.12it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:00<00:00, 31.86it/s]\u001b[A\n"," 73%|███████▎  | 27/37 [00:00<00:00, 31.65it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:00<00:00, 31.67it/s]\u001b[A\n","100%|██████████| 37/37 [00:01<00:00, 31.42it/s]\n"," 60%|██████    | 6/10 [06:38<04:26, 66.68s/it]\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:02<01:20,  2.23s/it]\u001b[A\n","  5%|▌         | 2/37 [00:02<00:37,  1.08s/it]\u001b[A\n","  8%|▊         | 3/37 [00:02<00:25,  1.32it/s]\u001b[A\n"," 11%|█         | 4/37 [00:03<00:18,  1.77it/s]\u001b[A\n"," 14%|█▎        | 5/37 [00:03<00:14,  2.17it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:03<00:13,  2.36it/s]\u001b[A\n"," 19%|█▉        | 7/37 [00:04<00:11,  2.68it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:04<00:05,  4.94it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:04<00:05,  4.68it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:04<00:05,  4.21it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:05<00:06,  3.96it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:05<00:06,  3.45it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:05<00:05,  3.94it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:06<00:05,  3.90it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:06<00:04,  3.81it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:06<00:05,  3.56it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:07<00:04,  3.50it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:07<00:05,  3.14it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:07<00:04,  3.17it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:08<00:04,  2.90it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:08<00:02,  4.72it/s]\u001b[A\n"," 78%|███████▊  | 29/37 [00:08<00:01,  5.97it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:09<00:01,  5.33it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:09<00:01,  4.61it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:09<00:00,  5.06it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:10<00:00,  4.22it/s]\u001b[A\n"," 95%|█████████▍| 35/37 [00:10<00:00,  3.79it/s]\u001b[A\n"," 97%|█████████▋| 36/37 [00:10<00:00,  3.63it/s]\u001b[A\n","100%|██████████| 37/37 [00:11<00:00,  3.31it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:00<00:10,  3.35it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:10,  3.36it/s]\u001b[A\n","  8%|▊         | 3/37 [00:00<00:10,  3.27it/s]\u001b[A\n"," 11%|█         | 4/37 [00:01<00:10,  3.28it/s]\u001b[A\n"," 14%|█▎        | 5/37 [00:01<00:10,  3.01it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:01<00:09,  3.13it/s]\u001b[A\n"," 19%|█▉        | 7/37 [00:02<00:09,  3.29it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:02<00:08,  3.43it/s]\u001b[A\n"," 24%|██▍       | 9/37 [00:02<00:08,  3.47it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:02<00:07,  3.60it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:03<00:07,  3.49it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:03<00:07,  3.46it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:03<00:07,  3.43it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:04<00:07,  3.07it/s]\u001b[A\n"," 41%|████      | 15/37 [00:04<00:06,  3.22it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:04<00:06,  3.39it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:05<00:05,  3.38it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:05<00:05,  3.48it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:05<00:05,  3.36it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:05<00:05,  3.31it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:06<00:04,  3.33it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:06<00:04,  3.25it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:06<00:04,  3.28it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:07<00:03,  3.37it/s]\u001b[A\n"," 68%|██████▊   | 25/37 [00:07<00:03,  3.02it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:07<00:03,  3.21it/s]\u001b[A\n"," 73%|███████▎  | 27/37 [00:08<00:03,  3.09it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:08<00:02,  3.20it/s]\u001b[A\n"," 78%|███████▊  | 29/37 [00:08<00:02,  3.24it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:09<00:02,  3.33it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:09<00:01,  3.28it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:09<00:01,  3.30it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:10<00:01,  3.14it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:10<00:00,  3.12it/s]\u001b[A\n"," 95%|█████████▍| 35/37 [00:10<00:00,  3.30it/s]\u001b[A\n"," 97%|█████████▋| 36/37 [00:10<00:00,  3.38it/s]\u001b[A\n","100%|██████████| 37/37 [00:11<00:00,  3.27it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:00<00:11,  3.16it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:11,  2.92it/s]\u001b[A\n","  8%|▊         | 3/37 [00:01<00:13,  2.60it/s]\u001b[A\n"," 11%|█         | 4/37 [00:01<00:11,  2.95it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:01<00:05,  5.30it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:01<00:03,  7.61it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:01<00:02,  9.86it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:01<00:02, 11.85it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:01<00:01, 13.48it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:02<00:01, 14.93it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:02<00:01, 16.21it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:02<00:00, 17.76it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:02<00:00, 18.17it/s]\u001b[A\n"," 68%|██████▊   | 25/37 [00:02<00:00, 18.38it/s]\u001b[A\n"," 73%|███████▎  | 27/37 [00:02<00:00, 18.44it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:02<00:00, 19.19it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:02<00:00, 19.39it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:02<00:00, 19.54it/s]\u001b[A\n","100%|██████████| 37/37 [00:03<00:00, 11.98it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:02, 17.16it/s]\u001b[A\n"," 11%|█         | 4/37 [00:00<00:01, 17.64it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:00<00:01, 18.20it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:00<00:02, 14.15it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:00<00:01, 15.69it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:00<00:01, 16.81it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:00<00:01, 17.62it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:00<00:01, 17.96it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:01<00:01, 17.96it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:01<00:00, 17.98it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:01<00:00, 17.95it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:01<00:00, 18.17it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:01<00:00, 18.54it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:01<00:00, 18.25it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:01<00:00, 18.12it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:01<00:00, 18.15it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:01<00:00, 18.28it/s]\u001b[A\n","100%|██████████| 37/37 [00:02<00:00, 17.67it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n"," 11%|█         | 4/37 [00:00<00:01, 32.02it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:00<00:00, 33.53it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:00<00:00, 33.37it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:00<00:00, 32.96it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:00<00:00, 32.46it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:00<00:00, 32.45it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:00<00:00, 32.43it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:00<00:00, 32.50it/s]\u001b[A\n","100%|██████████| 37/37 [00:01<00:00, 32.45it/s]\n"," 70%|███████   | 7/10 [07:43<03:18, 66.29s/it]\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:02<01:22,  2.28s/it]\u001b[A\n","  5%|▌         | 2/37 [00:02<00:37,  1.08s/it]\u001b[A\n","  8%|▊         | 3/37 [00:02<00:24,  1.40it/s]\u001b[A\n"," 11%|█         | 4/37 [00:03<00:17,  1.85it/s]\u001b[A\n"," 14%|█▎        | 5/37 [00:03<00:14,  2.14it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:03<00:13,  2.37it/s]\u001b[A\n"," 19%|█▉        | 7/37 [00:04<00:11,  2.63it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:04<00:05,  4.71it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:04<00:06,  3.93it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:05<00:06,  3.64it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:05<00:07,  3.38it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:05<00:06,  3.50it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:05<00:04,  4.36it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:06<00:05,  3.59it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:06<00:05,  3.60it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:07<00:05,  3.09it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:07<00:05,  2.94it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:07<00:05,  3.02it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:08<00:05,  2.85it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:08<00:04,  3.09it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:08<00:02,  4.64it/s]\u001b[A\n"," 78%|███████▊  | 29/37 [00:09<00:01,  5.54it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:09<00:01,  5.16it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:09<00:01,  4.80it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:10<00:00,  5.26it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:10<00:00,  4.77it/s]\u001b[A\n"," 95%|█████████▍| 35/37 [00:10<00:00,  3.82it/s]\u001b[A\n"," 97%|█████████▋| 36/37 [00:11<00:00,  3.63it/s]\u001b[A\n","100%|██████████| 37/37 [00:11<00:00,  3.20it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:00<00:09,  3.79it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:09,  3.77it/s]\u001b[A\n","  8%|▊         | 3/37 [00:00<00:08,  3.85it/s]\u001b[A\n"," 11%|█         | 4/37 [00:01<00:10,  3.11it/s]\u001b[A\n"," 14%|█▎        | 5/37 [00:01<00:11,  2.85it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:01<00:10,  2.93it/s]\u001b[A\n"," 19%|█▉        | 7/37 [00:02<00:09,  3.04it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:02<00:09,  3.17it/s]\u001b[A\n"," 24%|██▍       | 9/37 [00:02<00:08,  3.31it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:03<00:07,  3.39it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:03<00:07,  3.48it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:03<00:07,  3.53it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:04<00:07,  3.13it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:04<00:06,  3.32it/s]\u001b[A\n"," 41%|████      | 15/37 [00:04<00:07,  3.03it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:05<00:07,  2.91it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:05<00:06,  3.12it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:05<00:05,  3.23it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:05<00:05,  3.32it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:06<00:05,  3.39it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:06<00:04,  3.28it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:06<00:04,  3.01it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:07<00:04,  3.20it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:07<00:03,  3.28it/s]\u001b[A\n"," 68%|██████▊   | 25/37 [00:07<00:04,  2.85it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:08<00:03,  3.02it/s]\u001b[A\n"," 73%|███████▎  | 27/37 [00:08<00:03,  3.11it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:08<00:03,  2.92it/s]\u001b[A\n"," 78%|███████▊  | 29/37 [00:09<00:02,  3.05it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:09<00:02,  2.88it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:09<00:01,  3.05it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:10<00:01,  3.18it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:10<00:01,  3.30it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:10<00:00,  3.49it/s]\u001b[A\n"," 95%|█████████▍| 35/37 [00:10<00:00,  3.52it/s]\u001b[A\n"," 97%|█████████▋| 36/37 [00:11<00:00,  3.57it/s]\u001b[A\n","100%|██████████| 37/37 [00:11<00:00,  3.20it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:00<00:15,  2.27it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:15,  2.32it/s]\u001b[A\n","  8%|▊         | 3/37 [00:01<00:14,  2.32it/s]\u001b[A\n"," 11%|█         | 4/37 [00:01<00:11,  2.94it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:01<00:05,  5.29it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:01<00:03,  7.70it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:01<00:02, 10.02it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:01<00:01, 13.04it/s]\u001b[A\n"," 41%|████      | 15/37 [00:02<00:01, 14.42it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:02<00:01, 15.55it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:02<00:01, 16.60it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:02<00:00, 17.46it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:02<00:00, 17.90it/s]\u001b[A\n"," 68%|██████▊   | 25/37 [00:02<00:00, 18.32it/s]\u001b[A\n"," 73%|███████▎  | 27/37 [00:02<00:00, 18.24it/s]\u001b[A\n"," 78%|███████▊  | 29/37 [00:02<00:00, 18.42it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:02<00:00, 18.58it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:03<00:00, 18.72it/s]\u001b[A\n"," 95%|█████████▍| 35/37 [00:03<00:00, 18.91it/s]\u001b[A\n","100%|██████████| 37/37 [00:03<00:00, 11.50it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:01, 19.90it/s]\u001b[A\n"," 11%|█         | 4/37 [00:00<00:01, 19.45it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:00<00:01, 18.83it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:00<00:01, 18.80it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:00<00:01, 18.56it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:00<00:01, 18.82it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:00<00:01, 18.70it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:00<00:01, 18.66it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:00<00:01, 18.61it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:01<00:00, 18.26it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:01<00:00, 18.02it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:01<00:00, 17.75it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:01<00:00, 17.84it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:01<00:00, 18.01it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:01<00:00, 18.20it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:01<00:00, 18.36it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:01<00:00, 18.49it/s]\u001b[A\n","100%|██████████| 37/37 [00:02<00:00, 18.37it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n"," 11%|█         | 4/37 [00:00<00:01, 31.80it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:00<00:00, 33.31it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:00<00:00, 32.97it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:00<00:00, 32.64it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:00<00:00, 32.33it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:00<00:00, 32.32it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:00<00:00, 31.84it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:00<00:00, 32.22it/s]\u001b[A\n","100%|██████████| 37/37 [00:01<00:00, 32.10it/s]\n"," 80%|████████  | 8/10 [08:50<02:12, 66.40s/it]\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:02<01:12,  2.02s/it]\u001b[A\n","  5%|▌         | 2/37 [00:02<00:34,  1.01it/s]\u001b[A\n","  8%|▊         | 3/37 [00:02<00:22,  1.52it/s]\u001b[A\n"," 11%|█         | 4/37 [00:02<00:17,  1.94it/s]\u001b[A\n"," 14%|█▎        | 5/37 [00:03<00:13,  2.37it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:03<00:11,  2.62it/s]\u001b[A\n"," 19%|█▉        | 7/37 [00:03<00:11,  2.62it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:04<00:05,  4.64it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:04<00:06,  4.24it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:04<00:06,  4.13it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:04<00:06,  3.81it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:05<00:06,  3.72it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:05<00:04,  4.69it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:05<00:04,  4.72it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:06<00:04,  4.03it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:06<00:04,  3.94it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:06<00:04,  3.87it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:06<00:04,  3.55it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:07<00:04,  3.60it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:07<00:03,  3.63it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:07<00:01,  5.80it/s]\u001b[A\n"," 78%|███████▊  | 29/37 [00:08<00:01,  7.46it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:08<00:01,  6.37it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:08<00:01,  4.90it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:09<00:00,  4.92it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:09<00:00,  4.14it/s]\u001b[A\n"," 95%|█████████▍| 35/37 [00:09<00:00,  3.47it/s]\u001b[A\n"," 97%|█████████▋| 36/37 [00:10<00:00,  3.56it/s]\u001b[A\n","100%|██████████| 37/37 [00:10<00:00,  3.55it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:00<00:09,  3.70it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:09,  3.53it/s]\u001b[A\n","  8%|▊         | 3/37 [00:00<00:09,  3.64it/s]\u001b[A\n"," 11%|█         | 4/37 [00:01<00:09,  3.41it/s]\u001b[A\n"," 14%|█▎        | 5/37 [00:01<00:09,  3.48it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:01<00:10,  3.10it/s]\u001b[A\n"," 19%|█▉        | 7/37 [00:02<00:09,  3.17it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:02<00:09,  3.00it/s]\u001b[A\n"," 24%|██▍       | 9/37 [00:02<00:08,  3.19it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:03<00:08,  3.31it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:03<00:07,  3.37it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:03<00:07,  3.45it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:03<00:06,  3.59it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:04<00:06,  3.62it/s]\u001b[A\n"," 41%|████      | 15/37 [00:04<00:06,  3.37it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:04<00:06,  3.25it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:05<00:06,  3.28it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:05<00:06,  3.12it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:05<00:06,  2.87it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:06<00:06,  2.77it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:06<00:05,  2.86it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:06<00:04,  3.09it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:07<00:04,  3.40it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:07<00:03,  3.40it/s]\u001b[A\n"," 68%|██████▊   | 25/37 [00:07<00:03,  3.04it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:08<00:03,  3.26it/s]\u001b[A\n"," 73%|███████▎  | 27/37 [00:08<00:02,  3.34it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:08<00:02,  3.11it/s]\u001b[A\n"," 78%|███████▊  | 29/37 [00:09<00:02,  2.95it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:09<00:02,  3.12it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:09<00:01,  3.26it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:09<00:01,  3.33it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:10<00:01,  3.30it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:10<00:00,  3.37it/s]\u001b[A\n"," 95%|█████████▍| 35/37 [00:10<00:00,  3.37it/s]\u001b[A\n"," 97%|█████████▋| 36/37 [00:11<00:00,  3.08it/s]\u001b[A\n","100%|██████████| 37/37 [00:11<00:00,  3.20it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:00<00:11,  3.05it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:11,  3.06it/s]\u001b[A\n","  8%|▊         | 3/37 [00:01<00:12,  2.76it/s]\u001b[A\n"," 11%|█         | 4/37 [00:01<00:10,  3.03it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:01<00:05,  5.48it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:01<00:03,  7.92it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:01<00:02, 10.14it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:01<00:02, 12.07it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:01<00:01, 13.80it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:01<00:01, 15.24it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:02<00:01, 16.35it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:02<00:00, 17.29it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:02<00:00, 18.45it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:02<00:00, 19.18it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:02<00:00, 18.93it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:02<00:00, 19.47it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:02<00:00, 19.42it/s]\u001b[A\n"," 95%|█████████▍| 35/37 [00:02<00:00, 19.41it/s]\u001b[A\n","100%|██████████| 37/37 [00:03<00:00, 12.18it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:01, 19.47it/s]\u001b[A\n"," 11%|█         | 4/37 [00:00<00:01, 19.18it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:00<00:01, 18.40it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:00<00:01, 18.42it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:00<00:01, 17.94it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:00<00:01, 18.07it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:00<00:01, 17.13it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:00<00:01, 17.21it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:01<00:01, 17.72it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:01<00:00, 17.76it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:01<00:00, 17.90it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:01<00:00, 18.21it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:01<00:00, 18.13it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:01<00:00, 18.45it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:01<00:00, 18.20it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:01<00:00, 18.01it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:01<00:00, 18.09it/s]\u001b[A\n","100%|██████████| 37/37 [00:02<00:00, 17.89it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n"," 11%|█         | 4/37 [00:00<00:01, 32.21it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:00<00:00, 33.44it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:00<00:00, 33.61it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:00<00:00, 33.40it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:00<00:00, 32.80it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:00<00:00, 32.53it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:00<00:00, 32.22it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:00<00:00, 32.09it/s]\u001b[A\n","100%|██████████| 37/37 [00:01<00:00, 32.28it/s]\n"," 90%|█████████ | 9/10 [09:55<01:05, 65.98s/it]\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:02<01:19,  2.20s/it]\u001b[A\n","  5%|▌         | 2/37 [00:02<00:37,  1.08s/it]\u001b[A\n","  8%|▊         | 3/37 [00:02<00:24,  1.40it/s]\u001b[A\n"," 11%|█         | 4/37 [00:03<00:17,  1.88it/s]\u001b[A\n"," 14%|█▎        | 5/37 [00:03<00:14,  2.15it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:03<00:12,  2.48it/s]\u001b[A\n"," 19%|█▉        | 7/37 [00:04<00:12,  2.47it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:04<00:06,  4.43it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:04<00:06,  4.22it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:04<00:06,  4.00it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:05<00:06,  3.99it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:05<00:05,  3.86it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:05<00:04,  4.38it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:06<00:04,  4.17it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:06<00:04,  3.93it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:06<00:05,  3.57it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:07<00:05,  3.35it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:07<00:04,  3.50it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:07<00:04,  3.26it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:08<00:04,  3.08it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:08<00:02,  5.09it/s]\u001b[A\n"," 78%|███████▊  | 29/37 [00:08<00:01,  6.46it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:08<00:01,  5.59it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:09<00:01,  5.23it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:09<00:00,  5.70it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:09<00:00,  5.14it/s]\u001b[A\n"," 95%|█████████▍| 35/37 [00:10<00:00,  4.16it/s]\u001b[A\n"," 97%|█████████▋| 36/37 [00:10<00:00,  3.93it/s]\u001b[A\n","100%|██████████| 37/37 [00:10<00:00,  3.45it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:00<00:09,  3.72it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:11,  3.15it/s]\u001b[A\n","  8%|▊         | 3/37 [00:00<00:10,  3.32it/s]\u001b[A\n"," 11%|█         | 4/37 [00:01<00:09,  3.36it/s]\u001b[A\n"," 14%|█▎        | 5/37 [00:01<00:10,  2.96it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:01<00:09,  3.20it/s]\u001b[A\n"," 19%|█▉        | 7/37 [00:02<00:08,  3.39it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:02<00:08,  3.44it/s]\u001b[A\n"," 24%|██▍       | 9/37 [00:02<00:07,  3.53it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:02<00:07,  3.67it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:03<00:07,  3.61it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:03<00:07,  3.50it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:03<00:07,  3.11it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:04<00:07,  3.05it/s]\u001b[A\n"," 41%|████      | 15/37 [00:04<00:06,  3.20it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:04<00:06,  3.38it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:05<00:05,  3.34it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:05<00:05,  3.20it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:05<00:05,  3.22it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:06<00:05,  3.27it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:06<00:05,  3.12it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:06<00:04,  3.28it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:06<00:04,  3.30it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:07<00:04,  3.06it/s]\u001b[A\n"," 68%|██████▊   | 25/37 [00:07<00:04,  2.88it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:08<00:03,  2.98it/s]\u001b[A\n"," 73%|███████▎  | 27/37 [00:08<00:03,  3.09it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:08<00:02,  3.22it/s]\u001b[A\n"," 78%|███████▊  | 29/37 [00:08<00:02,  3.35it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:09<00:02,  3.41it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:09<00:01,  3.32it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:09<00:01,  2.98it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:10<00:01,  3.15it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:10<00:00,  3.23it/s]\u001b[A\n"," 95%|█████████▍| 35/37 [00:10<00:00,  3.30it/s]\u001b[A\n"," 97%|█████████▋| 36/37 [00:11<00:00,  3.45it/s]\u001b[A\n","100%|██████████| 37/37 [00:11<00:00,  3.26it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:00<00:12,  2.98it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:11,  3.01it/s]\u001b[A\n","  8%|▊         | 3/37 [00:01<00:12,  2.72it/s]\u001b[A\n"," 14%|█▎        | 5/37 [00:01<00:06,  4.79it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:01<00:06,  4.52it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:01<00:04,  6.86it/s]\u001b[A\n"," 24%|██▍       | 9/37 [00:02<00:06,  4.24it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:02<00:04,  6.12it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:02<00:03,  7.97it/s]\u001b[A\n"," 41%|████      | 15/37 [00:02<00:02, 10.02it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:02<00:01, 11.85it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:02<00:01, 13.59it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:02<00:00, 15.83it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:02<00:00, 16.74it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:03<00:00, 17.46it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:03<00:00, 17.64it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:03<00:00, 18.12it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:03<00:00, 18.35it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:03<00:00, 18.77it/s]\u001b[A\n","100%|██████████| 37/37 [00:03<00:00, 10.27it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:01, 19.55it/s]\u001b[A\n"," 11%|█         | 4/37 [00:00<00:01, 18.86it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:00<00:01, 18.88it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:00<00:01, 18.67it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:00<00:01, 18.74it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:00<00:01, 18.95it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:00<00:01, 19.05it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:00<00:01, 18.81it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:00<00:01, 18.24it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:01<00:00, 18.28it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:01<00:00, 18.00it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:01<00:00, 18.03it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:01<00:00, 17.97it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:01<00:00, 17.76it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:01<00:00, 18.10it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:01<00:00, 17.42it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:01<00:00, 17.63it/s]\u001b[A\n","100%|██████████| 37/37 [00:02<00:00, 18.15it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n"," 11%|█         | 4/37 [00:00<00:01, 32.36it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:00<00:00, 31.08it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:00<00:00, 31.17it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:00<00:00, 32.05it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:00<00:00, 32.24it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:00<00:00, 32.41it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:00<00:00, 32.67it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:00<00:00, 32.72it/s]\u001b[A\n","100%|██████████| 37/37 [00:01<00:00, 32.19it/s]\n","100%|██████████| 10/10 [11:01<00:00, 66.13s/it]\n"]}]},{"cell_type":"code","source":["# 이부분 세부적으로 다 바꿔야한다.\n","\n","\n","\n","for k in tqdm(range(10)):\n","\n","  globals()[f'set_df_{k}'] = pd.DataFrame()\n","  answer_df_list = glob(f'/content/drive/MyDrive/농산물예측/aT_data/model_output{try_cnt}/set_{k}/*.csv') # 예측한 결과 불러오기\n","  pum_list = glob(f'/content/drive/MyDrive/농산물예측/aT_data/aT_test_raw/sep_{k}/*.csv') # 기존 test input 불러오기\n","  pummok = [a for a in pum_list if 'pummok' in a.split('/')[-1]]\n","\n","  for i in answer_df_list:\n","    df = pd.read_csv(i)\n","    number = i.split('_')[-1].split('.')[0]\n","\n","    base_number = 0\n","    for p in pummok:\n","      if number == p.split('_')[-1].split('.')[0]:\n","        pum_df = pd.read_csv(p)\n","\n","        if len(pum_df) != 0:\n","           base_number = pum_df.iloc[len(pum_df)-1]['해당일자_전체평균가격(원)']  # 기존 각 sep 마다 test input의 마지막 target 값 가져오기 (변동률 계산을 위해)\n","        else:\n","          base_number = np.nan\n","\n","    globals()[f'set_df_{k}'][f'품목{number}']  = [base_number] + list(df[df.columns[-1]].values) # 각 품목당 순서를 t, t+1 ... t+28 로 변경\n","\n","  globals()[f'set_df_{k}'] = globals()[f'set_df_{k}'][[f'품목{col}' for col in range(37)]] # 열 순서를 품목0 ~ 품목36 으로 변경"],"metadata":{"id":"KFbNDeErZTRd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664373128493,"user_tz":-540,"elapsed":3758,"user":{"displayName":"이태영","userId":"15551978220746215296"}},"outputId":"37f7af12-7e19-4410-9dfe-c39680b90936"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:03<00:00,  2.95it/s]\n"]}]},{"cell_type":"code","source":["pd.options.display.max_columns=37\n","set_df_0"],"metadata":{"id":"LtnQ6kJsZTTf","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1664373420325,"user_tz":-540,"elapsed":371,"user":{"displayName":"이태영","userId":"15551978220746215296"}},"outputId":"bbc8e846-950a-427e-979d-af6b02b46d51"},"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["          품목0          품목1          품목2          품목3          품목4  \\\n","0   3871.1250  1362.117613  2909.783785  3400.075583  3947.809169   \n","1   4331.3030   993.348940  2158.246300  2067.298000  2258.714600   \n","2   4328.4040   991.515200  2204.182400  2078.231000  2259.344000   \n","3   4348.6160  1003.093500  2208.038600  2084.296600  2263.584000   \n","4   4397.5510  1004.224100  2218.305000  2086.166700  2261.692900   \n","5   4310.0820  1003.191160  2210.785200  2095.902800  2265.342500   \n","6   4335.8877  1001.183530  2228.288800  2103.104500  2257.629000   \n","7   4304.3135  1001.181150  2206.954800  2109.191200  2251.463900   \n","8   4364.2410  1000.988100  2234.770800  2108.298800  2245.187300   \n","9   4326.7160  1004.828740  2233.464000  2113.276400  2251.543500   \n","10  4378.7400  1010.901200  2233.260700  2128.209500  2240.857000   \n","11  4385.7793  1013.648900  2259.939200  2134.517600  2252.016000   \n","12  4290.3804  1013.598630  2265.859100  2148.380600  2252.589800   \n","13  4347.4340  1011.695700  2263.140100  2164.488800  2245.448700   \n","14  4416.3037  1006.678340  2285.891000  2170.733400  2245.113300   \n","15  4452.3843  1008.481900  2287.712000  2169.499500  2243.968800   \n","16  4451.8110  1006.445860  2284.341000  2195.471000  2231.948500   \n","17  4454.4830  1005.244140  2290.745600  2194.025100  2244.581000   \n","18  4387.8467  1003.084400  2320.633300  2190.598900  2239.028600   \n","19  4384.6963  1003.071000  2307.604700  2188.517000  2237.175000   \n","20  4416.5903  1008.955600  2348.050300  2195.814500  2238.983600   \n","21  4461.2397  1011.420960  2351.538800  2213.725300  2244.111000   \n","22  4406.6294  1012.660950  2342.397200  2212.802700  2236.927500   \n","23  4411.0370  1012.750370  2341.865500  2228.534200  2238.103800   \n","24  4380.3400  1015.620500  2346.759300  2228.083500  2240.845200   \n","25  4461.3420  1012.830140  2344.137700  2231.927200  2238.661100   \n","26  4427.5913  1017.291900  2371.251000  2238.393000  2245.266600   \n","27  4510.8520  1016.598270  2390.858000  2241.653300  2244.188200   \n","28  4493.2110  1014.832600  2388.304400  2251.250200  2240.945600   \n","\n","            품목5        품목6          품목7          품목8          품목9        품목10  \\\n","0   9253.947514  2717.2800  3361.030923  4911.899864  1173.018633  1337.03482   \n","1   5037.488300  3927.6104  2285.101800  3876.249500  1796.380000  1387.84070   \n","2   5028.175000  3955.0862  2317.143300  3851.359600  1776.671100  1384.71840   \n","3   5043.238300  4176.1690  2283.227500  3853.590300  1847.084500  1382.81870   \n","4   5034.169000  3943.2650  2288.268600  3901.203400  1859.200400  1379.69850   \n","5   5032.450000  4239.8380  2254.718500  3912.626200  1733.581300  1380.29700   \n","6   5025.945300  4382.5215  2253.151000  3917.410400  1729.824600  1384.30810   \n","7   5050.416000  4463.7427  2256.933000  3900.723400  1794.450100  1382.57910   \n","8   5026.674000  4640.7970  2255.644800  3899.213000  1748.787800  1379.78000   \n","9   5052.160000  4556.1807  2246.634800  3900.556000  1703.768300  1380.22270   \n","10  5051.953000  4577.8320  2239.525400  3921.298600  1806.311500  1379.26860   \n","11  5040.920000  4369.7524  2254.219500  3933.470700  1743.447800  1375.88720   \n","12  5053.707000  4358.9863  2276.035000  3949.408400  1755.928000  1376.27660   \n","13  5062.281000  4437.5527  2268.540300  3969.331500  1805.578200  1376.47360   \n","14  5053.157700  4485.4683  2295.463900  3983.677500  1704.892100  1371.65560   \n","15  5074.783000  4485.1987  2288.947800  4005.064700  1770.867300  1370.85140   \n","16  5089.102500  4585.3490  2293.488000  4003.746800  1737.462400  1370.06350   \n","17  5080.479000  4514.1450  2298.006800  3993.185300  1774.102000  1367.72680   \n","18  5054.666500  4460.6484  2306.259800  4010.631000  1743.205000  1370.36350   \n","19  5087.481400  4726.3887  2334.249800  4012.276600  1789.085000  1369.76800   \n","20  5034.053700  4733.5806  2366.470700  4044.081000  1715.064200  1365.10240   \n","21  5061.619000  4813.2970  2335.272200  4036.850300  1807.214800  1365.77550   \n","22  5050.917000  4893.0767  2373.211000  4068.927200  1809.864700  1365.26920   \n","23  5034.101600  4716.3770  2411.939500  4075.321000  1795.950700  1362.51680   \n","24  5049.471700  4873.6900  2390.129200  4061.261000  1719.436200  1364.90770   \n","25  5034.740000  5059.6064  2423.521200  4090.260700  1767.652800  1363.13130   \n","26  5021.249000  5250.4194  2409.111800  4120.391000  1792.356300  1360.45920   \n","27  5053.090300  5168.5996  2454.511500  4106.116700  1804.457000  1359.96790   \n","28  5037.051000  5126.5107  2482.737300  4117.031700  1793.084200  1359.79980   \n","\n","            품목11         품목12         품목13         품목14         품목15  \\\n","0    2053.354945  3003.205509  3148.414256  6573.839289  1089.126835   \n","1   24553.191000  2367.903800  2795.486300  5322.643000  1457.427100   \n","2   20636.832000  2352.043700  2789.008000  5008.429700  1458.851300   \n","3   23223.504000  2331.359900  2789.405800  4837.277300  1451.939500   \n","4   32001.658000  2337.411100  2702.119100  4623.895000  1445.064200   \n","5   22533.855000  2340.768600  2703.396700  5837.175000  1452.464600   \n","6   28180.980000  2319.991200  2671.378400  4362.984400  1450.346800   \n","7   27837.220000  2326.502400  2677.406200  4485.562500  1439.404500   \n","8   19392.533000  2319.920400  2699.442000  4451.930000  1435.479900   \n","9   24834.195000  2318.175800  2723.021000  4724.058600  1444.821500   \n","10  26638.701000  2325.565000  2721.543500  4413.550000  1443.328400   \n","11  22008.793000  2331.066200  2731.988000  4666.574000  1451.992000   \n","12  26069.883000  2320.289000  2720.106700  4879.695300  1461.772300   \n","13  24662.250000  2312.772200  2722.177000  4545.246000  1455.398600   \n","14  31155.135000  2309.793000  2740.548800  4432.281700  1463.751200   \n","15  29675.422000  2307.603500  2723.200400  4496.100600  1464.640500   \n","16  24630.986000  2290.695600  2708.603800  4993.760700  1454.877800   \n","17  28517.150000  2297.691200  2684.141600  4780.458000  1472.033000   \n","18  30741.863000  2291.526100  2611.952400  4727.948700  1473.810300   \n","19  27597.357000  2276.658400  2614.309600  4921.828000  1469.575900   \n","20  26981.450000  2283.438000  2601.401900  4742.366000  1470.577600   \n","21  23179.604000  2263.473600  2611.928700  4155.006000  1479.093400   \n","22  21170.738000  2275.638700  2607.519300  5297.480000  1479.346100   \n","23  22302.562000  2249.690200  2617.456500  4568.357400  1481.690100   \n","24  25379.370000  2235.643000  2666.634800  4953.052700  1492.500500   \n","25  18052.280000  2240.618400  2601.114000  4236.959500  1492.370100   \n","26  25926.469000  2234.169700  2591.945800  4117.864300  1475.576500   \n","27  24642.102000  2242.066200  2598.652300  5916.294400  1480.561400   \n","28  29860.494000  2233.807100  2637.772000  5202.982400  1477.309600   \n","\n","           품목16         품목17          품목18         품목19         품목20  \\\n","0   3838.815254  2254.390206  12538.931552  2990.863631  2965.711352   \n","1   4958.281000  1961.134300   8875.458000  2205.381300  2050.292700   \n","2   4958.810500  1965.998000   8937.922000  2237.840000  2376.152800   \n","3   4969.080600  1935.868200   8834.809000  2465.228500  2052.772700   \n","4   5021.134300  1933.196300   8882.368000  2198.077400  1898.874100   \n","5   5054.623000  1911.518400   9109.551000  2548.736300  2224.213600   \n","6   4962.264600  1906.888500   8692.572000  2131.103000  2131.067400   \n","7   5209.512700  1888.287100   8648.073000  2351.371600  2341.122600   \n","8   5076.598600  1895.973800   8375.496000  2612.342500  2331.237300   \n","9   5204.546000  1910.351400   8289.506000  2121.973600  2227.344000   \n","10  5239.818000  1899.361600   8547.578000  2133.871000  2192.250700   \n","11  5306.123000  1944.190800   8619.359000  2711.167000  2072.881800   \n","12  5141.770500  1952.782000   8815.769000  2428.482400  2324.527800   \n","13  5148.779000  1951.614100   8916.661000  2167.328100  2254.277600   \n","14  5259.502000  1963.886800   8833.325000  2369.062000  2375.375200   \n","15  5204.174000  1991.804300   8892.659000  2450.773400  2453.231000   \n","16  5132.739300  1992.953500   8970.880000  2124.751200  2200.927200   \n","17  5206.348000  2013.030000   8680.923000  2490.325000  2143.726600   \n","18  5168.343000  2011.347200   8307.694000  1921.838500  2587.722200   \n","19  5102.868700  2021.639800   8546.420000  2576.207300  2249.600000   \n","20  5187.773400  1999.002100   8560.265000  2284.275600  2169.863800   \n","21  5287.288000  1989.472900   8385.704000  2700.361600  2205.095200   \n","22  5145.146000  1979.398700   8952.221000  2119.307000  2349.020500   \n","23  5037.363300  1987.322400   8690.068000  2227.102800  2016.415300   \n","24  5175.467000  1971.655300   8461.120000  2400.449500  2218.394000   \n","25  5112.432000  1961.192300   8575.219000  2089.722700  2395.793500   \n","26  5162.740700  1938.184100   8623.569000  2511.206000  2244.237500   \n","27  5296.813000  1935.692700   8404.585000  2367.245400  2451.172000   \n","28  5122.673300  1915.363400   8385.728000  2142.205300  2303.162600   \n","\n","          품목21        품목22         품목23        품목24         품목25         품목26  \\\n","0   505.929304  937.367871  4147.240906  1231.11322  3323.910031  3056.787271   \n","1   373.716000  711.842900  3029.241200  1197.44360  2017.355300  1764.750700   \n","2   370.840730  713.489600  3008.336000  1149.67590  2049.184800  1760.335100   \n","3   369.723940  704.677100  2966.445300  1220.44920  2043.145300  1757.033200   \n","4   369.862900  700.531400  2976.676300  1173.57340  2027.523900  1746.036600   \n","5   369.122530  697.747000  3008.990500  1195.89540  2014.200100  1756.269800   \n","6   368.849240  701.011200  2994.429200  1042.95910  2022.519700  1754.149400   \n","7   371.424530  704.722900  3075.510700  1185.02200  2043.405500  1743.296400   \n","8   372.287540  707.774300  3132.410200   970.45520  2067.653300  1751.316200   \n","9   371.292800  708.926700  3166.308600  1124.10310  2093.050300  1754.428100   \n","10  375.304260  711.107060  3156.233000  1238.65000  2091.750500  1763.780300   \n","11  373.361700  702.844670  3125.526900  1088.32210  2074.936000  1757.506100   \n","12  376.283450  698.301640  3122.132300  1173.25920  2090.039800  1765.105200   \n","13  376.420200  701.469300  3133.055700  1670.96810  2072.743400  1746.263300   \n","14  377.563300  700.854250  3150.363800  1147.09830  2104.609000  1731.283400   \n","15  376.539000  702.422550  3178.556400  1083.85960  2135.345700  1744.403100   \n","16  379.273350  690.162800  3093.123800  1178.50730  2121.370400  1731.775000   \n","17  379.665950  692.035000  3092.315400  1275.37960  2129.809000  1738.935300   \n","18  382.993040  688.424800  3041.377000  1096.99460  2124.643800  1733.595500   \n","19  387.894400  696.763800  3079.418500  1077.67850  2129.980700  1730.092000   \n","20  389.150180  699.574600  3066.271000  1193.24900  2147.000500  1744.426300   \n","21  393.310420  696.505300  3080.458500  1440.09910  2141.301000  1767.006100   \n","22  396.494780  687.374630  3011.442900  1085.02360  2144.184600  1754.239700   \n","23  396.421630  693.789300  3006.818400  1046.73570  2139.967300  1769.924700   \n","24  397.426700  688.365360  3051.429400  1017.08940  2148.986300  1757.285600   \n","25  402.372620  696.487700  3104.315700  1355.89150  2158.472000  1759.276400   \n","26  405.697240  700.087600  3116.392300  1148.91460  2213.895500  1753.581200   \n","27  404.428620  688.598100  3139.081000   957.14060  2232.987800  1752.608400   \n","28  408.834230  690.701200  3075.499000  1077.60440  2243.037600  1731.641200   \n","\n","           품목27        품목28         품목29         품목30         품목31  \\\n","0   8640.811309  602.005658  1105.412623  1566.274239  3633.464557   \n","1   4544.730500  639.221000  1159.130500   873.735500  3258.399400   \n","2   4568.331000  612.393600  1165.316700   871.072940  3002.607400   \n","3   4576.788600  613.968930  1175.820800   868.591430  3314.302500   \n","4   4571.051300  628.153700  1154.141500   870.914100  3197.354200   \n","5   4554.143000  613.949000  1170.039600   867.592700  3264.183000   \n","6   4604.757300  608.853940  1163.180800   868.487800  3172.422000   \n","7   4596.709000  604.204100  1159.262700   864.708200  2997.270000   \n","8   4622.731000  626.219500  1185.085800   866.412230  3266.363500   \n","9   4615.487300  627.856140  1194.568600   870.791000  3115.854200   \n","10  4644.800300  614.851600  1180.596400   877.657100  3225.019500   \n","11  4627.535000  654.876100  1195.546100   880.614140  2901.059600   \n","12  4618.847700  639.367250  1188.123400   879.073500  2926.287600   \n","13  4615.461000  606.534240  1147.948900   876.332460  3091.318400   \n","14  4606.427000  597.856140  1158.868300   881.678800  3170.110800   \n","15  4627.774000  624.979550  1162.519300   880.505740  2815.181600   \n","16  4626.379000  624.571040  1159.222300   883.963400  3041.658200   \n","17  4605.157000  630.943540  1198.214100   890.369400  2842.203900   \n","18  4582.843300  611.735600  1159.324100   885.103940  2866.254200   \n","19  4567.817000  639.145300  1146.644500   885.199400  2865.125000   \n","20  4563.862300  607.136200  1173.683100   888.809630  2787.124800   \n","21  4615.544400  615.966200  1192.425800   887.927900  2854.000000   \n","22  4583.888000  609.141600  1156.614000   889.141050  2994.671900   \n","23  4611.416500  605.022200  1189.120100   893.342400  2935.377400   \n","24  4606.114700  602.284300  1177.384800   888.962770  3076.596700   \n","25  4609.574000  611.266500  1160.367000   888.898300  3010.917200   \n","26  4636.761000  615.649100  1188.551000   890.041300  3064.721000   \n","27  4608.690400  622.352400  1194.310300   886.021300  2700.038000   \n","28  4618.614300  605.870060  1174.000700   888.990000  2901.249300   \n","\n","           품목32         품목33         품목34         품목35         품목36  \n","0   5454.710444  5619.188362  5230.620027  2905.100888  2087.675036  \n","1   7211.023400  4309.042000  4826.796000  2192.981700  1488.974500  \n","2   6603.682600  4307.788600  4760.649400  2166.878400  1470.022000  \n","3   7261.082000  4310.624500  4809.882300  2153.314000  1456.241200  \n","4   6739.692400  4314.697300  4717.029300  2145.176000  1440.013300  \n","5   6572.834000  4338.635000  4680.273400  2122.151900  1427.626300  \n","6   7010.591300  4333.365700  4792.822300  2085.121000  1408.167500  \n","7   6600.131000  4351.463000  4635.328600  2052.381000  1406.300000  \n","8   6613.333000  4348.056000  4541.077000  2034.075200  1407.661600  \n","9   6725.372600  4370.799000  4810.010700  2048.822000  1415.212200  \n","10  7102.679000  4371.741000  4802.950000  2065.708000  1424.471000  \n","11  7125.097700  4384.424300  4743.325000  2086.928200  1420.007600  \n","12  6834.530300  4395.286600  4921.532000  2081.878700  1420.789700  \n","13  6473.540000  4399.036000  4772.990000  2065.869600  1421.969100  \n","14  7325.484000  4419.595000  4754.972000  2052.691700  1420.518600  \n","15  6443.945300  4431.491000  4705.385700  2058.623800  1427.654300  \n","16  6858.830000  4415.175000  4775.627000  2061.059800  1421.246000  \n","17  6662.406700  4435.136700  4586.609000  2051.627400  1415.696500  \n","18  7117.979500  4459.232000  4673.701700  2036.343300  1411.588400  \n","19  6739.371000  4457.160600  4647.592300  2036.594000  1405.246800  \n","20  7106.982400  4467.325000  4631.165000  2044.902500  1402.504300  \n","21  6790.073000  4476.116000  4616.513000  2054.382600  1392.652300  \n","22  7166.387000  4491.342300  4595.599600  2036.401600  1395.041400  \n","23  6800.929700  4497.812000  4647.694300  2053.938500  1401.152500  \n","24  6541.481400  4490.768600  4759.827600  2041.750000  1399.785800  \n","25  7481.287000  4511.704000  4653.213000  2031.020800  1400.906400  \n","26  6727.393600  4532.716300  4659.631000  2018.388300  1398.113500  \n","27  7045.590000  4538.401000  4569.978500  2019.485400  1399.887300  \n","28  7030.070300  4540.016000  4590.793500  2020.951300  1395.179900  "],"text/html":["\n","  <div id=\"df-943db4fb-bf89-418e-8ada-fc80015c0671\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>품목0</th>\n","      <th>품목1</th>\n","      <th>품목2</th>\n","      <th>품목3</th>\n","      <th>품목4</th>\n","      <th>품목5</th>\n","      <th>품목6</th>\n","      <th>품목7</th>\n","      <th>품목8</th>\n","      <th>품목9</th>\n","      <th>품목10</th>\n","      <th>품목11</th>\n","      <th>품목12</th>\n","      <th>품목13</th>\n","      <th>품목14</th>\n","      <th>품목15</th>\n","      <th>품목16</th>\n","      <th>품목17</th>\n","      <th>품목18</th>\n","      <th>품목19</th>\n","      <th>품목20</th>\n","      <th>품목21</th>\n","      <th>품목22</th>\n","      <th>품목23</th>\n","      <th>품목24</th>\n","      <th>품목25</th>\n","      <th>품목26</th>\n","      <th>품목27</th>\n","      <th>품목28</th>\n","      <th>품목29</th>\n","      <th>품목30</th>\n","      <th>품목31</th>\n","      <th>품목32</th>\n","      <th>품목33</th>\n","      <th>품목34</th>\n","      <th>품목35</th>\n","      <th>품목36</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3871.1250</td>\n","      <td>1362.117613</td>\n","      <td>2909.783785</td>\n","      <td>3400.075583</td>\n","      <td>3947.809169</td>\n","      <td>9253.947514</td>\n","      <td>2717.2800</td>\n","      <td>3361.030923</td>\n","      <td>4911.899864</td>\n","      <td>1173.018633</td>\n","      <td>1337.03482</td>\n","      <td>2053.354945</td>\n","      <td>3003.205509</td>\n","      <td>3148.414256</td>\n","      <td>6573.839289</td>\n","      <td>1089.126835</td>\n","      <td>3838.815254</td>\n","      <td>2254.390206</td>\n","      <td>12538.931552</td>\n","      <td>2990.863631</td>\n","      <td>2965.711352</td>\n","      <td>505.929304</td>\n","      <td>937.367871</td>\n","      <td>4147.240906</td>\n","      <td>1231.11322</td>\n","      <td>3323.910031</td>\n","      <td>3056.787271</td>\n","      <td>8640.811309</td>\n","      <td>602.005658</td>\n","      <td>1105.412623</td>\n","      <td>1566.274239</td>\n","      <td>3633.464557</td>\n","      <td>5454.710444</td>\n","      <td>5619.188362</td>\n","      <td>5230.620027</td>\n","      <td>2905.100888</td>\n","      <td>2087.675036</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4331.3030</td>\n","      <td>993.348940</td>\n","      <td>2158.246300</td>\n","      <td>2067.298000</td>\n","      <td>2258.714600</td>\n","      <td>5037.488300</td>\n","      <td>3927.6104</td>\n","      <td>2285.101800</td>\n","      <td>3876.249500</td>\n","      <td>1796.380000</td>\n","      <td>1387.84070</td>\n","      <td>24553.191000</td>\n","      <td>2367.903800</td>\n","      <td>2795.486300</td>\n","      <td>5322.643000</td>\n","      <td>1457.427100</td>\n","      <td>4958.281000</td>\n","      <td>1961.134300</td>\n","      <td>8875.458000</td>\n","      <td>2205.381300</td>\n","      <td>2050.292700</td>\n","      <td>373.716000</td>\n","      <td>711.842900</td>\n","      <td>3029.241200</td>\n","      <td>1197.44360</td>\n","      <td>2017.355300</td>\n","      <td>1764.750700</td>\n","      <td>4544.730500</td>\n","      <td>639.221000</td>\n","      <td>1159.130500</td>\n","      <td>873.735500</td>\n","      <td>3258.399400</td>\n","      <td>7211.023400</td>\n","      <td>4309.042000</td>\n","      <td>4826.796000</td>\n","      <td>2192.981700</td>\n","      <td>1488.974500</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4328.4040</td>\n","      <td>991.515200</td>\n","      <td>2204.182400</td>\n","      <td>2078.231000</td>\n","      <td>2259.344000</td>\n","      <td>5028.175000</td>\n","      <td>3955.0862</td>\n","      <td>2317.143300</td>\n","      <td>3851.359600</td>\n","      <td>1776.671100</td>\n","      <td>1384.71840</td>\n","      <td>20636.832000</td>\n","      <td>2352.043700</td>\n","      <td>2789.008000</td>\n","      <td>5008.429700</td>\n","      <td>1458.851300</td>\n","      <td>4958.810500</td>\n","      <td>1965.998000</td>\n","      <td>8937.922000</td>\n","      <td>2237.840000</td>\n","      <td>2376.152800</td>\n","      <td>370.840730</td>\n","      <td>713.489600</td>\n","      <td>3008.336000</td>\n","      <td>1149.67590</td>\n","      <td>2049.184800</td>\n","      <td>1760.335100</td>\n","      <td>4568.331000</td>\n","      <td>612.393600</td>\n","      <td>1165.316700</td>\n","      <td>871.072940</td>\n","      <td>3002.607400</td>\n","      <td>6603.682600</td>\n","      <td>4307.788600</td>\n","      <td>4760.649400</td>\n","      <td>2166.878400</td>\n","      <td>1470.022000</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4348.6160</td>\n","      <td>1003.093500</td>\n","      <td>2208.038600</td>\n","      <td>2084.296600</td>\n","      <td>2263.584000</td>\n","      <td>5043.238300</td>\n","      <td>4176.1690</td>\n","      <td>2283.227500</td>\n","      <td>3853.590300</td>\n","      <td>1847.084500</td>\n","      <td>1382.81870</td>\n","      <td>23223.504000</td>\n","      <td>2331.359900</td>\n","      <td>2789.405800</td>\n","      <td>4837.277300</td>\n","      <td>1451.939500</td>\n","      <td>4969.080600</td>\n","      <td>1935.868200</td>\n","      <td>8834.809000</td>\n","      <td>2465.228500</td>\n","      <td>2052.772700</td>\n","      <td>369.723940</td>\n","      <td>704.677100</td>\n","      <td>2966.445300</td>\n","      <td>1220.44920</td>\n","      <td>2043.145300</td>\n","      <td>1757.033200</td>\n","      <td>4576.788600</td>\n","      <td>613.968930</td>\n","      <td>1175.820800</td>\n","      <td>868.591430</td>\n","      <td>3314.302500</td>\n","      <td>7261.082000</td>\n","      <td>4310.624500</td>\n","      <td>4809.882300</td>\n","      <td>2153.314000</td>\n","      <td>1456.241200</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4397.5510</td>\n","      <td>1004.224100</td>\n","      <td>2218.305000</td>\n","      <td>2086.166700</td>\n","      <td>2261.692900</td>\n","      <td>5034.169000</td>\n","      <td>3943.2650</td>\n","      <td>2288.268600</td>\n","      <td>3901.203400</td>\n","      <td>1859.200400</td>\n","      <td>1379.69850</td>\n","      <td>32001.658000</td>\n","      <td>2337.411100</td>\n","      <td>2702.119100</td>\n","      <td>4623.895000</td>\n","      <td>1445.064200</td>\n","      <td>5021.134300</td>\n","      <td>1933.196300</td>\n","      <td>8882.368000</td>\n","      <td>2198.077400</td>\n","      <td>1898.874100</td>\n","      <td>369.862900</td>\n","      <td>700.531400</td>\n","      <td>2976.676300</td>\n","      <td>1173.57340</td>\n","      <td>2027.523900</td>\n","      <td>1746.036600</td>\n","      <td>4571.051300</td>\n","      <td>628.153700</td>\n","      <td>1154.141500</td>\n","      <td>870.914100</td>\n","      <td>3197.354200</td>\n","      <td>6739.692400</td>\n","      <td>4314.697300</td>\n","      <td>4717.029300</td>\n","      <td>2145.176000</td>\n","      <td>1440.013300</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>4310.0820</td>\n","      <td>1003.191160</td>\n","      <td>2210.785200</td>\n","      <td>2095.902800</td>\n","      <td>2265.342500</td>\n","      <td>5032.450000</td>\n","      <td>4239.8380</td>\n","      <td>2254.718500</td>\n","      <td>3912.626200</td>\n","      <td>1733.581300</td>\n","      <td>1380.29700</td>\n","      <td>22533.855000</td>\n","      <td>2340.768600</td>\n","      <td>2703.396700</td>\n","      <td>5837.175000</td>\n","      <td>1452.464600</td>\n","      <td>5054.623000</td>\n","      <td>1911.518400</td>\n","      <td>9109.551000</td>\n","      <td>2548.736300</td>\n","      <td>2224.213600</td>\n","      <td>369.122530</td>\n","      <td>697.747000</td>\n","      <td>3008.990500</td>\n","      <td>1195.89540</td>\n","      <td>2014.200100</td>\n","      <td>1756.269800</td>\n","      <td>4554.143000</td>\n","      <td>613.949000</td>\n","      <td>1170.039600</td>\n","      <td>867.592700</td>\n","      <td>3264.183000</td>\n","      <td>6572.834000</td>\n","      <td>4338.635000</td>\n","      <td>4680.273400</td>\n","      <td>2122.151900</td>\n","      <td>1427.626300</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>4335.8877</td>\n","      <td>1001.183530</td>\n","      <td>2228.288800</td>\n","      <td>2103.104500</td>\n","      <td>2257.629000</td>\n","      <td>5025.945300</td>\n","      <td>4382.5215</td>\n","      <td>2253.151000</td>\n","      <td>3917.410400</td>\n","      <td>1729.824600</td>\n","      <td>1384.30810</td>\n","      <td>28180.980000</td>\n","      <td>2319.991200</td>\n","      <td>2671.378400</td>\n","      <td>4362.984400</td>\n","      <td>1450.346800</td>\n","      <td>4962.264600</td>\n","      <td>1906.888500</td>\n","      <td>8692.572000</td>\n","      <td>2131.103000</td>\n","      <td>2131.067400</td>\n","      <td>368.849240</td>\n","      <td>701.011200</td>\n","      <td>2994.429200</td>\n","      <td>1042.95910</td>\n","      <td>2022.519700</td>\n","      <td>1754.149400</td>\n","      <td>4604.757300</td>\n","      <td>608.853940</td>\n","      <td>1163.180800</td>\n","      <td>868.487800</td>\n","      <td>3172.422000</td>\n","      <td>7010.591300</td>\n","      <td>4333.365700</td>\n","      <td>4792.822300</td>\n","      <td>2085.121000</td>\n","      <td>1408.167500</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>4304.3135</td>\n","      <td>1001.181150</td>\n","      <td>2206.954800</td>\n","      <td>2109.191200</td>\n","      <td>2251.463900</td>\n","      <td>5050.416000</td>\n","      <td>4463.7427</td>\n","      <td>2256.933000</td>\n","      <td>3900.723400</td>\n","      <td>1794.450100</td>\n","      <td>1382.57910</td>\n","      <td>27837.220000</td>\n","      <td>2326.502400</td>\n","      <td>2677.406200</td>\n","      <td>4485.562500</td>\n","      <td>1439.404500</td>\n","      <td>5209.512700</td>\n","      <td>1888.287100</td>\n","      <td>8648.073000</td>\n","      <td>2351.371600</td>\n","      <td>2341.122600</td>\n","      <td>371.424530</td>\n","      <td>704.722900</td>\n","      <td>3075.510700</td>\n","      <td>1185.02200</td>\n","      <td>2043.405500</td>\n","      <td>1743.296400</td>\n","      <td>4596.709000</td>\n","      <td>604.204100</td>\n","      <td>1159.262700</td>\n","      <td>864.708200</td>\n","      <td>2997.270000</td>\n","      <td>6600.131000</td>\n","      <td>4351.463000</td>\n","      <td>4635.328600</td>\n","      <td>2052.381000</td>\n","      <td>1406.300000</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>4364.2410</td>\n","      <td>1000.988100</td>\n","      <td>2234.770800</td>\n","      <td>2108.298800</td>\n","      <td>2245.187300</td>\n","      <td>5026.674000</td>\n","      <td>4640.7970</td>\n","      <td>2255.644800</td>\n","      <td>3899.213000</td>\n","      <td>1748.787800</td>\n","      <td>1379.78000</td>\n","      <td>19392.533000</td>\n","      <td>2319.920400</td>\n","      <td>2699.442000</td>\n","      <td>4451.930000</td>\n","      <td>1435.479900</td>\n","      <td>5076.598600</td>\n","      <td>1895.973800</td>\n","      <td>8375.496000</td>\n","      <td>2612.342500</td>\n","      <td>2331.237300</td>\n","      <td>372.287540</td>\n","      <td>707.774300</td>\n","      <td>3132.410200</td>\n","      <td>970.45520</td>\n","      <td>2067.653300</td>\n","      <td>1751.316200</td>\n","      <td>4622.731000</td>\n","      <td>626.219500</td>\n","      <td>1185.085800</td>\n","      <td>866.412230</td>\n","      <td>3266.363500</td>\n","      <td>6613.333000</td>\n","      <td>4348.056000</td>\n","      <td>4541.077000</td>\n","      <td>2034.075200</td>\n","      <td>1407.661600</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>4326.7160</td>\n","      <td>1004.828740</td>\n","      <td>2233.464000</td>\n","      <td>2113.276400</td>\n","      <td>2251.543500</td>\n","      <td>5052.160000</td>\n","      <td>4556.1807</td>\n","      <td>2246.634800</td>\n","      <td>3900.556000</td>\n","      <td>1703.768300</td>\n","      <td>1380.22270</td>\n","      <td>24834.195000</td>\n","      <td>2318.175800</td>\n","      <td>2723.021000</td>\n","      <td>4724.058600</td>\n","      <td>1444.821500</td>\n","      <td>5204.546000</td>\n","      <td>1910.351400</td>\n","      <td>8289.506000</td>\n","      <td>2121.973600</td>\n","      <td>2227.344000</td>\n","      <td>371.292800</td>\n","      <td>708.926700</td>\n","      <td>3166.308600</td>\n","      <td>1124.10310</td>\n","      <td>2093.050300</td>\n","      <td>1754.428100</td>\n","      <td>4615.487300</td>\n","      <td>627.856140</td>\n","      <td>1194.568600</td>\n","      <td>870.791000</td>\n","      <td>3115.854200</td>\n","      <td>6725.372600</td>\n","      <td>4370.799000</td>\n","      <td>4810.010700</td>\n","      <td>2048.822000</td>\n","      <td>1415.212200</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>4378.7400</td>\n","      <td>1010.901200</td>\n","      <td>2233.260700</td>\n","      <td>2128.209500</td>\n","      <td>2240.857000</td>\n","      <td>5051.953000</td>\n","      <td>4577.8320</td>\n","      <td>2239.525400</td>\n","      <td>3921.298600</td>\n","      <td>1806.311500</td>\n","      <td>1379.26860</td>\n","      <td>26638.701000</td>\n","      <td>2325.565000</td>\n","      <td>2721.543500</td>\n","      <td>4413.550000</td>\n","      <td>1443.328400</td>\n","      <td>5239.818000</td>\n","      <td>1899.361600</td>\n","      <td>8547.578000</td>\n","      <td>2133.871000</td>\n","      <td>2192.250700</td>\n","      <td>375.304260</td>\n","      <td>711.107060</td>\n","      <td>3156.233000</td>\n","      <td>1238.65000</td>\n","      <td>2091.750500</td>\n","      <td>1763.780300</td>\n","      <td>4644.800300</td>\n","      <td>614.851600</td>\n","      <td>1180.596400</td>\n","      <td>877.657100</td>\n","      <td>3225.019500</td>\n","      <td>7102.679000</td>\n","      <td>4371.741000</td>\n","      <td>4802.950000</td>\n","      <td>2065.708000</td>\n","      <td>1424.471000</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>4385.7793</td>\n","      <td>1013.648900</td>\n","      <td>2259.939200</td>\n","      <td>2134.517600</td>\n","      <td>2252.016000</td>\n","      <td>5040.920000</td>\n","      <td>4369.7524</td>\n","      <td>2254.219500</td>\n","      <td>3933.470700</td>\n","      <td>1743.447800</td>\n","      <td>1375.88720</td>\n","      <td>22008.793000</td>\n","      <td>2331.066200</td>\n","      <td>2731.988000</td>\n","      <td>4666.574000</td>\n","      <td>1451.992000</td>\n","      <td>5306.123000</td>\n","      <td>1944.190800</td>\n","      <td>8619.359000</td>\n","      <td>2711.167000</td>\n","      <td>2072.881800</td>\n","      <td>373.361700</td>\n","      <td>702.844670</td>\n","      <td>3125.526900</td>\n","      <td>1088.32210</td>\n","      <td>2074.936000</td>\n","      <td>1757.506100</td>\n","      <td>4627.535000</td>\n","      <td>654.876100</td>\n","      <td>1195.546100</td>\n","      <td>880.614140</td>\n","      <td>2901.059600</td>\n","      <td>7125.097700</td>\n","      <td>4384.424300</td>\n","      <td>4743.325000</td>\n","      <td>2086.928200</td>\n","      <td>1420.007600</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>4290.3804</td>\n","      <td>1013.598630</td>\n","      <td>2265.859100</td>\n","      <td>2148.380600</td>\n","      <td>2252.589800</td>\n","      <td>5053.707000</td>\n","      <td>4358.9863</td>\n","      <td>2276.035000</td>\n","      <td>3949.408400</td>\n","      <td>1755.928000</td>\n","      <td>1376.27660</td>\n","      <td>26069.883000</td>\n","      <td>2320.289000</td>\n","      <td>2720.106700</td>\n","      <td>4879.695300</td>\n","      <td>1461.772300</td>\n","      <td>5141.770500</td>\n","      <td>1952.782000</td>\n","      <td>8815.769000</td>\n","      <td>2428.482400</td>\n","      <td>2324.527800</td>\n","      <td>376.283450</td>\n","      <td>698.301640</td>\n","      <td>3122.132300</td>\n","      <td>1173.25920</td>\n","      <td>2090.039800</td>\n","      <td>1765.105200</td>\n","      <td>4618.847700</td>\n","      <td>639.367250</td>\n","      <td>1188.123400</td>\n","      <td>879.073500</td>\n","      <td>2926.287600</td>\n","      <td>6834.530300</td>\n","      <td>4395.286600</td>\n","      <td>4921.532000</td>\n","      <td>2081.878700</td>\n","      <td>1420.789700</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>4347.4340</td>\n","      <td>1011.695700</td>\n","      <td>2263.140100</td>\n","      <td>2164.488800</td>\n","      <td>2245.448700</td>\n","      <td>5062.281000</td>\n","      <td>4437.5527</td>\n","      <td>2268.540300</td>\n","      <td>3969.331500</td>\n","      <td>1805.578200</td>\n","      <td>1376.47360</td>\n","      <td>24662.250000</td>\n","      <td>2312.772200</td>\n","      <td>2722.177000</td>\n","      <td>4545.246000</td>\n","      <td>1455.398600</td>\n","      <td>5148.779000</td>\n","      <td>1951.614100</td>\n","      <td>8916.661000</td>\n","      <td>2167.328100</td>\n","      <td>2254.277600</td>\n","      <td>376.420200</td>\n","      <td>701.469300</td>\n","      <td>3133.055700</td>\n","      <td>1670.96810</td>\n","      <td>2072.743400</td>\n","      <td>1746.263300</td>\n","      <td>4615.461000</td>\n","      <td>606.534240</td>\n","      <td>1147.948900</td>\n","      <td>876.332460</td>\n","      <td>3091.318400</td>\n","      <td>6473.540000</td>\n","      <td>4399.036000</td>\n","      <td>4772.990000</td>\n","      <td>2065.869600</td>\n","      <td>1421.969100</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>4416.3037</td>\n","      <td>1006.678340</td>\n","      <td>2285.891000</td>\n","      <td>2170.733400</td>\n","      <td>2245.113300</td>\n","      <td>5053.157700</td>\n","      <td>4485.4683</td>\n","      <td>2295.463900</td>\n","      <td>3983.677500</td>\n","      <td>1704.892100</td>\n","      <td>1371.65560</td>\n","      <td>31155.135000</td>\n","      <td>2309.793000</td>\n","      <td>2740.548800</td>\n","      <td>4432.281700</td>\n","      <td>1463.751200</td>\n","      <td>5259.502000</td>\n","      <td>1963.886800</td>\n","      <td>8833.325000</td>\n","      <td>2369.062000</td>\n","      <td>2375.375200</td>\n","      <td>377.563300</td>\n","      <td>700.854250</td>\n","      <td>3150.363800</td>\n","      <td>1147.09830</td>\n","      <td>2104.609000</td>\n","      <td>1731.283400</td>\n","      <td>4606.427000</td>\n","      <td>597.856140</td>\n","      <td>1158.868300</td>\n","      <td>881.678800</td>\n","      <td>3170.110800</td>\n","      <td>7325.484000</td>\n","      <td>4419.595000</td>\n","      <td>4754.972000</td>\n","      <td>2052.691700</td>\n","      <td>1420.518600</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>4452.3843</td>\n","      <td>1008.481900</td>\n","      <td>2287.712000</td>\n","      <td>2169.499500</td>\n","      <td>2243.968800</td>\n","      <td>5074.783000</td>\n","      <td>4485.1987</td>\n","      <td>2288.947800</td>\n","      <td>4005.064700</td>\n","      <td>1770.867300</td>\n","      <td>1370.85140</td>\n","      <td>29675.422000</td>\n","      <td>2307.603500</td>\n","      <td>2723.200400</td>\n","      <td>4496.100600</td>\n","      <td>1464.640500</td>\n","      <td>5204.174000</td>\n","      <td>1991.804300</td>\n","      <td>8892.659000</td>\n","      <td>2450.773400</td>\n","      <td>2453.231000</td>\n","      <td>376.539000</td>\n","      <td>702.422550</td>\n","      <td>3178.556400</td>\n","      <td>1083.85960</td>\n","      <td>2135.345700</td>\n","      <td>1744.403100</td>\n","      <td>4627.774000</td>\n","      <td>624.979550</td>\n","      <td>1162.519300</td>\n","      <td>880.505740</td>\n","      <td>2815.181600</td>\n","      <td>6443.945300</td>\n","      <td>4431.491000</td>\n","      <td>4705.385700</td>\n","      <td>2058.623800</td>\n","      <td>1427.654300</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>4451.8110</td>\n","      <td>1006.445860</td>\n","      <td>2284.341000</td>\n","      <td>2195.471000</td>\n","      <td>2231.948500</td>\n","      <td>5089.102500</td>\n","      <td>4585.3490</td>\n","      <td>2293.488000</td>\n","      <td>4003.746800</td>\n","      <td>1737.462400</td>\n","      <td>1370.06350</td>\n","      <td>24630.986000</td>\n","      <td>2290.695600</td>\n","      <td>2708.603800</td>\n","      <td>4993.760700</td>\n","      <td>1454.877800</td>\n","      <td>5132.739300</td>\n","      <td>1992.953500</td>\n","      <td>8970.880000</td>\n","      <td>2124.751200</td>\n","      <td>2200.927200</td>\n","      <td>379.273350</td>\n","      <td>690.162800</td>\n","      <td>3093.123800</td>\n","      <td>1178.50730</td>\n","      <td>2121.370400</td>\n","      <td>1731.775000</td>\n","      <td>4626.379000</td>\n","      <td>624.571040</td>\n","      <td>1159.222300</td>\n","      <td>883.963400</td>\n","      <td>3041.658200</td>\n","      <td>6858.830000</td>\n","      <td>4415.175000</td>\n","      <td>4775.627000</td>\n","      <td>2061.059800</td>\n","      <td>1421.246000</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>4454.4830</td>\n","      <td>1005.244140</td>\n","      <td>2290.745600</td>\n","      <td>2194.025100</td>\n","      <td>2244.581000</td>\n","      <td>5080.479000</td>\n","      <td>4514.1450</td>\n","      <td>2298.006800</td>\n","      <td>3993.185300</td>\n","      <td>1774.102000</td>\n","      <td>1367.72680</td>\n","      <td>28517.150000</td>\n","      <td>2297.691200</td>\n","      <td>2684.141600</td>\n","      <td>4780.458000</td>\n","      <td>1472.033000</td>\n","      <td>5206.348000</td>\n","      <td>2013.030000</td>\n","      <td>8680.923000</td>\n","      <td>2490.325000</td>\n","      <td>2143.726600</td>\n","      <td>379.665950</td>\n","      <td>692.035000</td>\n","      <td>3092.315400</td>\n","      <td>1275.37960</td>\n","      <td>2129.809000</td>\n","      <td>1738.935300</td>\n","      <td>4605.157000</td>\n","      <td>630.943540</td>\n","      <td>1198.214100</td>\n","      <td>890.369400</td>\n","      <td>2842.203900</td>\n","      <td>6662.406700</td>\n","      <td>4435.136700</td>\n","      <td>4586.609000</td>\n","      <td>2051.627400</td>\n","      <td>1415.696500</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>4387.8467</td>\n","      <td>1003.084400</td>\n","      <td>2320.633300</td>\n","      <td>2190.598900</td>\n","      <td>2239.028600</td>\n","      <td>5054.666500</td>\n","      <td>4460.6484</td>\n","      <td>2306.259800</td>\n","      <td>4010.631000</td>\n","      <td>1743.205000</td>\n","      <td>1370.36350</td>\n","      <td>30741.863000</td>\n","      <td>2291.526100</td>\n","      <td>2611.952400</td>\n","      <td>4727.948700</td>\n","      <td>1473.810300</td>\n","      <td>5168.343000</td>\n","      <td>2011.347200</td>\n","      <td>8307.694000</td>\n","      <td>1921.838500</td>\n","      <td>2587.722200</td>\n","      <td>382.993040</td>\n","      <td>688.424800</td>\n","      <td>3041.377000</td>\n","      <td>1096.99460</td>\n","      <td>2124.643800</td>\n","      <td>1733.595500</td>\n","      <td>4582.843300</td>\n","      <td>611.735600</td>\n","      <td>1159.324100</td>\n","      <td>885.103940</td>\n","      <td>2866.254200</td>\n","      <td>7117.979500</td>\n","      <td>4459.232000</td>\n","      <td>4673.701700</td>\n","      <td>2036.343300</td>\n","      <td>1411.588400</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>4384.6963</td>\n","      <td>1003.071000</td>\n","      <td>2307.604700</td>\n","      <td>2188.517000</td>\n","      <td>2237.175000</td>\n","      <td>5087.481400</td>\n","      <td>4726.3887</td>\n","      <td>2334.249800</td>\n","      <td>4012.276600</td>\n","      <td>1789.085000</td>\n","      <td>1369.76800</td>\n","      <td>27597.357000</td>\n","      <td>2276.658400</td>\n","      <td>2614.309600</td>\n","      <td>4921.828000</td>\n","      <td>1469.575900</td>\n","      <td>5102.868700</td>\n","      <td>2021.639800</td>\n","      <td>8546.420000</td>\n","      <td>2576.207300</td>\n","      <td>2249.600000</td>\n","      <td>387.894400</td>\n","      <td>696.763800</td>\n","      <td>3079.418500</td>\n","      <td>1077.67850</td>\n","      <td>2129.980700</td>\n","      <td>1730.092000</td>\n","      <td>4567.817000</td>\n","      <td>639.145300</td>\n","      <td>1146.644500</td>\n","      <td>885.199400</td>\n","      <td>2865.125000</td>\n","      <td>6739.371000</td>\n","      <td>4457.160600</td>\n","      <td>4647.592300</td>\n","      <td>2036.594000</td>\n","      <td>1405.246800</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>4416.5903</td>\n","      <td>1008.955600</td>\n","      <td>2348.050300</td>\n","      <td>2195.814500</td>\n","      <td>2238.983600</td>\n","      <td>5034.053700</td>\n","      <td>4733.5806</td>\n","      <td>2366.470700</td>\n","      <td>4044.081000</td>\n","      <td>1715.064200</td>\n","      <td>1365.10240</td>\n","      <td>26981.450000</td>\n","      <td>2283.438000</td>\n","      <td>2601.401900</td>\n","      <td>4742.366000</td>\n","      <td>1470.577600</td>\n","      <td>5187.773400</td>\n","      <td>1999.002100</td>\n","      <td>8560.265000</td>\n","      <td>2284.275600</td>\n","      <td>2169.863800</td>\n","      <td>389.150180</td>\n","      <td>699.574600</td>\n","      <td>3066.271000</td>\n","      <td>1193.24900</td>\n","      <td>2147.000500</td>\n","      <td>1744.426300</td>\n","      <td>4563.862300</td>\n","      <td>607.136200</td>\n","      <td>1173.683100</td>\n","      <td>888.809630</td>\n","      <td>2787.124800</td>\n","      <td>7106.982400</td>\n","      <td>4467.325000</td>\n","      <td>4631.165000</td>\n","      <td>2044.902500</td>\n","      <td>1402.504300</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>4461.2397</td>\n","      <td>1011.420960</td>\n","      <td>2351.538800</td>\n","      <td>2213.725300</td>\n","      <td>2244.111000</td>\n","      <td>5061.619000</td>\n","      <td>4813.2970</td>\n","      <td>2335.272200</td>\n","      <td>4036.850300</td>\n","      <td>1807.214800</td>\n","      <td>1365.77550</td>\n","      <td>23179.604000</td>\n","      <td>2263.473600</td>\n","      <td>2611.928700</td>\n","      <td>4155.006000</td>\n","      <td>1479.093400</td>\n","      <td>5287.288000</td>\n","      <td>1989.472900</td>\n","      <td>8385.704000</td>\n","      <td>2700.361600</td>\n","      <td>2205.095200</td>\n","      <td>393.310420</td>\n","      <td>696.505300</td>\n","      <td>3080.458500</td>\n","      <td>1440.09910</td>\n","      <td>2141.301000</td>\n","      <td>1767.006100</td>\n","      <td>4615.544400</td>\n","      <td>615.966200</td>\n","      <td>1192.425800</td>\n","      <td>887.927900</td>\n","      <td>2854.000000</td>\n","      <td>6790.073000</td>\n","      <td>4476.116000</td>\n","      <td>4616.513000</td>\n","      <td>2054.382600</td>\n","      <td>1392.652300</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>4406.6294</td>\n","      <td>1012.660950</td>\n","      <td>2342.397200</td>\n","      <td>2212.802700</td>\n","      <td>2236.927500</td>\n","      <td>5050.917000</td>\n","      <td>4893.0767</td>\n","      <td>2373.211000</td>\n","      <td>4068.927200</td>\n","      <td>1809.864700</td>\n","      <td>1365.26920</td>\n","      <td>21170.738000</td>\n","      <td>2275.638700</td>\n","      <td>2607.519300</td>\n","      <td>5297.480000</td>\n","      <td>1479.346100</td>\n","      <td>5145.146000</td>\n","      <td>1979.398700</td>\n","      <td>8952.221000</td>\n","      <td>2119.307000</td>\n","      <td>2349.020500</td>\n","      <td>396.494780</td>\n","      <td>687.374630</td>\n","      <td>3011.442900</td>\n","      <td>1085.02360</td>\n","      <td>2144.184600</td>\n","      <td>1754.239700</td>\n","      <td>4583.888000</td>\n","      <td>609.141600</td>\n","      <td>1156.614000</td>\n","      <td>889.141050</td>\n","      <td>2994.671900</td>\n","      <td>7166.387000</td>\n","      <td>4491.342300</td>\n","      <td>4595.599600</td>\n","      <td>2036.401600</td>\n","      <td>1395.041400</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>4411.0370</td>\n","      <td>1012.750370</td>\n","      <td>2341.865500</td>\n","      <td>2228.534200</td>\n","      <td>2238.103800</td>\n","      <td>5034.101600</td>\n","      <td>4716.3770</td>\n","      <td>2411.939500</td>\n","      <td>4075.321000</td>\n","      <td>1795.950700</td>\n","      <td>1362.51680</td>\n","      <td>22302.562000</td>\n","      <td>2249.690200</td>\n","      <td>2617.456500</td>\n","      <td>4568.357400</td>\n","      <td>1481.690100</td>\n","      <td>5037.363300</td>\n","      <td>1987.322400</td>\n","      <td>8690.068000</td>\n","      <td>2227.102800</td>\n","      <td>2016.415300</td>\n","      <td>396.421630</td>\n","      <td>693.789300</td>\n","      <td>3006.818400</td>\n","      <td>1046.73570</td>\n","      <td>2139.967300</td>\n","      <td>1769.924700</td>\n","      <td>4611.416500</td>\n","      <td>605.022200</td>\n","      <td>1189.120100</td>\n","      <td>893.342400</td>\n","      <td>2935.377400</td>\n","      <td>6800.929700</td>\n","      <td>4497.812000</td>\n","      <td>4647.694300</td>\n","      <td>2053.938500</td>\n","      <td>1401.152500</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>4380.3400</td>\n","      <td>1015.620500</td>\n","      <td>2346.759300</td>\n","      <td>2228.083500</td>\n","      <td>2240.845200</td>\n","      <td>5049.471700</td>\n","      <td>4873.6900</td>\n","      <td>2390.129200</td>\n","      <td>4061.261000</td>\n","      <td>1719.436200</td>\n","      <td>1364.90770</td>\n","      <td>25379.370000</td>\n","      <td>2235.643000</td>\n","      <td>2666.634800</td>\n","      <td>4953.052700</td>\n","      <td>1492.500500</td>\n","      <td>5175.467000</td>\n","      <td>1971.655300</td>\n","      <td>8461.120000</td>\n","      <td>2400.449500</td>\n","      <td>2218.394000</td>\n","      <td>397.426700</td>\n","      <td>688.365360</td>\n","      <td>3051.429400</td>\n","      <td>1017.08940</td>\n","      <td>2148.986300</td>\n","      <td>1757.285600</td>\n","      <td>4606.114700</td>\n","      <td>602.284300</td>\n","      <td>1177.384800</td>\n","      <td>888.962770</td>\n","      <td>3076.596700</td>\n","      <td>6541.481400</td>\n","      <td>4490.768600</td>\n","      <td>4759.827600</td>\n","      <td>2041.750000</td>\n","      <td>1399.785800</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>4461.3420</td>\n","      <td>1012.830140</td>\n","      <td>2344.137700</td>\n","      <td>2231.927200</td>\n","      <td>2238.661100</td>\n","      <td>5034.740000</td>\n","      <td>5059.6064</td>\n","      <td>2423.521200</td>\n","      <td>4090.260700</td>\n","      <td>1767.652800</td>\n","      <td>1363.13130</td>\n","      <td>18052.280000</td>\n","      <td>2240.618400</td>\n","      <td>2601.114000</td>\n","      <td>4236.959500</td>\n","      <td>1492.370100</td>\n","      <td>5112.432000</td>\n","      <td>1961.192300</td>\n","      <td>8575.219000</td>\n","      <td>2089.722700</td>\n","      <td>2395.793500</td>\n","      <td>402.372620</td>\n","      <td>696.487700</td>\n","      <td>3104.315700</td>\n","      <td>1355.89150</td>\n","      <td>2158.472000</td>\n","      <td>1759.276400</td>\n","      <td>4609.574000</td>\n","      <td>611.266500</td>\n","      <td>1160.367000</td>\n","      <td>888.898300</td>\n","      <td>3010.917200</td>\n","      <td>7481.287000</td>\n","      <td>4511.704000</td>\n","      <td>4653.213000</td>\n","      <td>2031.020800</td>\n","      <td>1400.906400</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>4427.5913</td>\n","      <td>1017.291900</td>\n","      <td>2371.251000</td>\n","      <td>2238.393000</td>\n","      <td>2245.266600</td>\n","      <td>5021.249000</td>\n","      <td>5250.4194</td>\n","      <td>2409.111800</td>\n","      <td>4120.391000</td>\n","      <td>1792.356300</td>\n","      <td>1360.45920</td>\n","      <td>25926.469000</td>\n","      <td>2234.169700</td>\n","      <td>2591.945800</td>\n","      <td>4117.864300</td>\n","      <td>1475.576500</td>\n","      <td>5162.740700</td>\n","      <td>1938.184100</td>\n","      <td>8623.569000</td>\n","      <td>2511.206000</td>\n","      <td>2244.237500</td>\n","      <td>405.697240</td>\n","      <td>700.087600</td>\n","      <td>3116.392300</td>\n","      <td>1148.91460</td>\n","      <td>2213.895500</td>\n","      <td>1753.581200</td>\n","      <td>4636.761000</td>\n","      <td>615.649100</td>\n","      <td>1188.551000</td>\n","      <td>890.041300</td>\n","      <td>3064.721000</td>\n","      <td>6727.393600</td>\n","      <td>4532.716300</td>\n","      <td>4659.631000</td>\n","      <td>2018.388300</td>\n","      <td>1398.113500</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>4510.8520</td>\n","      <td>1016.598270</td>\n","      <td>2390.858000</td>\n","      <td>2241.653300</td>\n","      <td>2244.188200</td>\n","      <td>5053.090300</td>\n","      <td>5168.5996</td>\n","      <td>2454.511500</td>\n","      <td>4106.116700</td>\n","      <td>1804.457000</td>\n","      <td>1359.96790</td>\n","      <td>24642.102000</td>\n","      <td>2242.066200</td>\n","      <td>2598.652300</td>\n","      <td>5916.294400</td>\n","      <td>1480.561400</td>\n","      <td>5296.813000</td>\n","      <td>1935.692700</td>\n","      <td>8404.585000</td>\n","      <td>2367.245400</td>\n","      <td>2451.172000</td>\n","      <td>404.428620</td>\n","      <td>688.598100</td>\n","      <td>3139.081000</td>\n","      <td>957.14060</td>\n","      <td>2232.987800</td>\n","      <td>1752.608400</td>\n","      <td>4608.690400</td>\n","      <td>622.352400</td>\n","      <td>1194.310300</td>\n","      <td>886.021300</td>\n","      <td>2700.038000</td>\n","      <td>7045.590000</td>\n","      <td>4538.401000</td>\n","      <td>4569.978500</td>\n","      <td>2019.485400</td>\n","      <td>1399.887300</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>4493.2110</td>\n","      <td>1014.832600</td>\n","      <td>2388.304400</td>\n","      <td>2251.250200</td>\n","      <td>2240.945600</td>\n","      <td>5037.051000</td>\n","      <td>5126.5107</td>\n","      <td>2482.737300</td>\n","      <td>4117.031700</td>\n","      <td>1793.084200</td>\n","      <td>1359.79980</td>\n","      <td>29860.494000</td>\n","      <td>2233.807100</td>\n","      <td>2637.772000</td>\n","      <td>5202.982400</td>\n","      <td>1477.309600</td>\n","      <td>5122.673300</td>\n","      <td>1915.363400</td>\n","      <td>8385.728000</td>\n","      <td>2142.205300</td>\n","      <td>2303.162600</td>\n","      <td>408.834230</td>\n","      <td>690.701200</td>\n","      <td>3075.499000</td>\n","      <td>1077.60440</td>\n","      <td>2243.037600</td>\n","      <td>1731.641200</td>\n","      <td>4618.614300</td>\n","      <td>605.870060</td>\n","      <td>1174.000700</td>\n","      <td>888.990000</td>\n","      <td>2901.249300</td>\n","      <td>7030.070300</td>\n","      <td>4540.016000</td>\n","      <td>4590.793500</td>\n","      <td>2020.951300</td>\n","      <td>1395.179900</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-943db4fb-bf89-418e-8ada-fc80015c0671')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-943db4fb-bf89-418e-8ada-fc80015c0671 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-943db4fb-bf89-418e-8ada-fc80015c0671');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":["date = [f'd+{i}' for i in range(1,15)] + ['d+22 ~ 28 평균']\n","\n","\n","for k in range(10):\n","  globals()[f'answer_df_{k}'] = pd.DataFrame()\n","  for c in globals()[f'set_df_{k}'].columns:\n","    base_d = globals()[f'set_df_{k}'][c][0] # 변동률 기준 t 값\n","\n","    ans_1_14 = []\n","    for i in range(14):\n","      ans_1_14.append((globals()[f'set_df_{k}'][c].iloc[i+1]- base_d)/base_d)  # t+1 ~ t+14 까지는 (t+n - t)/t 로 계산\n","\n","    ans_22_28 = (globals()[f'set_df_{k}'][c][22:29].mean() - base_d)/base_d # t+22 ~ t+28은 np.mean(t+22 ~ t+28) - t / t\n","\n","    globals()[f'answer_df_{k}'][f'{c} 변동률'] = ans_1_14 + [ans_22_28]\n","  \n","  globals()[f'answer_df_{k}']['Set'] = k # set 번호 설정\n","  globals()[f'answer_df_{k}']['일자'] = date # 일자 설정"],"metadata":{"id":"po9UvRnbZTWO","executionInfo":{"status":"ok","timestamp":1664373145627,"user_tz":-540,"elapsed":993,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["# 위에서 계산된 변동률 들을 합쳐주는 과정\n","\n","all_df =pd.DataFrame()\n","for i in range(10):\n","  if i== 0 :\n","    all_df = pd.concat([all_df, globals()[f'answer_df_{i}']],axis=1)\n","  else:\n","    all_df = pd.concat([all_df, globals()[f'answer_df_{i}']])\n","\n","\n","all_df = all_df[['Set','일자'] + list(all_df.columns[:-2])]\n","all_df.reset_index(drop=True, inplace=True)"],"metadata":{"id":"W8oRs1htZTX-","executionInfo":{"status":"ok","timestamp":1664373147004,"user_tz":-540,"elapsed":1,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["# set, 일자 기억하기위해 따로 저장\n","\n","re_set = list(all_df['Set'])\n","re_date = list(all_df['일자'])\n","\n","\n","# 정답 양식 불러오기\n","out_ans = pd.read_csv('/content/drive/MyDrive/농산물예측/aT_data/answer_example.csv')\n","\n","# 두 dataframe 합치기 (nan + 숫자 = nan 이용)\n","submit_df = all_df + out_ans\n","\n","submit_df['Set'] = re_set\n","submit_df['일자'] = re_date\n","\n","\n","# 최종 저장\n","submit_df.to_csv(f'/content/drive/MyDrive/농산물예측/submit{try_cnt}.csv',index=False)"],"metadata":{"id":"PVskh79GZTaM","executionInfo":{"status":"ok","timestamp":1664373151451,"user_tz":-540,"elapsed":968,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["all_df"],"metadata":{"id":"eju9y7sWZTcT","colab":{"base_uri":"https://localhost:8080/","height":467},"executionInfo":{"status":"ok","timestamp":1664373154312,"user_tz":-540,"elapsed":1073,"user":{"displayName":"이태영","userId":"15551978220746215296"}},"outputId":"ecb6c8e8-783d-45db-c6a7-e122227c12cd"},"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     Set            일자   품목0 변동률   품목1 변동률   품목2 변동률   품목3 변동률   품목4 변동률  \\\n","0      0           d+1  0.118874 -0.270732 -0.258279 -0.391985 -0.427856   \n","1      0           d+2  0.118126 -0.272078 -0.242493 -0.388769 -0.427697   \n","2      0           d+3  0.123347 -0.263578 -0.241167 -0.386985 -0.426623   \n","3      0           d+4  0.135988 -0.262748 -0.237639 -0.386435 -0.427102   \n","4      0           d+5  0.113393 -0.263506 -0.240224 -0.383572 -0.426177   \n","..   ...           ...       ...       ...       ...       ...       ...   \n","145    9          d+11 -0.260949 -0.350212  0.513991 -0.112217 -0.374453   \n","146    9          d+12 -0.250499 -0.346660  0.518495 -0.110076 -0.376030   \n","147    9          d+13 -0.239476 -0.344385  0.536425 -0.104954 -0.377726   \n","148    9          d+14 -0.251981 -0.344121  0.557980 -0.103142 -0.376229   \n","149    9  d+22 ~ 28 평균 -0.249724 -0.344876  0.618534 -0.074654 -0.378794   \n","\n","      품목5 변동률   품목6 변동률   품목7 변동률  ...  품목27 변동률  품목28 변동률  품목29 변동률  \\\n","0   -0.455639  0.445420 -0.320119  ... -0.474039  0.061819  0.048595   \n","1   -0.456645  0.455531 -0.310586  ... -0.471308  0.017256  0.054192   \n","2   -0.455018  0.536893 -0.320676  ... -0.470329  0.019872  0.063694   \n","3   -0.455998  0.451181 -0.319177  ... -0.470993  0.043435  0.044082   \n","4   -0.456183  0.560324 -0.329159  ... -0.472950  0.019839  0.058464   \n","..        ...       ...       ...  ...       ...       ...       ...   \n","145  0.060436 -0.637192       NaN  ... -0.142992 -0.090743 -0.151957   \n","146  0.066678 -0.627177       NaN  ... -0.144263 -0.102340 -0.147161   \n","147  0.068783 -0.625062       NaN  ... -0.145321 -0.122137 -0.128823   \n","148  0.068552 -0.627929       NaN  ... -0.149444 -0.114474 -0.133465   \n","149  0.087105 -0.645735       NaN  ... -0.139393 -0.119656 -0.133975   \n","\n","     품목30 변동률  품목31 변동률  품목32 변동률  품목33 변동률  품목34 변동률  품목35 변동률  품목36 변동률  \n","0   -0.442157 -0.103225  0.321981 -0.233156 -0.077204 -0.245127 -0.286779  \n","1   -0.443857 -0.173624  0.210639 -0.233379 -0.089850 -0.254113 -0.295857  \n","2   -0.445441 -0.087840  0.331158 -0.232874 -0.080437 -0.258782 -0.302458  \n","3   -0.443958 -0.120026  0.235573 -0.232149 -0.098189 -0.261583 -0.310231  \n","4   -0.446079 -0.101633  0.204983 -0.227889 -0.105216 -0.269508 -0.316165  \n","..        ...       ...       ...       ...       ...       ...       ...  \n","145 -0.198864 -0.150941  0.172934  0.762823  1.609838  0.002992 -0.258633  \n","146 -0.196767 -0.148903  0.167046  0.768853  1.646364  0.000296 -0.261752  \n","147 -0.193320 -0.126914  0.141314  0.776342  1.580684  0.000681 -0.263712  \n","148 -0.188627 -0.119528  0.176468  0.772814  1.602368 -0.000997 -0.265522  \n","149 -0.183115 -0.171695  0.144778  0.824938  1.561416 -0.018376 -0.279471  \n","\n","[150 rows x 39 columns]"],"text/html":["\n","  <div id=\"df-8c74f0ac-92ed-406b-b035-204ad74feb22\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Set</th>\n","      <th>일자</th>\n","      <th>품목0 변동률</th>\n","      <th>품목1 변동률</th>\n","      <th>품목2 변동률</th>\n","      <th>품목3 변동률</th>\n","      <th>품목4 변동률</th>\n","      <th>품목5 변동률</th>\n","      <th>품목6 변동률</th>\n","      <th>품목7 변동률</th>\n","      <th>...</th>\n","      <th>품목27 변동률</th>\n","      <th>품목28 변동률</th>\n","      <th>품목29 변동률</th>\n","      <th>품목30 변동률</th>\n","      <th>품목31 변동률</th>\n","      <th>품목32 변동률</th>\n","      <th>품목33 변동률</th>\n","      <th>품목34 변동률</th>\n","      <th>품목35 변동률</th>\n","      <th>품목36 변동률</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>d+1</td>\n","      <td>0.118874</td>\n","      <td>-0.270732</td>\n","      <td>-0.258279</td>\n","      <td>-0.391985</td>\n","      <td>-0.427856</td>\n","      <td>-0.455639</td>\n","      <td>0.445420</td>\n","      <td>-0.320119</td>\n","      <td>...</td>\n","      <td>-0.474039</td>\n","      <td>0.061819</td>\n","      <td>0.048595</td>\n","      <td>-0.442157</td>\n","      <td>-0.103225</td>\n","      <td>0.321981</td>\n","      <td>-0.233156</td>\n","      <td>-0.077204</td>\n","      <td>-0.245127</td>\n","      <td>-0.286779</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>d+2</td>\n","      <td>0.118126</td>\n","      <td>-0.272078</td>\n","      <td>-0.242493</td>\n","      <td>-0.388769</td>\n","      <td>-0.427697</td>\n","      <td>-0.456645</td>\n","      <td>0.455531</td>\n","      <td>-0.310586</td>\n","      <td>...</td>\n","      <td>-0.471308</td>\n","      <td>0.017256</td>\n","      <td>0.054192</td>\n","      <td>-0.443857</td>\n","      <td>-0.173624</td>\n","      <td>0.210639</td>\n","      <td>-0.233379</td>\n","      <td>-0.089850</td>\n","      <td>-0.254113</td>\n","      <td>-0.295857</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>d+3</td>\n","      <td>0.123347</td>\n","      <td>-0.263578</td>\n","      <td>-0.241167</td>\n","      <td>-0.386985</td>\n","      <td>-0.426623</td>\n","      <td>-0.455018</td>\n","      <td>0.536893</td>\n","      <td>-0.320676</td>\n","      <td>...</td>\n","      <td>-0.470329</td>\n","      <td>0.019872</td>\n","      <td>0.063694</td>\n","      <td>-0.445441</td>\n","      <td>-0.087840</td>\n","      <td>0.331158</td>\n","      <td>-0.232874</td>\n","      <td>-0.080437</td>\n","      <td>-0.258782</td>\n","      <td>-0.302458</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>d+4</td>\n","      <td>0.135988</td>\n","      <td>-0.262748</td>\n","      <td>-0.237639</td>\n","      <td>-0.386435</td>\n","      <td>-0.427102</td>\n","      <td>-0.455998</td>\n","      <td>0.451181</td>\n","      <td>-0.319177</td>\n","      <td>...</td>\n","      <td>-0.470993</td>\n","      <td>0.043435</td>\n","      <td>0.044082</td>\n","      <td>-0.443958</td>\n","      <td>-0.120026</td>\n","      <td>0.235573</td>\n","      <td>-0.232149</td>\n","      <td>-0.098189</td>\n","      <td>-0.261583</td>\n","      <td>-0.310231</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>d+5</td>\n","      <td>0.113393</td>\n","      <td>-0.263506</td>\n","      <td>-0.240224</td>\n","      <td>-0.383572</td>\n","      <td>-0.426177</td>\n","      <td>-0.456183</td>\n","      <td>0.560324</td>\n","      <td>-0.329159</td>\n","      <td>...</td>\n","      <td>-0.472950</td>\n","      <td>0.019839</td>\n","      <td>0.058464</td>\n","      <td>-0.446079</td>\n","      <td>-0.101633</td>\n","      <td>0.204983</td>\n","      <td>-0.227889</td>\n","      <td>-0.105216</td>\n","      <td>-0.269508</td>\n","      <td>-0.316165</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>145</th>\n","      <td>9</td>\n","      <td>d+11</td>\n","      <td>-0.260949</td>\n","      <td>-0.350212</td>\n","      <td>0.513991</td>\n","      <td>-0.112217</td>\n","      <td>-0.374453</td>\n","      <td>0.060436</td>\n","      <td>-0.637192</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>-0.142992</td>\n","      <td>-0.090743</td>\n","      <td>-0.151957</td>\n","      <td>-0.198864</td>\n","      <td>-0.150941</td>\n","      <td>0.172934</td>\n","      <td>0.762823</td>\n","      <td>1.609838</td>\n","      <td>0.002992</td>\n","      <td>-0.258633</td>\n","    </tr>\n","    <tr>\n","      <th>146</th>\n","      <td>9</td>\n","      <td>d+12</td>\n","      <td>-0.250499</td>\n","      <td>-0.346660</td>\n","      <td>0.518495</td>\n","      <td>-0.110076</td>\n","      <td>-0.376030</td>\n","      <td>0.066678</td>\n","      <td>-0.627177</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>-0.144263</td>\n","      <td>-0.102340</td>\n","      <td>-0.147161</td>\n","      <td>-0.196767</td>\n","      <td>-0.148903</td>\n","      <td>0.167046</td>\n","      <td>0.768853</td>\n","      <td>1.646364</td>\n","      <td>0.000296</td>\n","      <td>-0.261752</td>\n","    </tr>\n","    <tr>\n","      <th>147</th>\n","      <td>9</td>\n","      <td>d+13</td>\n","      <td>-0.239476</td>\n","      <td>-0.344385</td>\n","      <td>0.536425</td>\n","      <td>-0.104954</td>\n","      <td>-0.377726</td>\n","      <td>0.068783</td>\n","      <td>-0.625062</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>-0.145321</td>\n","      <td>-0.122137</td>\n","      <td>-0.128823</td>\n","      <td>-0.193320</td>\n","      <td>-0.126914</td>\n","      <td>0.141314</td>\n","      <td>0.776342</td>\n","      <td>1.580684</td>\n","      <td>0.000681</td>\n","      <td>-0.263712</td>\n","    </tr>\n","    <tr>\n","      <th>148</th>\n","      <td>9</td>\n","      <td>d+14</td>\n","      <td>-0.251981</td>\n","      <td>-0.344121</td>\n","      <td>0.557980</td>\n","      <td>-0.103142</td>\n","      <td>-0.376229</td>\n","      <td>0.068552</td>\n","      <td>-0.627929</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>-0.149444</td>\n","      <td>-0.114474</td>\n","      <td>-0.133465</td>\n","      <td>-0.188627</td>\n","      <td>-0.119528</td>\n","      <td>0.176468</td>\n","      <td>0.772814</td>\n","      <td>1.602368</td>\n","      <td>-0.000997</td>\n","      <td>-0.265522</td>\n","    </tr>\n","    <tr>\n","      <th>149</th>\n","      <td>9</td>\n","      <td>d+22 ~ 28 평균</td>\n","      <td>-0.249724</td>\n","      <td>-0.344876</td>\n","      <td>0.618534</td>\n","      <td>-0.074654</td>\n","      <td>-0.378794</td>\n","      <td>0.087105</td>\n","      <td>-0.645735</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>-0.139393</td>\n","      <td>-0.119656</td>\n","      <td>-0.133975</td>\n","      <td>-0.183115</td>\n","      <td>-0.171695</td>\n","      <td>0.144778</td>\n","      <td>0.824938</td>\n","      <td>1.561416</td>\n","      <td>-0.018376</td>\n","      <td>-0.279471</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>150 rows × 39 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8c74f0ac-92ed-406b-b035-204ad74feb22')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-8c74f0ac-92ed-406b-b035-204ad74feb22 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-8c74f0ac-92ed-406b-b035-204ad74feb22');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["out_ans"],"metadata":{"id":"GPrw8UcObezM","colab":{"base_uri":"https://localhost:8080/","height":467},"executionInfo":{"status":"ok","timestamp":1664373157075,"user_tz":-540,"elapsed":513,"user":{"displayName":"이태영","userId":"15551978220746215296"}},"outputId":"193dd93b-8023-440a-abd3-71b8a9de7812"},"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     Set            일자  품목0 변동률  품목1 변동률  품목2 변동률  품목3 변동률  품목4 변동률  품목5 변동률  \\\n","0      0           d+1      0.0      0.0      0.0      0.0      0.0      0.0   \n","1      0           d+2      0.0      0.0      0.0      0.0      0.0      0.0   \n","2      0           d+3      NaN      NaN      NaN      NaN      NaN      NaN   \n","3      0           d+4      0.0      0.0      0.0      0.0      0.0      0.0   \n","4      0           d+5      0.0      0.0      0.0      0.0      0.0      0.0   \n","..   ...           ...      ...      ...      ...      ...      ...      ...   \n","145    9          d+11      0.0      0.0      0.0      0.0      0.0      0.0   \n","146    9          d+12      0.0      0.0      0.0      0.0      0.0      0.0   \n","147    9          d+13      0.0      0.0      0.0      0.0      0.0      0.0   \n","148    9          d+14      0.0      0.0      0.0      0.0      0.0      0.0   \n","149    9  d+22 ~ 28 평균      0.0      0.0      0.0      0.0      0.0      0.0   \n","\n","     품목6 변동률  품목7 변동률  ...  품목27 변동률  품목28 변동률  품목29 변동률  품목30 변동률  품목31 변동률  \\\n","0        0.0      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n","1        NaN      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n","2        0.0      NaN  ...       NaN       NaN       NaN       NaN       NaN   \n","3        NaN      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n","4        NaN      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n","..       ...      ...  ...       ...       ...       ...       ...       ...   \n","145      0.0      NaN  ...       0.0       0.0       0.0       0.0       0.0   \n","146      0.0      NaN  ...       0.0       0.0       0.0       0.0       0.0   \n","147      0.0      NaN  ...       0.0       0.0       0.0       0.0       0.0   \n","148      0.0      NaN  ...       0.0       0.0       0.0       0.0       0.0   \n","149      NaN      NaN  ...       0.0       0.0       0.0       0.0       0.0   \n","\n","     품목32 변동률  품목33 변동률  품목34 변동률  품목35 변동률  품목36 변동률  \n","0         0.0       0.0       0.0       0.0       0.0  \n","1         0.0       0.0       0.0       0.0       0.0  \n","2         NaN       NaN       NaN       NaN       NaN  \n","3         0.0       0.0       0.0       0.0       0.0  \n","4         0.0       0.0       0.0       0.0       0.0  \n","..        ...       ...       ...       ...       ...  \n","145       0.0       0.0       0.0       0.0       0.0  \n","146       0.0       0.0       0.0       0.0       0.0  \n","147       0.0       0.0       0.0       0.0       0.0  \n","148       0.0       0.0       0.0       0.0       0.0  \n","149       0.0       0.0       0.0       0.0       0.0  \n","\n","[150 rows x 39 columns]"],"text/html":["\n","  <div id=\"df-77f0d4e1-fa2e-428e-a082-655e9915d9ff\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Set</th>\n","      <th>일자</th>\n","      <th>품목0 변동률</th>\n","      <th>품목1 변동률</th>\n","      <th>품목2 변동률</th>\n","      <th>품목3 변동률</th>\n","      <th>품목4 변동률</th>\n","      <th>품목5 변동률</th>\n","      <th>품목6 변동률</th>\n","      <th>품목7 변동률</th>\n","      <th>...</th>\n","      <th>품목27 변동률</th>\n","      <th>품목28 변동률</th>\n","      <th>품목29 변동률</th>\n","      <th>품목30 변동률</th>\n","      <th>품목31 변동률</th>\n","      <th>품목32 변동률</th>\n","      <th>품목33 변동률</th>\n","      <th>품목34 변동률</th>\n","      <th>품목35 변동률</th>\n","      <th>품목36 변동률</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>d+1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>d+2</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>d+3</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>d+4</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>d+5</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>145</th>\n","      <td>9</td>\n","      <td>d+11</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>146</th>\n","      <td>9</td>\n","      <td>d+12</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>147</th>\n","      <td>9</td>\n","      <td>d+13</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>148</th>\n","      <td>9</td>\n","      <td>d+14</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>149</th>\n","      <td>9</td>\n","      <td>d+22 ~ 28 평균</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>150 rows × 39 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-77f0d4e1-fa2e-428e-a082-655e9915d9ff')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-77f0d4e1-fa2e-428e-a082-655e9915d9ff button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-77f0d4e1-fa2e-428e-a082-655e9915d9ff');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":43}]},{"cell_type":"code","source":["submit_df"],"metadata":{"id":"dkdtrqd_be1e","colab":{"base_uri":"https://localhost:8080/","height":467},"executionInfo":{"status":"ok","timestamp":1664373160857,"user_tz":-540,"elapsed":1158,"user":{"displayName":"이태영","userId":"15551978220746215296"}},"outputId":"bff1b382-3364-4401-bee2-44f5facdb703"},"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     Set            일자   품목0 변동률   품목1 변동률   품목2 변동률   품목3 변동률   품목4 변동률  \\\n","0      0           d+1  0.118874 -0.270732 -0.258279 -0.391985 -0.427856   \n","1      0           d+2  0.118126 -0.272078 -0.242493 -0.388769 -0.427697   \n","2      0           d+3       NaN       NaN       NaN       NaN       NaN   \n","3      0           d+4  0.135988 -0.262748 -0.237639 -0.386435 -0.427102   \n","4      0           d+5  0.113393 -0.263506 -0.240224 -0.383572 -0.426177   \n","..   ...           ...       ...       ...       ...       ...       ...   \n","145    9          d+11 -0.260949 -0.350212  0.513991 -0.112217 -0.374453   \n","146    9          d+12 -0.250499 -0.346660  0.518495 -0.110076 -0.376030   \n","147    9          d+13 -0.239476 -0.344385  0.536425 -0.104954 -0.377726   \n","148    9          d+14 -0.251981 -0.344121  0.557980 -0.103142 -0.376229   \n","149    9  d+22 ~ 28 평균 -0.249724 -0.344876  0.618534 -0.074654 -0.378794   \n","\n","      품목5 변동률   품목6 변동률   품목7 변동률  ...  품목27 변동률  품목28 변동률  품목29 변동률  \\\n","0   -0.455639  0.445420 -0.320119  ... -0.474039  0.061819  0.048595   \n","1   -0.456645       NaN -0.310586  ... -0.471308  0.017256  0.054192   \n","2         NaN  0.536893       NaN  ...       NaN       NaN       NaN   \n","3   -0.455998       NaN -0.319177  ... -0.470993  0.043435  0.044082   \n","4   -0.456183       NaN -0.329159  ... -0.472950  0.019839  0.058464   \n","..        ...       ...       ...  ...       ...       ...       ...   \n","145  0.060436 -0.637192       NaN  ... -0.142992 -0.090743 -0.151957   \n","146  0.066678 -0.627177       NaN  ... -0.144263 -0.102340 -0.147161   \n","147  0.068783 -0.625062       NaN  ... -0.145321 -0.122137 -0.128823   \n","148  0.068552 -0.627929       NaN  ... -0.149444 -0.114474 -0.133465   \n","149  0.087105       NaN       NaN  ... -0.139393 -0.119656 -0.133975   \n","\n","     품목30 변동률  품목31 변동률  품목32 변동률  품목33 변동률  품목34 변동률  품목35 변동률  품목36 변동률  \n","0   -0.442157 -0.103225  0.321981 -0.233156 -0.077204 -0.245127 -0.286779  \n","1   -0.443857 -0.173624  0.210639 -0.233379 -0.089850 -0.254113 -0.295857  \n","2         NaN       NaN       NaN       NaN       NaN       NaN       NaN  \n","3   -0.443958 -0.120026  0.235573 -0.232149 -0.098189 -0.261583 -0.310231  \n","4   -0.446079 -0.101633  0.204983 -0.227889 -0.105216 -0.269508 -0.316165  \n","..        ...       ...       ...       ...       ...       ...       ...  \n","145 -0.198864 -0.150941  0.172934  0.762823  1.609838  0.002992 -0.258633  \n","146 -0.196767 -0.148903  0.167046  0.768853  1.646364  0.000296 -0.261752  \n","147 -0.193320 -0.126914  0.141314  0.776342  1.580684  0.000681 -0.263712  \n","148 -0.188627 -0.119528  0.176468  0.772814  1.602368 -0.000997 -0.265522  \n","149 -0.183115 -0.171695  0.144778  0.824938  1.561416 -0.018376 -0.279471  \n","\n","[150 rows x 39 columns]"],"text/html":["\n","  <div id=\"df-b400af2b-4a2d-4fa1-8d6e-c210e402ed93\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Set</th>\n","      <th>일자</th>\n","      <th>품목0 변동률</th>\n","      <th>품목1 변동률</th>\n","      <th>품목2 변동률</th>\n","      <th>품목3 변동률</th>\n","      <th>품목4 변동률</th>\n","      <th>품목5 변동률</th>\n","      <th>품목6 변동률</th>\n","      <th>품목7 변동률</th>\n","      <th>...</th>\n","      <th>품목27 변동률</th>\n","      <th>품목28 변동률</th>\n","      <th>품목29 변동률</th>\n","      <th>품목30 변동률</th>\n","      <th>품목31 변동률</th>\n","      <th>품목32 변동률</th>\n","      <th>품목33 변동률</th>\n","      <th>품목34 변동률</th>\n","      <th>품목35 변동률</th>\n","      <th>품목36 변동률</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>d+1</td>\n","      <td>0.118874</td>\n","      <td>-0.270732</td>\n","      <td>-0.258279</td>\n","      <td>-0.391985</td>\n","      <td>-0.427856</td>\n","      <td>-0.455639</td>\n","      <td>0.445420</td>\n","      <td>-0.320119</td>\n","      <td>...</td>\n","      <td>-0.474039</td>\n","      <td>0.061819</td>\n","      <td>0.048595</td>\n","      <td>-0.442157</td>\n","      <td>-0.103225</td>\n","      <td>0.321981</td>\n","      <td>-0.233156</td>\n","      <td>-0.077204</td>\n","      <td>-0.245127</td>\n","      <td>-0.286779</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>d+2</td>\n","      <td>0.118126</td>\n","      <td>-0.272078</td>\n","      <td>-0.242493</td>\n","      <td>-0.388769</td>\n","      <td>-0.427697</td>\n","      <td>-0.456645</td>\n","      <td>NaN</td>\n","      <td>-0.310586</td>\n","      <td>...</td>\n","      <td>-0.471308</td>\n","      <td>0.017256</td>\n","      <td>0.054192</td>\n","      <td>-0.443857</td>\n","      <td>-0.173624</td>\n","      <td>0.210639</td>\n","      <td>-0.233379</td>\n","      <td>-0.089850</td>\n","      <td>-0.254113</td>\n","      <td>-0.295857</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>d+3</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.536893</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>d+4</td>\n","      <td>0.135988</td>\n","      <td>-0.262748</td>\n","      <td>-0.237639</td>\n","      <td>-0.386435</td>\n","      <td>-0.427102</td>\n","      <td>-0.455998</td>\n","      <td>NaN</td>\n","      <td>-0.319177</td>\n","      <td>...</td>\n","      <td>-0.470993</td>\n","      <td>0.043435</td>\n","      <td>0.044082</td>\n","      <td>-0.443958</td>\n","      <td>-0.120026</td>\n","      <td>0.235573</td>\n","      <td>-0.232149</td>\n","      <td>-0.098189</td>\n","      <td>-0.261583</td>\n","      <td>-0.310231</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>d+5</td>\n","      <td>0.113393</td>\n","      <td>-0.263506</td>\n","      <td>-0.240224</td>\n","      <td>-0.383572</td>\n","      <td>-0.426177</td>\n","      <td>-0.456183</td>\n","      <td>NaN</td>\n","      <td>-0.329159</td>\n","      <td>...</td>\n","      <td>-0.472950</td>\n","      <td>0.019839</td>\n","      <td>0.058464</td>\n","      <td>-0.446079</td>\n","      <td>-0.101633</td>\n","      <td>0.204983</td>\n","      <td>-0.227889</td>\n","      <td>-0.105216</td>\n","      <td>-0.269508</td>\n","      <td>-0.316165</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>145</th>\n","      <td>9</td>\n","      <td>d+11</td>\n","      <td>-0.260949</td>\n","      <td>-0.350212</td>\n","      <td>0.513991</td>\n","      <td>-0.112217</td>\n","      <td>-0.374453</td>\n","      <td>0.060436</td>\n","      <td>-0.637192</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>-0.142992</td>\n","      <td>-0.090743</td>\n","      <td>-0.151957</td>\n","      <td>-0.198864</td>\n","      <td>-0.150941</td>\n","      <td>0.172934</td>\n","      <td>0.762823</td>\n","      <td>1.609838</td>\n","      <td>0.002992</td>\n","      <td>-0.258633</td>\n","    </tr>\n","    <tr>\n","      <th>146</th>\n","      <td>9</td>\n","      <td>d+12</td>\n","      <td>-0.250499</td>\n","      <td>-0.346660</td>\n","      <td>0.518495</td>\n","      <td>-0.110076</td>\n","      <td>-0.376030</td>\n","      <td>0.066678</td>\n","      <td>-0.627177</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>-0.144263</td>\n","      <td>-0.102340</td>\n","      <td>-0.147161</td>\n","      <td>-0.196767</td>\n","      <td>-0.148903</td>\n","      <td>0.167046</td>\n","      <td>0.768853</td>\n","      <td>1.646364</td>\n","      <td>0.000296</td>\n","      <td>-0.261752</td>\n","    </tr>\n","    <tr>\n","      <th>147</th>\n","      <td>9</td>\n","      <td>d+13</td>\n","      <td>-0.239476</td>\n","      <td>-0.344385</td>\n","      <td>0.536425</td>\n","      <td>-0.104954</td>\n","      <td>-0.377726</td>\n","      <td>0.068783</td>\n","      <td>-0.625062</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>-0.145321</td>\n","      <td>-0.122137</td>\n","      <td>-0.128823</td>\n","      <td>-0.193320</td>\n","      <td>-0.126914</td>\n","      <td>0.141314</td>\n","      <td>0.776342</td>\n","      <td>1.580684</td>\n","      <td>0.000681</td>\n","      <td>-0.263712</td>\n","    </tr>\n","    <tr>\n","      <th>148</th>\n","      <td>9</td>\n","      <td>d+14</td>\n","      <td>-0.251981</td>\n","      <td>-0.344121</td>\n","      <td>0.557980</td>\n","      <td>-0.103142</td>\n","      <td>-0.376229</td>\n","      <td>0.068552</td>\n","      <td>-0.627929</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>-0.149444</td>\n","      <td>-0.114474</td>\n","      <td>-0.133465</td>\n","      <td>-0.188627</td>\n","      <td>-0.119528</td>\n","      <td>0.176468</td>\n","      <td>0.772814</td>\n","      <td>1.602368</td>\n","      <td>-0.000997</td>\n","      <td>-0.265522</td>\n","    </tr>\n","    <tr>\n","      <th>149</th>\n","      <td>9</td>\n","      <td>d+22 ~ 28 평균</td>\n","      <td>-0.249724</td>\n","      <td>-0.344876</td>\n","      <td>0.618534</td>\n","      <td>-0.074654</td>\n","      <td>-0.378794</td>\n","      <td>0.087105</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>-0.139393</td>\n","      <td>-0.119656</td>\n","      <td>-0.133975</td>\n","      <td>-0.183115</td>\n","      <td>-0.171695</td>\n","      <td>0.144778</td>\n","      <td>0.824938</td>\n","      <td>1.561416</td>\n","      <td>-0.018376</td>\n","      <td>-0.279471</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>150 rows × 39 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b400af2b-4a2d-4fa1-8d6e-c210e402ed93')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-b400af2b-4a2d-4fa1-8d6e-c210e402ed93 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-b400af2b-4a2d-4fa1-8d6e-c210e402ed93');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["# 테스트 데이터 적용할거\n","\n","# 정규표현식\n","# OLS feature selection\n","# 스케일링 마지막"],"metadata":{"id":"w0EpPCJ3TmT9"},"execution_count":null,"outputs":[]}]}