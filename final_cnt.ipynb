{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyPKfkWp3EyfmNEer3yfJbuz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1GbUXzCUKbHD","executionInfo":{"status":"ok","timestamp":1664503956189,"user_tz":-540,"elapsed":19279,"user":{"displayName":"이태영","userId":"15551978220746215296"}},"outputId":"547cf4a4-a7a9-4225-a0ae-42aee16180cf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install talib-binary"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VLsHhbKGKkso","executionInfo":{"status":"ok","timestamp":1664503961704,"user_tz":-540,"elapsed":4728,"user":{"displayName":"이태영","userId":"15551978220746215296"}},"outputId":"1f55a208-d26b-4786-fdb9-ba2ed45d28b8"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting talib-binary\n","  Downloading talib_binary-0.4.19-cp37-cp37m-manylinux1_x86_64.whl (2.4 MB)\n","\u001b[K     |████████████████████████████████| 2.4 MB 9.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from talib-binary) (1.21.6)\n","Installing collected packages: talib-binary\n","Successfully installed talib-binary-0.4.19\n"]}]},{"cell_type":"code","source":["!pip install pandasql"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Aj00v7gKKlmX","executionInfo":{"status":"ok","timestamp":1664503966354,"user_tz":-540,"elapsed":4657,"user":{"displayName":"이태영","userId":"15551978220746215296"}},"outputId":"240f5e50-4fca-4475-a248-ab01c343f008"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pandasql\n","  Downloading pandasql-0.7.3.tar.gz (26 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pandasql) (1.21.6)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from pandasql) (1.3.5)\n","Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.7/dist-packages (from pandasql) (1.4.41)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pandasql) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pandasql) (2022.2.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->pandasql) (1.15.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->pandasql) (1.1.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->pandasql) (4.12.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy->pandasql) (3.8.1)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy->pandasql) (4.1.1)\n","Building wheels for collected packages: pandasql\n","  Building wheel for pandasql (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pandasql: filename=pandasql-0.7.3-py3-none-any.whl size=26784 sha256=708bbb33999cc633d434f6a4b1abfa11822b2f587ac5c94b5b364b57ee3bd10b\n","  Stored in directory: /root/.cache/pip/wheels/5c/4b/ec/41f4e116c8053c3654e2c2a47c62b4fca34cc67ef7b55deb7f\n","Successfully built pandasql\n","Installing collected packages: pandasql\n","Successfully installed pandasql-0.7.3\n"]}]},{"cell_type":"code","source":["!pip install fracdiff"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":495},"id":"EL6Ne8_3LXvb","executionInfo":{"status":"ok","timestamp":1664504164392,"user_tz":-540,"elapsed":9277,"user":{"displayName":"이태영","userId":"15551978220746215296"}},"outputId":"b635ba0e-6675-4384-ef73-f8e08377fcb6"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting fracdiff\n","  Downloading fracdiff-0.8.0-py3-none-any.whl (13 kB)\n","Requirement already satisfied: numpy<2.0.0,>=1.21.3 in /usr/local/lib/python3.7/dist-packages (from fracdiff) (1.21.6)\n","Requirement already satisfied: scipy<2.0.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from fracdiff) (1.7.3)\n","Collecting statsmodels<0.14.0,>=0.13.0\n","  Downloading statsmodels-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n","\u001b[K     |████████████████████████████████| 9.8 MB 10.1 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.7/dist-packages (from statsmodels<0.14.0,>=0.13.0->fracdiff) (21.3)\n","Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.7/dist-packages (from statsmodels<0.14.0,>=0.13.0->fracdiff) (1.3.5)\n","Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.7/dist-packages (from statsmodels<0.14.0,>=0.13.0->fracdiff) (0.5.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=21.3->statsmodels<0.14.0,>=0.13.0->fracdiff) (3.0.9)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25->statsmodels<0.14.0,>=0.13.0->fracdiff) (2022.2.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25->statsmodels<0.14.0,>=0.13.0->fracdiff) (2.8.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.5.2->statsmodels<0.14.0,>=0.13.0->fracdiff) (1.15.0)\n","Installing collected packages: statsmodels, fracdiff\n","  Attempting uninstall: statsmodels\n","    Found existing installation: statsmodels 0.12.2\n","    Uninstalling statsmodels-0.12.2:\n","      Successfully uninstalled statsmodels-0.12.2\n","Successfully installed fracdiff-0.8.0 statsmodels-0.13.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["statsmodels"]}}},"metadata":{}}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","import warnings\n","from glob import glob\n","import tensorflow as tf\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from sklearn.model_selection import train_test_split\n","from pandasql import sqldf\n","from sklearn.preprocessing import StandardScaler\n","import os\n","import warnings\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","warnings.filterwarnings(action='ignore')\n","from matplotlib import font_manager, rc\n","from sklearn.model_selection import train_test_split\n","import statsmodels.api as sm\n","\n","\n","from fracdiff import *\n","# font_path = \"C:\\\\Windows\\\\Fonts\\\\\\x7f\\x7f\\x7f\\x7fBOLD.TTF\"\n","# font = font_manager.FontProperties(fname=font_path).get_name()\n","# rc('font', family=font)\n","from sklearn.manifold import TSNE\n","import re\n","import talib as tb"],"metadata":{"id":"aD-UgHulKmZQ","executionInfo":{"status":"ok","timestamp":1664504169171,"user_tz":-540,"elapsed":5,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["def weather(주소,name = 'train'):\n","    t=0\n","    if name =='test':\n","        test_dir = 주소.split('/')[-2][-1]\n","    for i in tqdm(range(0,37)):\n","        globals()[f'{name}_weather_'+str(i)]=pd.DataFrame()\n","\n","        for k in range(0,3):\n","            try:\n","                weather = pd.read_csv(주소+f'/weather_{i}_{t}.csv')\n","                if len(weather)==0:\n","                    weather = pd.read_csv(주소+f'/weather_0_0.csv')\n","                    weather=weather.set_index('datadate')\n","                    weather[weather.columns] = 0\n","                    weather=weather.reset_index()\n","                weather = weather.set_index('datadate')\n","                weather['습도(%)']=weather['습도(%)'].apply(lambda x : np.nan if x==' ' else x)\n","                weather['습도(%)']=weather['습도(%)'].astype('float')\n","                weather.columns = weather.columns+f'_{k%3}'\n","                globals()[f'{name}_weather_'+str(i)]=pd.merge(globals()[f'{name}_weather_'+str(i)],weather,how='right',left_index=True, right_index=True)    \n","\n","\n","            except:\n","                break\n","            t+=1\n","\n","\n","\n","        globals()[f'{name}_weather_'+str(i)]=globals()[f'{name}_weather_'+str(i)].reset_index()\n","        cols = globals()[f'{name}_weather_'+str(i)].columns\n","        globals()[f'{name}_weather_'+str(i)]['월일']=globals()[f'{name}_weather_'+str(i)]['datadate']%10000\n","       \n","\n","        \n","        if name == 'train':\n","            qwe =  globals()[f'{name}_weather_'+str(i)].groupby('월일').mean().reset_index()\n","            qwe.columns = qwe.columns+'_평균'\n","            globals()[f'train_weather_평균_'+str(i)]= qwe\n","\n","\n","\n","            \n","\n","\n","        globals()[f'{name}_weather_'+str(i)] = pd.merge(globals()[f'{name}_weather_'+str(i)],globals()[f'train_weather_평균_'+str(i)],how='left',left_on = '월일',right_on='월일_평균')\n","        for c in cols:\n","            globals()[f'{name}_weather_'+str(i)][c]=globals()[f'{name}_weather_'+str(i)].apply(lambda x: x[c] if np.isnan(x[c]) == False else (0 if np.isnan(x[c+'_평균']) else x[c+'_평균']),axis=1)\n","                \n","    \n","        \n","        \n","        if name=='test':\n","            globals()['sep_'+test_dir+f'_{name}_weather_'+str(i)] = globals()[f'{name}_weather_'+str(i)][cols]\n","\n","        else:\n","            globals()[f'{name}_weather_'+str(i)] = globals()[f'{name}_weather_'+str(i)][cols]\n","\n","        "],"metadata":{"id":"GWPAy8MYKnby","executionInfo":{"status":"ok","timestamp":1664504171216,"user_tz":-540,"elapsed":4,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["def add_dosomae(주소,name='train', option=1):\n","        if name == 'test':\n","            test_dir = 주소.split('/')[-2][-1]\n","        data_list = glob(주소+'*') # train raw 데이터 넣을지 / test raw 데이터 넣을지 경로\n","        domae = []\n","        somae = []\n","\n","        for i in data_list:\n","            if 'domae' in i:\n","                domae.append(i)\n","            if 'somae' in i:\n","                somae.append(i)\n","        \n","        sum_df = pd.DataFrame()\n","                \n","        if option == 1:\n","            df = domae\n","            text = 'domae'\n","        else:\n","            df = somae\n","            text = 'somae'\n","\n","\n","        for i in tqdm(df):\n","            test = pd.read_csv(i)\n","            \n","            k=i.split('/')[-1].split(\"_\")[1].split(\".\")[0]\n","            test.fillna(0,inplace=True) # 널값 0으로 채워주고\n","            if len(test)==0:\n","                test = pd.read_csv(주소+f'/domae_0.csv')\n","                test=test.set_index('datadate')\n","                test[test.columns] = 0\n","                test=test.reset_index()\n","                \n","\n","            \n","            test['조사단위(kg)'] = test['조사단위(kg)'].replace('g$|kg$|개|포기|접', '',regex=True)\n","            test=test.astype({'조사단위(kg)':'float'})\n","            test.loc[test['조사단위(kg)'] >= 100, '단위당가격'] = test['가격(원)']* test['조사단위(kg)'] /1000\n","            test.loc[test['조사단위(kg)'] < 100, '단위당가격'] = test['가격(원)']/ test['조사단위(kg)']\n","            \n","            sep = test.loc[(test['등급명'] == '상품') | (test['등급명'] == 'S과') | (test['등급명'] == 0)]   # 모든 상품에 대해서 수행하지 않고 GRAD_NM이 '상품', 'S과' 만 해당하는 품목 가져옴\n","            sep = sep[['datadate', '등급명', '조사단위(kg)', '가격(원)']]\n","\n","            sep.rename(columns={\"가격(원)\": \"가격\"}, inplace=True)\n","            sep2 = sqldf(\n","                f\"select datadate, max(가격) as '일자별_{text}가격_최대(원)', avg(가격) as '일자별_{text}가격_평균(원)', min(가격) as '일자별_{text}가격_최소(원)' from sep group by datadate\")\n","\n","            sep2.fillna(0,inplace=True)\n","#             if len(sep2) == 0:\n","#                 if name=='test':\n","#                     sep2 =  globals()['sep_'+test_dir+f'_{name}_{text}_0'] \n","#                     sep2\n","\n","#                 else:\n","#                     sep2 =  globals()[f'{name}_{text}_0'] \n","#                     sep2[sep2.columns] = 0 \n","                \n","                \n","            if name=='test':\n","                globals()['sep_'+test_dir+f'_{name}_{text}_{k}'] = sep2\n","                \n","            else:\n","                globals()[f'{name}_{text}_{k}'] = sep2\n"," \n","                \n","\n","\n","        \n","                               \n","               \n","             "],"metadata":{"id":"nMzVnk9KKra2","executionInfo":{"status":"ok","timestamp":1664504171659,"user_tz":-540,"elapsed":1,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["def pummok(주소,name='train'):\n","    if name == 'test':\n","        test_dir = 주소.split('/')[-2][-1]\n","    for i in tqdm(range(0,37)):\n","\n","        try:\n","            pummok = pd.read_csv(주소+f'/pummok_{i}.csv')\n","            if len(pummok) == 0 :\n","                pummok = pd.read_csv(주소+f'/pummok_0.csv')\n","                pummok = pummok.groupby('datadate').mean()\n","                pummok = pummok[['단가(원)','거래량','해당일자_전체평균가격(원)']]\n","                pummok[pummok.columns] = 0\n","                pummok = pummok.reset_index()\n","                \n","            else:   \n","                pummok = pummok.groupby('datadate').mean()\n","                pummok = pummok[['단가(원)','거래량','해당일자_전체평균가격(원)']]\n","\n","                # 새로 추가한 부분\n","                pummok['해당일자_전체평균가격(원)'].fillna(0, inplace=True)\n","                pummok['wma14'] =  tb.WMA(pummok['해당일자_전체평균가격(원)'], 14)\n","                pummok['wma14'].fillna(method='bfill', inplace=True)\n","\n","                # wma * 1.2 적용  -> 그림이 제일 비슷하게 나온다.\n","                pummok['해당일자_전체평균가격(원)'] = np.where(pummok['해당일자_전체평균가격(원)']==0, pummok['wma14']*1.2, pummok['해당일자_전체평균가격(원)'])\n","                pummok.drop('wma14', axis=1, inplace=True)\n","\n","\n","                \n","                pummok = pummok.reset_index()\n","                pummok.fillna(0, inplace = True)\n","                # pummok.fillna(0,inplace = True)\n","                \n","                \n","                \n","\n","        except:\n","            continue\n","\n","        if name=='test':\n","            globals()['sep_'+test_dir+f'_{name}_pummok_{i}'] = pummok\n","\n","        else:\n","            globals()[f'{name}_pummok_{i}'] = pummok\n"],"metadata":{"id":"H2LS4MDGKrdC","executionInfo":{"status":"ok","timestamp":1664504173404,"user_tz":-540,"elapsed":2,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["def imexport(주소, name='train'):\n","    if name == 'test':\n","        test_dir = 주소.split('/')[-2][-1]\n","    for i in tqdm(range(0,37)):\n","        try:\n","            imexport=pd.read_csv(주소+f'/imexport_{i}.csv')\n","            \n","            if len(imexport) == 0:\n","                imexport = pd.read_csv(주소+f'/imexport_0.csv')\n","                imexport = imexport.groupby('datadate').mean()\n","                imexport[imexport.columns] = 0\n","\n","            else:  \n","                imexport = imexport.groupby('datadate').mean()\n","\n","\n","        except:\n","            \n","            imexport = pd.read_csv(주소+f'/imexport_0.csv')\n","            imexport = imexport.groupby('datadate').mean()\n","            imexport[imexport.columns] = 0\n","            pass\n","\n","\n","        imexport.fillna(0,inplace=True)\n","        if name=='test':\n","            globals()['sep_'+test_dir+f'_{name}_imexport_{i}'] = imexport\n","\n","        else:\n","            globals()[f'{name}_imexport_{i}'] = imexport\n","\n","\n","    \n","    "],"metadata":{"id":"KaT52A8UKrfR","executionInfo":{"status":"ok","timestamp":1664504175077,"user_tz":-540,"elapsed":4,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["def make_csv(주소,name = 'train'):\n","    imexport(주소,name)\n","    pummok(주소,name)\n","    add_dosomae(주소,name)\n","    add_dosomae(주소,name,2)\n","    weather(주소,name)\n","    for i in range(0,37):\n","        if name == 'test':\n","\n","            num = 주소.split('/')[-2][-1]\n","            \n","            \n","            temp = globals()[f'sep_{num}_{name}_pummok_{i}']\n","\n","            temp1 = globals()[f'sep_{num}_{name}_weather_{i}']\n","\n","            temp2 = globals()[f'sep_{num}_{name}_domae_{i}']\n","   \n","            temp3 = globals()[f'sep_{num}_{name}_somae_{i}']\n","\n","            temp4 = globals()[f'sep_{num}_{name}_imexport_{i}']\n","            temp.set_index('datadate',inplace=True)\n","            temp1.set_index('datadate',inplace=True)\n","            temp2.set_index('datadate',inplace=True)\n","            temp3.set_index('datadate',inplace=True)\n","\n","            temp = temp.join(temp1)\n","            temp = temp.join(temp2)\n","            temp = temp.join(temp3)\n","            temp.reset_index(inplace = True)\n","            temp['datadate'] = temp['datadate'].astype('str')\n","            temp['yyyymm'] =  temp['datadate'].apply(lambda x: x[:6] )\n","            temp4.reset_index(inplace=True)\n","            temp4['yyyymm'] = temp4['datadate'].astype('str')\n","            temp4.drop('datadate',inplace=True,axis=1)\n","            temp = temp.merge(temp4, how='left',on='yyyymm')\n","            temp.set_index('datadate',inplace = True)\n","            temp.drop('yyyymm',inplace=True,axis=1)\n","            a = fdiff(temp['해당일자_전체평균가격(원)'], n=0.35)\n","            diff = pd.DataFrame(a, index=temp.index, columns=[\"fracdiff\"])\n","            temp['fracdiff'] = diff['fracdiff']\n","\n","\n","            temp.fillna(0, inplace = True) # 내가 추가한 코드\n","            globals()[f'{name}_total_sep_{num}_{i}'] = temp\n","\n","\n","        else:\n","            temp = globals()[f'{name}_pummok_{i}']\n","            temp1 = globals()[f'{name}_weather_{i}']\n","            temp2 = globals()[f'{name}_domae_{i}']\n","            temp3 = globals()[f'{name}_somae_{i}']\n","            temp4 = globals()[f'{name}_imexport_{i}']\n","            temp.set_index('datadate',inplace=True)\n","            temp1.set_index('datadate',inplace=True)\n","            temp2.set_index('datadate',inplace=True)\n","            temp3.set_index('datadate',inplace=True)\n","            temp = temp.join(temp1)\n","            temp = temp.join(temp2)\n","            temp = temp.join(temp3)\n","            temp.reset_index(inplace = True)\n","            temp['datadate'] = temp['datadate'].astype('str')\n","            temp['yyyymm'] =  temp['datadate'].apply(lambda x: x[:6] )\n","            temp4.reset_index(inplace=True)\n","            temp4['yyyymm'] = temp4['datadate'].astype('str')\n","            temp4.drop('datadate',inplace=True,axis=1)\n","            temp = temp.merge(temp4, how='left',on='yyyymm')\n","            temp.set_index('datadate',inplace = True)\n","            temp.drop('yyyymm',inplace=True,axis=1)\n","            a = fdiff(temp['해당일자_전체평균가격(원)'], n=0.35)\n","            diff = pd.DataFrame(a, index=temp.index, columns=[\"fracdiff\"])\n","            temp['fracdiff'] = diff['fracdiff']\n","\n","            temp.fillna(0, inplace = True)\n","            globals()[f'{name}_total_{i}'] = temp\n","        \n","    "],"metadata":{"id":"tpRFC4-aKrhl","executionInfo":{"status":"ok","timestamp":1664504177900,"user_tz":-540,"elapsed":3,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# train_data만들기 train_total_0 ~ train_total_36까지 만들어준다.\n","make_csv('/content/drive/MyDrive/농산물예측/aT_data/aT_train_raw/','train')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GU6FatMZKrjp","executionInfo":{"status":"ok","timestamp":1664504251897,"user_tz":-540,"elapsed":68113,"user":{"displayName":"이태영","userId":"15551978220746215296"}},"outputId":"d7d057b8-6fec-4335-a19a-e02262e923d2"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 37/37 [00:00<00:00, 187.74it/s]\n","100%|██████████| 37/37 [00:11<00:00,  3.16it/s]\n","100%|██████████| 37/37 [00:06<00:00,  5.82it/s]\n","100%|██████████| 37/37 [00:22<00:00,  1.61it/s]\n","100%|██████████| 37/37 [00:25<00:00,  1.43it/s]\n"]}]},{"cell_type":"code","source":["def make_smoothing(df):\n","  df['해당일자_전체평균가격(원)'] = tb.WMA(df['해당일자_전체평균가격(원)'], 7)\n","  df.fillna(method='bfill', inplace=True)\n","  return df"],"metadata":{"id":"vMHV9fdmKrl_","executionInfo":{"status":"ok","timestamp":1664504251898,"user_tz":-540,"elapsed":12,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# train_data smoothing 해주기~\n","for i in range(37):\n","  globals()[f'train_smoothing_{i}'] = make_smoothing(globals()[f'train_total_{i}'])"],"metadata":{"id":"UWU9LhlhK2vR","executionInfo":{"status":"ok","timestamp":1664504251898,"user_tz":-540,"elapsed":11,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# nan이 생겼나 확인\n","for i in range(37):\n","  print(len(globals()[f'train_smoothing_{i}']), globals()[f'train_smoothing_{i}'].isna().sum().sum())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zR9KR4rRK7F3","executionInfo":{"status":"ok","timestamp":1664504251899,"user_tz":-540,"elapsed":11,"user":{"displayName":"이태영","userId":"15551978220746215296"}},"outputId":"055a26a5-cde3-46d0-b6b5-d862c874b450"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["1461 0\n","1461 0\n","1461 0\n","1461 0\n","1461 0\n","1461 0\n","1461 0\n","1461 0\n","1461 0\n","1461 0\n","1461 0\n","1461 0\n","1461 0\n","1461 0\n","1461 0\n","1461 0\n","1461 0\n","1461 0\n","1461 0\n","1461 0\n","1461 0\n","1461 0\n","1461 0\n","1461 0\n","1461 0\n","1461 0\n","1461 0\n","1461 0\n","1461 0\n","1461 0\n","1461 0\n","1461 0\n","1461 0\n","1461 0\n","1461 0\n","1461 0\n","1461 0\n"]}]},{"cell_type":"code","source":["# OLS Feature selection을 위한 함수\n","def OLS_report(df, seed):\n","    \n","    \n","    \n","    if len(df.columns)<len(train_smoothing_0.columns): # 주산지2의 날씨 데이터가 없다. 예외처리\n","      model = sm.OLS.from_formula(\"해당일자_전체평균가격 ~단가+거래량+초기온도_0+최대온도_0+최저온도_0+ 평균온도_0+강수량_0+ 습도_0+초기온도_1+최대온도_1+최저온도_1+ 평균온도_1+강수량_1+ 습도_1+일자별_domae가격_최대+일자별_domae가격_평균+ 일자별_domae가격_최소+ 일자별_somae가격_최대+일자별_somae가격_평균+ 일자별_somae가격_최소+ 수출중량+ 수출금액+ 수입중량+ 수입금액+무역수지\"\n","                                ,data=df)\n","    else:\n","      model = sm.OLS.from_formula(\"해당일자_전체평균가격 ~단가+거래량+초기온도_0+최대온도_0+최저온도_0+ 평균온도_0+강수량_0+ 습도_0+초기온도_1+최대온도_1+최저온도_1+ 평균온도_1+강수량_1+ 습도_1+초기온도_2+최대온도_2+최저온도_2+ 평균온도_2+강수량_2+ 습도_2 +일자별_domae가격_최대+일자별_domae가격_평균+ 일자별_domae가격_최소+ 일자별_somae가격_최대+일자별_somae가격_평균+ 일자별_somae가격_최소+ 수출중량+ 수출금액+ 수입중량+ 수입금액+무역수지\"\n","                                ,data=df)\n","    return model.fit()\n","\n","\n","\n","\n","def results_summary_to_dataframe(results):\n","    '''take the result of an statsmodel results table and transforms it into a dataframe'''\n","    pvals = results.pvalues\n","    coeff = results.params\n","    # conf_lower = results.conf_int()[0]\n","    # conf_higher = results.conf_int()[1]\n","\n","    results_df = pd.DataFrame({\"pvals\":pvals,\n","                               \"coeff\":coeff\n","                              #  \"conf_lower\":conf_lower,\n","                              #  \"conf_higher\":conf_higher\n","                                })\n","\n","    #Reordering...\n","    results_df = results_df[[\"coeff\",\"pvals\"]]\n","    return results_df\n","\n","\n","\n","\n","\n","def OLS_feature_selection(df):\n","  df.columns = df.columns.str.replace(r'\\([^)]*\\)','',regex= True) # 컬럼에서 문자 기호 같은거 있으면 OLS 안돌아간다.     \n","                                                                  # 테스트 데이터에도 적용해야한다. 컬럼명 바꿔야하니깐.\n","  result = OLS_report(df,42)\n","  ols_report_df = results_summary_to_dataframe(result).reset_index()\n","  ols_report_df = ols_report_df[ols_report_df['index'] !='Intercept']\n","  selected_col = list(ols_report_df[ols_report_df['pvals']<0.05]['index'].values) \n","  selected_df = df[selected_col]\n","  selected_df['해당일자_전체평균가격'] = df['해당일자_전체평균가격']\n","  return selected_df        "],"metadata":{"id":"UTWK3SPHK8CE","executionInfo":{"status":"ok","timestamp":1664504251899,"user_tz":-540,"elapsed":9,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["# 유의수준을 토대로 columns selection\n","for i in range(37):\n","  globals()[f'total_{i}'] = OLS_feature_selection(globals()[f'train_smoothing_{i}'])"],"metadata":{"id":"iQHnQ5cVLMIk","executionInfo":{"status":"ok","timestamp":1664504255509,"user_tz":-540,"elapsed":3618,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["# 품목별로 컬럼 확인\n","for i in range(37):\n","  print(f\"{i} 품목의 selected_columns : {list(globals()[f'total_{i}'].columns)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TpYK3NbSLub0","executionInfo":{"status":"ok","timestamp":1664504255510,"user_tz":-540,"elapsed":17,"user":{"displayName":"이태영","userId":"15551978220746215296"}},"outputId":"ebb5f920-6b30-4847-81d6-441f597d55ef"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["0 품목의 selected_columns : ['단가', '습도_0', '최저온도_1', '평균온도_1', '최대온도_2', '습도_2', '일자별_somae가격_최대', '일자별_somae가격_최소', '해당일자_전체평균가격']\n","1 품목의 selected_columns : ['단가', '거래량', '습도_0', '최대온도_1', '평균온도_1', '습도_1', '최대온도_2', '최저온도_2', '평균온도_2', '습도_2', '일자별_domae가격_최대', '일자별_domae가격_평균', '일자별_somae가격_최대', '일자별_somae가격_평균', '일자별_somae가격_최소', '수출중량', '수출금액', '해당일자_전체평균가격']\n","2 품목의 selected_columns : ['단가', '거래량', '최저온도_0', '습도_0', '최저온도_1', '최저온도_2', '습도_2', '일자별_somae가격_평균', '일자별_somae가격_최소', '수출중량', '수출금액', '수입금액', '무역수지', '해당일자_전체평균가격']\n","3 품목의 selected_columns : ['단가', '거래량', '최저온도_1', '평균온도_1', '습도_2', '일자별_domae가격_최대', '일자별_domae가격_평균', '일자별_somae가격_평균', '일자별_somae가격_최소', '수출중량', '수출금액', '수입중량', '수입금액', '무역수지', '해당일자_전체평균가격']\n","4 품목의 selected_columns : ['단가', '거래량', '최저온도_0', '최저온도_2', '습도_2', '일자별_domae가격_평균', '일자별_domae가격_최소', '일자별_somae가격_최대', '일자별_somae가격_최소', '수출중량', '무역수지', '해당일자_전체평균가격']\n","5 품목의 selected_columns : ['단가', '거래량', '최대온도_0', '평균온도_0', '평균온도_1', '습도_1', '최대온도_2', '일자별_domae가격_평균', '일자별_domae가격_최소', '수출중량', '수출금액', '수입중량', '수입금액', '무역수지', '해당일자_전체평균가격']\n","6 품목의 selected_columns : ['단가', '거래량', '최대온도_0', '강수량_1', '습도_1', '초기온도_2', '해당일자_전체평균가격']\n","7 품목의 selected_columns : ['단가', '거래량', '초기온도_0', '강수량_0', '평균온도_1', '강수량_1', '일자별_somae가격_평균', '해당일자_전체평균가격']\n","8 품목의 selected_columns : ['단가', '거래량', '최저온도_0', '평균온도_0', '습도_0', '최저온도_1', '최대온도_2', '습도_2', '일자별_domae가격_평균', '일자별_domae가격_최소', '일자별_somae가격_최대', '일자별_somae가격_최소', '해당일자_전체평균가격']\n","9 품목의 selected_columns : ['단가', '거래량', '최대온도_0', '최대온도_1', '평균온도_1', '강수량_1', '최대온도_2', '최저온도_2', '평균온도_2', '습도_2', '일자별_domae가격_최대', '일자별_domae가격_평균', '일자별_domae가격_최소', '일자별_somae가격_평균', '일자별_somae가격_최소', '수출중량', '수입중량', '해당일자_전체평균가격']\n","10 품목의 selected_columns : ['단가', '거래량', '최대온도_1', '강수량_1', '습도_2', '일자별_domae가격_최대', '일자별_domae가격_평균', '일자별_domae가격_최소', '일자별_somae가격_최대', '수입중량', '해당일자_전체평균가격']\n","11 품목의 selected_columns : ['단가', '거래량', '평균온도_0', '습도_0', '초기온도_1', '최저온도_1', '평균온도_1', '강수량_1', '해당일자_전체평균가격']\n","12 품목의 selected_columns : ['단가', '거래량', '습도_0', '최대온도_2', '평균온도_2', '습도_2', '일자별_domae가격_최대', '일자별_domae가격_최소', '일자별_somae가격_평균', '일자별_somae가격_최소', '수출중량', '수입중량', '해당일자_전체평균가격']\n","13 품목의 selected_columns : ['단가', '거래량', '습도_0', '일자별_domae가격_최대', '일자별_domae가격_평균', '일자별_domae가격_최소', '일자별_somae가격_최대', '일자별_somae가격_최소', '수출중량', '수출금액', '수입중량', '수입금액', '무역수지', '해당일자_전체평균가격']\n","14 품목의 selected_columns : ['단가', '거래량', '평균온도_0', '습도_0', '최대온도_1', '평균온도_1', '습도_1', '최대온도_2', '최저온도_2', '평균온도_2', '강수량_2', '습도_2', '일자별_domae가격_최대', '일자별_domae가격_평균', '일자별_domae가격_최소', '일자별_somae가격_최대', '일자별_somae가격_최소', '해당일자_전체평균가격']\n","15 품목의 selected_columns : ['단가', '거래량', '평균온도_0', '강수량_1', '습도_1', '평균온도_2', '일자별_somae가격_최대', '일자별_somae가격_평균', '수출중량', '수출금액', '수입금액', '무역수지', '해당일자_전체평균가격']\n","16 품목의 selected_columns : ['단가', '거래량', '평균온도_0', '습도_0', '최대온도_1', '평균온도_1', '습도_1', '초기온도_2', '최대온도_2', '최저온도_2', '평균온도_2', '습도_2', '일자별_somae가격_최대', '일자별_somae가격_평균', '해당일자_전체평균가격']\n","17 품목의 selected_columns : ['단가', '거래량', '최대온도_0', '최저온도_1', '최대온도_2', '습도_2', '일자별_domae가격_최대', '일자별_domae가격_평균', '일자별_somae가격_최대', '일자별_somae가격_평균', '수출중량', '무역수지', '해당일자_전체평균가격']\n","18 품목의 selected_columns : ['단가', '거래량', '최대온도_0', '평균온도_0', '습도_0', '초기온도_1', '강수량_1', '초기온도_2', '최대온도_2', '수출중량', '수출금액', '수입중량', '해당일자_전체평균가격']\n","19 품목의 selected_columns : ['단가', '거래량', '습도_1', '습도_2', '일자별_domae가격_최대', '일자별_domae가격_평균', '일자별_somae가격_최대', '수출중량', '수출금액', '수입중량', '수입금액', '무역수지', '해당일자_전체평균가격']\n","20 품목의 selected_columns : ['단가', '거래량', '최대온도_0', '최저온도_0', '습도_0', '최대온도_1', '일자별_somae가격_최대', '일자별_somae가격_평균', '수출중량', '수출금액', '수입중량', '무역수지', '해당일자_전체평균가격']\n","21 품목의 selected_columns : ['단가', '거래량', '습도_0', '강수량_1', '강수량_2', '습도_2', '일자별_domae가격_평균', '일자별_domae가격_최소', '일자별_somae가격_최대', '일자별_somae가격_평균', '일자별_somae가격_최소', '수출중량', '수출금액', '수입금액', '무역수지', '해당일자_전체평균가격']\n","22 품목의 selected_columns : ['단가', '거래량', '최저온도_0', '평균온도_0', '습도_0', '초기온도_1', '강수량_1', '최저온도_2', '평균온도_2', '강수량_2', '습도_2', '일자별_somae가격_최소', '수출중량', '수출금액', '수입중량', '수입금액', '해당일자_전체평균가격']\n","23 품목의 selected_columns : ['단가', '거래량', '최저온도_1', '강수량_1', '평균온도_2', '강수량_2', '일자별_domae가격_최대', '일자별_domae가격_평균', '일자별_somae가격_최대', '일자별_somae가격_최소', '해당일자_전체평균가격']\n","24 품목의 selected_columns : ['단가', '거래량', '일자별_domae가격_최대', '일자별_somae가격_최대', '일자별_somae가격_평균', '일자별_somae가격_최소', '해당일자_전체평균가격']\n","25 품목의 selected_columns : ['단가', '거래량', '최대온도_0', '최저온도_0', '평균온도_0', '습도_0', '최대온도_1', '최저온도_1', '평균온도_1', '습도_2', '일자별_somae가격_최대', '일자별_somae가격_평균', '일자별_somae가격_최소', '수입중량', '무역수지', '해당일자_전체평균가격']\n","26 품목의 selected_columns : ['단가', '거래량', '최저온도_1', '강수량_1', '최대온도_2', '최저온도_2', '습도_2', '일자별_domae가격_최대', '일자별_domae가격_평균', '일자별_somae가격_최소', '해당일자_전체평균가격']\n","27 품목의 selected_columns : ['단가', '거래량', '최저온도_0', '평균온도_0', '일자별_domae가격_최대', '일자별_domae가격_평균', '일자별_somae가격_최대', '일자별_somae가격_평균', '일자별_somae가격_최소', '해당일자_전체평균가격']\n","28 품목의 selected_columns : ['단가', '거래량', '최대온도_1', '평균온도_1', '일자별_somae가격_최대', '수출중량', '수출금액', '수입금액', '해당일자_전체평균가격']\n","29 품목의 selected_columns : ['단가', '거래량', '최대온도_0', '초기온도_1', '일자별_domae가격_최소', '일자별_somae가격_평균', '일자별_somae가격_최소', '수입중량', '수입금액', '해당일자_전체평균가격']\n","30 품목의 selected_columns : ['단가', '거래량', '최대온도_1', '평균온도_1', '습도_1', '최대온도_2', '평균온도_2', '강수량_2', '습도_2', '일자별_domae가격_최대', '일자별_domae가격_평균', '일자별_domae가격_최소', '일자별_somae가격_최대', '일자별_somae가격_평균', '일자별_somae가격_최소', '수입중량', '무역수지', '해당일자_전체평균가격']\n","31 품목의 selected_columns : ['단가', '거래량', '최대온도_0', '최대온도_1', '최저온도_1', '평균온도_2', '일자별_domae가격_평균', '일자별_domae가격_최소', '해당일자_전체평균가격']\n","32 품목의 selected_columns : ['단가', '거래량', '최대온도_0', '최저온도_0', '평균온도_0', '강수량_0', '최대온도_1', '습도_1', '최저온도_2', '습도_2', '일자별_domae가격_최대', '일자별_domae가격_평균', '일자별_domae가격_최소', '일자별_somae가격_최소', '수출중량', '수입중량', '해당일자_전체평균가격']\n","33 품목의 selected_columns : ['단가', '거래량', '습도_0', '습도_1', '일자별_domae가격_평균', '일자별_domae가격_최소', '일자별_somae가격_평균', '일자별_somae가격_최소', '수출중량', '수출금액', '수입중량', '수입금액', '무역수지', '해당일자_전체평균가격']\n","34 품목의 selected_columns : ['단가', '거래량', '초기온도_0', '습도_1', '강수량_2', '습도_2', '일자별_domae가격_최대', '일자별_domae가격_평균', '수출중량', '수출금액', '수입중량', '수입금액', '무역수지', '해당일자_전체평균가격']\n","35 품목의 selected_columns : ['단가', '거래량', '최저온도_0', '최대온도_2', '일자별_domae가격_최소', '일자별_somae가격_최대', '일자별_somae가격_평균', '수출중량', '수입중량', '해당일자_전체평균가격']\n","36 품목의 selected_columns : ['단가', '거래량', '습도_1', '최대온도_2', '일자별_domae가격_평균', '일자별_somae가격_최대', '일자별_somae가격_평균', '수출중량', '해당일자_전체평균가격']\n"]}]},{"cell_type":"code","source":["for i in range(37):\n","  print(globals()[f'total_{i}']['해당일자_전체평균가격'].isna().sum())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vEfHF_rBLNM1","executionInfo":{"status":"ok","timestamp":1664504255511,"user_tz":-540,"elapsed":14,"user":{"displayName":"이태영","userId":"15551978220746215296"}},"outputId":"60f9cf01-4a68-4562-984d-8e116fcd680a"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n"]}]},{"cell_type":"code","source":["def scaling_df(df):\n","  scaler = StandardScaler()\n","  \n","  \n","  tmp = df.drop('해당일자_전체평균가격',axis=1).copy()\n","  scaler.fit(tmp)\n","  df_scaled = scaler.transform(tmp)\n","  df_scaled = pd.DataFrame(data=df_scaled, columns = tmp.columns)\n","  df_scaled['해당일자_전체평균가격'] = df['해당일자_전체평균가격'].values \n","  return df_scaled\n","\n","for i in range(37):\n","  globals()[f'train_scaled_{i}'] = scaling_df(globals()[f'total_{i}'])"],"metadata":{"id":"SA1KN8RtLv6z","executionInfo":{"status":"ok","timestamp":1664504255511,"user_tz":-540,"elapsed":7,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["for i in range(37):\n","  print(globals()[f'train_scaled_{i}'].isna().sum().sum())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TmiFk94vLxlZ","executionInfo":{"status":"ok","timestamp":1664504256074,"user_tz":-540,"elapsed":570,"user":{"displayName":"이태영","userId":"15551978220746215296"}},"outputId":"0e1074ee-a73b-4fe3-c004-d830ddc07314"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n"]}]},{"cell_type":"code","source":["globals()[f'train_scaled_0']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"lSXEp8FvLzGc","executionInfo":{"status":"ok","timestamp":1664504290791,"user_tz":-540,"elapsed":5,"user":{"displayName":"이태영","userId":"15551978220746215296"}},"outputId":"42de8d3e-04fa-4570-9549-4fd6fc68451b"},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["            단가  습도_0    최저온도_1    평균온도_1    최대온도_2      습도_2  일자별_somae가격_최대  \\\n","0    -1.562158   0.0 -0.839649 -1.055373 -0.883624  0.232086       -1.324653   \n","1     6.636992   0.0 -0.606699 -0.673602 -0.789047 -0.017175        0.577939   \n","2     0.611502   0.0 -0.723174 -0.883576 -0.949829 -0.391066        0.577939   \n","3     1.017981   0.0 -0.785891 -0.845399 -0.770131  0.120809        0.577939   \n","4     1.516286   0.0 -0.839649 -0.902665 -0.940371 -0.773859        0.577939   \n","...        ...   ...       ...       ...       ...       ...             ...   \n","1456 -1.562158   0.0 -0.741093 -0.959930 -0.817420 -0.017175       -1.324653   \n","1457  1.422662   0.0 -0.606699 -0.664058 -0.580976  0.427934        1.532417   \n","1458  1.433806   0.0 -0.785891 -1.084005 -1.309223  1.327053        1.532417   \n","1459  1.881196   0.0 -1.529540 -1.866634 -2.207711 -1.299087        1.532417   \n","1460  1.476654   0.0 -1.843126 -2.019342 -2.056387 -0.773859        1.532417   \n","\n","      일자별_somae가격_최소  해당일자_전체평균가격  \n","0          -1.342761  6114.961048  \n","1           0.777304  6114.961048  \n","2           0.777304  6114.961048  \n","3           0.777304  6114.961048  \n","4           0.777304  6114.961048  \n","...              ...          ...  \n","1456       -1.342761  7669.473042  \n","1457        0.856559  7334.439109  \n","1458        0.856559  7132.553300  \n","1459        0.856559  6946.171088  \n","1460        0.856559  6974.918333  \n","\n","[1461 rows x 9 columns]"],"text/html":["\n","  <div id=\"df-52d6bc3a-53b4-48a0-9bc8-5ff98db2db5b\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>단가</th>\n","      <th>습도_0</th>\n","      <th>최저온도_1</th>\n","      <th>평균온도_1</th>\n","      <th>최대온도_2</th>\n","      <th>습도_2</th>\n","      <th>일자별_somae가격_최대</th>\n","      <th>일자별_somae가격_최소</th>\n","      <th>해당일자_전체평균가격</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-1.562158</td>\n","      <td>0.0</td>\n","      <td>-0.839649</td>\n","      <td>-1.055373</td>\n","      <td>-0.883624</td>\n","      <td>0.232086</td>\n","      <td>-1.324653</td>\n","      <td>-1.342761</td>\n","      <td>6114.961048</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>6.636992</td>\n","      <td>0.0</td>\n","      <td>-0.606699</td>\n","      <td>-0.673602</td>\n","      <td>-0.789047</td>\n","      <td>-0.017175</td>\n","      <td>0.577939</td>\n","      <td>0.777304</td>\n","      <td>6114.961048</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.611502</td>\n","      <td>0.0</td>\n","      <td>-0.723174</td>\n","      <td>-0.883576</td>\n","      <td>-0.949829</td>\n","      <td>-0.391066</td>\n","      <td>0.577939</td>\n","      <td>0.777304</td>\n","      <td>6114.961048</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1.017981</td>\n","      <td>0.0</td>\n","      <td>-0.785891</td>\n","      <td>-0.845399</td>\n","      <td>-0.770131</td>\n","      <td>0.120809</td>\n","      <td>0.577939</td>\n","      <td>0.777304</td>\n","      <td>6114.961048</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1.516286</td>\n","      <td>0.0</td>\n","      <td>-0.839649</td>\n","      <td>-0.902665</td>\n","      <td>-0.940371</td>\n","      <td>-0.773859</td>\n","      <td>0.577939</td>\n","      <td>0.777304</td>\n","      <td>6114.961048</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1456</th>\n","      <td>-1.562158</td>\n","      <td>0.0</td>\n","      <td>-0.741093</td>\n","      <td>-0.959930</td>\n","      <td>-0.817420</td>\n","      <td>-0.017175</td>\n","      <td>-1.324653</td>\n","      <td>-1.342761</td>\n","      <td>7669.473042</td>\n","    </tr>\n","    <tr>\n","      <th>1457</th>\n","      <td>1.422662</td>\n","      <td>0.0</td>\n","      <td>-0.606699</td>\n","      <td>-0.664058</td>\n","      <td>-0.580976</td>\n","      <td>0.427934</td>\n","      <td>1.532417</td>\n","      <td>0.856559</td>\n","      <td>7334.439109</td>\n","    </tr>\n","    <tr>\n","      <th>1458</th>\n","      <td>1.433806</td>\n","      <td>0.0</td>\n","      <td>-0.785891</td>\n","      <td>-1.084005</td>\n","      <td>-1.309223</td>\n","      <td>1.327053</td>\n","      <td>1.532417</td>\n","      <td>0.856559</td>\n","      <td>7132.553300</td>\n","    </tr>\n","    <tr>\n","      <th>1459</th>\n","      <td>1.881196</td>\n","      <td>0.0</td>\n","      <td>-1.529540</td>\n","      <td>-1.866634</td>\n","      <td>-2.207711</td>\n","      <td>-1.299087</td>\n","      <td>1.532417</td>\n","      <td>0.856559</td>\n","      <td>6946.171088</td>\n","    </tr>\n","    <tr>\n","      <th>1460</th>\n","      <td>1.476654</td>\n","      <td>0.0</td>\n","      <td>-1.843126</td>\n","      <td>-2.019342</td>\n","      <td>-2.056387</td>\n","      <td>-0.773859</td>\n","      <td>1.532417</td>\n","      <td>0.856559</td>\n","      <td>6974.918333</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1461 rows × 9 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-52d6bc3a-53b4-48a0-9bc8-5ff98db2db5b')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-52d6bc3a-53b4-48a0-9bc8-5ff98db2db5b button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-52d6bc3a-53b4-48a0-9bc8-5ff98db2db5b');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["try_cnt = 17 # 모델이나 체크포인트를 각자 다른 폴더에 저장해주기 위해\n","            # 새로운거 시도할때마다 숫자 바꿔서 내면된다.\n"],"metadata":{"id":"xsjsp9mEL9iw","executionInfo":{"status":"ok","timestamp":1664504311864,"user_tz":-540,"elapsed":2,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["def make_Tensor(array):\n","    return tf.convert_to_tensor(array, dtype=tf.float32)\n","\n","def astype_data(data):\n","    df = data.astype(np.float32)\n","    return make_Tensor(df)"],"metadata":{"id":"5uMowx6RMCv8","executionInfo":{"status":"ok","timestamp":1664504315470,"user_tz":-540,"elapsed":7,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n","\n","    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n","    x = layers.MultiHeadAttention(\n","        key_dim=head_size, num_heads=num_heads, dropout=dropout\n","    )(x, x)\n","    x = layers.Dropout(dropout)(x)\n","    res = x + inputs\n","\n","    x = layers.LayerNormalization(epsilon=1e-6)(res)\n","    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n","    x = layers.Dropout(dropout)(x)\n","    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n","    return x + res"],"metadata":{"id":"EslZZbJyMDdL","executionInfo":{"status":"ok","timestamp":1664504319941,"user_tz":-540,"elapsed":413,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["def build_model(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout=0, mlp_dropout=0):\n","    inputs = keras.Input(shape=input_shape)\n","    x = inputs\n","    for _ in range(num_transformer_blocks):\n","        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n","\n","    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n","    for dim in mlp_units:\n","        x = layers.Dense(dim, activation=\"relu\")(x)\n","        x = layers.Dropout(mlp_dropout)(x)\n","    outputs = layers.Dense(28)(x) # 4주 예측\n","    return keras.Model(inputs, outputs)"],"metadata":{"id":"3lnFt5ybMEkr","executionInfo":{"status":"ok","timestamp":1664504323981,"user_tz":-540,"elapsed":7,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["def call_back_set(name, epoch, batch_size):\n","    early_stopping = EarlyStopping(monitor='val_loss', patience=100)\n","\n","    if os.path.exists(f'/content/drive/MyDrive/농산물예측/aT_data/check{try_cnt}') == False:\n","        os.mkdir(f'/content/drive/MyDrive/농산물예측/aT_data/check{try_cnt}')\n","\n","    filename = f'/content/drive/MyDrive/농산물예측/aT_data/check{try_cnt}/{name}-{epoch}-{batch_size}.h5'\n","\n","    checkpoint = ModelCheckpoint(filename,\n","                                 monitor='val_loss',\n","                                 verbose=1,\n","                                 save_best_only=True,\n","                                 save_weights_only=True,\n","                                 mode='auto'\n","                                 )\n","    return [early_stopping, checkpoint]"],"metadata":{"id":"4QjZvkgpMFkp","executionInfo":{"status":"ok","timestamp":1664504327925,"user_tz":-540,"elapsed":1064,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["def train(x_train, y_train, x_val, y_val, name, epoch, batch_size, learning_rate = 0.001, verbose = 1):\n","\n","\n","    model = build_model(\n","    x_train.shape[1:],\n","    head_size=256,\n","    num_heads=4,\n","    ff_dim=4,\n","    num_transformer_blocks=4,\n","    mlp_units=[128],\n","    mlp_dropout=0.4,\n","    dropout=0.25,\n","    )\n","\n","    model.compile(\n","        loss=\"mean_squared_error\",\n","        optimizer=keras.optimizers.Adam(learning_rate=learning_rate)\n","    )\n","\n","\n","    # Train the model\n","    with tf.device('/device:GPU:0'):\n","        history1 = model.fit(\n","            x_train, y_train,\n","            epochs = epoch,\n","            steps_per_epoch=len(x_train) / batch_size,\n","            batch_size=batch_size,\n","            validation_data=(x_val, y_val),\n","            validation_steps=len(x_val) / batch_size,\n","            shuffle=False,\n","            callbacks=call_back_set(name, epoch, batch_size),\n","            verbose=verbose)\n","\n","    return model"],"metadata":{"id":"rGyVaV-SMGi2","executionInfo":{"status":"ok","timestamp":1664504333612,"user_tz":-540,"elapsed":521,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["def time_window(df, t, t_sep):\n","    seq_len = t\n","    seqence_length = seq_len + t_sep\n","\n","    result = []\n","    for index in tqdm(range(len(df) - seqence_length)):\n","        result.append(df[index: index + seqence_length].values)\n","\n","    return np.array(result)"],"metadata":{"id":"aZT5OW9GMH2U","executionInfo":{"status":"ok","timestamp":1664504337130,"user_tz":-540,"elapsed":2,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["epoch = 1000\n","batch = 15"],"metadata":{"id":"3HOsgXAjMIzV","executionInfo":{"status":"ok","timestamp":1664504340970,"user_tz":-540,"elapsed":373,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["for i in range(0, 37):\n","\n","\n","    df_number = i\n","    df = globals()[f'train_scaled_{i}']\n","    \n","    # nan 처리\n","    df.fillna(0, inplace = True)\n","    \n","  \n","\n","    \n","\n","    # 변수와 타겟 분리\n","    x, y = df[[i for i in df.columns if i != '해당일자_전체평균가격']], df['해당일자_전체평균가격']\n","\n","    # 2주 입력을 통한 이후 4주 예측을 위해 y의 첫 14일을 제외\n","    y = y[14:]\n","\n","    # time series window 생성\n","    data_x = time_window(x, 13, 1)\n","    data_y = time_window(y, 27, 1)\n","\n","    # y의 길이와 같은 길이로 설정\n","    xdata = data_x[:len(data_y)]\n","    ydata = data_y\n","\n","    # train, validation 분리 (8 : 2)\n","    x_train, x_val, y_train, y_val = train_test_split(xdata, ydata, test_size=0.2, shuffle=False, random_state=119)\n","\n","    # transformer 모델 훈련\n","    transformer = train(astype_data(x_train), y_train, astype_data(x_val), y_val, f'transformer-{df_number}', epoch, batch)\n","    transformer.load_weights(f'/content/drive/MyDrive/농산물예측/aT_data/check{try_cnt}/transformer-{df_number}-{epoch}-{batch}.h5')\n","\n","    if os.path.exists(f'/content/drive/MyDrive/농산물예측/aT_data/model{try_cnt}') == False:\n","         os.mkdir(f'/content/drive/MyDrive/농산물예측/aT_data/model{try_cnt}')\n","\n","    \n","    \n","    \n","    # 모델 저장\n","    transformer.save(f'/content/drive/MyDrive/농산물예측/aT_data/model{try_cnt}/transformer-{df_number}-{epoch}-{batch}.h5')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qN4mjolzMJuy","executionInfo":{"status":"ok","timestamp":1664514834241,"user_tz":-540,"elapsed":2841271,"user":{"displayName":"이태영","userId":"15551978220746215296"}},"outputId":"c09c0738-f771-434f-8112-a347c3b947ad"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1447/1447 [00:00<00:00, 20312.39it/s]\n","100%|██████████| 1419/1419 [00:00<00:00, 22165.39it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/1000\n","75/75 [============================>.] - ETA: 0s - loss: 9277248.0000\n","Epoch 1: val_loss improved from inf to 3344569.75000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-0-1000-15.h5\n","75/75 [==============================] - 6s 28ms/step - loss: 9209012.0000 - val_loss: 3344569.7500\n","Epoch 2/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 1457059.1250\n","Epoch 2: val_loss improved from 3344569.75000 to 682554.12500, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-0-1000-15.h5\n","75/75 [==============================] - 2s 20ms/step - loss: 1436989.0000 - val_loss: 682554.1250\n","Epoch 3/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 1481807.6250\n","Epoch 3: val_loss improved from 682554.12500 to 654900.87500, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-0-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 1460196.3750 - val_loss: 654900.8750\n","Epoch 4/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 1432278.5000\n","Epoch 4: val_loss improved from 654900.87500 to 600120.31250, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-0-1000-15.h5\n","75/75 [==============================] - 1s 17ms/step - loss: 1409884.7500 - val_loss: 600120.3125\n","Epoch 5/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 1480574.1250\n","Epoch 5: val_loss did not improve from 600120.31250\n","75/75 [==============================] - 1s 16ms/step - loss: 1469011.3750 - val_loss: 685641.6875\n","Epoch 6/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 1509297.7500\n","Epoch 6: val_loss did not improve from 600120.31250\n","75/75 [==============================] - 1s 16ms/step - loss: 1481939.6250 - val_loss: 629205.0625\n","Epoch 7/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 1328896.8750\n","Epoch 7: val_loss improved from 600120.31250 to 578527.62500, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-0-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 1308859.3750 - val_loss: 578527.6250\n","Epoch 8/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 1202720.5000\n","Epoch 8: val_loss improved from 578527.62500 to 498210.53125, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-0-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 1184255.1250 - val_loss: 498210.5312\n","Epoch 9/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 1064755.2500\n","Epoch 9: val_loss did not improve from 498210.53125\n","75/75 [==============================] - 1s 17ms/step - loss: 1053044.1250 - val_loss: 515677.4062\n","Epoch 10/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 1025159.6250\n","Epoch 10: val_loss did not improve from 498210.53125\n","75/75 [==============================] - 1s 16ms/step - loss: 1009763.6875 - val_loss: 507768.5625\n","Epoch 11/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 893840.5000\n","Epoch 11: val_loss did not improve from 498210.53125\n","75/75 [==============================] - 1s 16ms/step - loss: 884351.2500 - val_loss: 527196.7500\n","Epoch 12/1000\n","76/75 [==============================] - ETA: 0s - loss: 840537.6250\n","Epoch 12: val_loss did not improve from 498210.53125\n","75/75 [==============================] - 1s 16ms/step - loss: 840537.6250 - val_loss: 572805.3750\n","Epoch 13/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 789249.3750\n","Epoch 13: val_loss did not improve from 498210.53125\n","75/75 [==============================] - 1s 16ms/step - loss: 786775.3750 - val_loss: 548030.5625\n","Epoch 14/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 774042.0000\n","Epoch 14: val_loss did not improve from 498210.53125\n","75/75 [==============================] - 1s 16ms/step - loss: 772857.5625 - val_loss: 591778.8125\n","Epoch 15/1000\n","76/75 [==============================] - ETA: 0s - loss: 878488.1250\n","Epoch 15: val_loss did not improve from 498210.53125\n","75/75 [==============================] - 1s 17ms/step - loss: 878488.1250 - val_loss: 551061.8125\n","Epoch 16/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 812210.8125\n","Epoch 16: val_loss did not improve from 498210.53125\n","75/75 [==============================] - 1s 16ms/step - loss: 809611.1250 - val_loss: 554247.9375\n","Epoch 17/1000\n","76/75 [==============================] - ETA: 0s - loss: 745611.8125\n","Epoch 17: val_loss did not improve from 498210.53125\n","75/75 [==============================] - 1s 17ms/step - loss: 745611.8125 - val_loss: 530739.6875\n","Epoch 18/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 748412.8750\n","Epoch 18: val_loss did not improve from 498210.53125\n","75/75 [==============================] - 1s 16ms/step - loss: 745901.8750 - val_loss: 564272.2500\n","Epoch 19/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 803781.8750\n","Epoch 19: val_loss improved from 498210.53125 to 497679.37500, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-0-1000-15.h5\n","75/75 [==============================] - 2s 21ms/step - loss: 808887.2500 - val_loss: 497679.3750\n","Epoch 20/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 708192.7500\n","Epoch 20: val_loss improved from 497679.37500 to 478438.37500, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-0-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 705784.8750 - val_loss: 478438.3750\n","Epoch 21/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 717174.3125\n","Epoch 21: val_loss improved from 478438.37500 to 419903.75000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-0-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 717335.3750 - val_loss: 419903.7500\n","Epoch 22/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 916132.1875\n","Epoch 22: val_loss did not improve from 419903.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 920333.1875 - val_loss: 445102.5625\n","Epoch 23/1000\n","75/75 [============================>.] - ETA: 0s - loss: 850438.5625\n","Epoch 23: val_loss did not improve from 419903.75000\n","75/75 [==============================] - 1s 17ms/step - loss: 844790.3750 - val_loss: 420249.7500\n","Epoch 24/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 831544.6875\n","Epoch 24: val_loss improved from 419903.75000 to 413054.40625, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-0-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 832598.6875 - val_loss: 413054.4062\n","Epoch 25/1000\n","76/75 [==============================] - ETA: 0s - loss: 801292.3750\n","Epoch 25: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 801292.3750 - val_loss: 419229.6250\n","Epoch 26/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 709368.3750\n","Epoch 26: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 714916.5000 - val_loss: 466935.5938\n","Epoch 27/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 765847.8750\n","Epoch 27: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 764988.3750 - val_loss: 500731.5000\n","Epoch 28/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 744020.0000\n","Epoch 28: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 749051.4375 - val_loss: 481018.0938\n","Epoch 29/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 691763.8125\n","Epoch 29: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 693712.8750 - val_loss: 469470.1875\n","Epoch 30/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 663560.2500\n","Epoch 30: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 668597.0625 - val_loss: 502579.5938\n","Epoch 31/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 683308.6250\n","Epoch 31: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 17ms/step - loss: 685147.1250 - val_loss: 452895.0938\n","Epoch 32/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 678292.0000\n","Epoch 32: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 681018.0625 - val_loss: 430198.1562\n","Epoch 33/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 704642.5000\n","Epoch 33: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 708679.3125 - val_loss: 481666.3750\n","Epoch 34/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 826473.8125\n","Epoch 34: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 835945.0625 - val_loss: 440883.8750\n","Epoch 35/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 744906.5000\n","Epoch 35: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 755340.7500 - val_loss: 560862.5625\n","Epoch 36/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 705063.9375\n","Epoch 36: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 699952.5625 - val_loss: 502002.8125\n","Epoch 37/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 647519.0000\n","Epoch 37: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 652198.2500 - val_loss: 519306.3750\n","Epoch 38/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 609648.2500\n","Epoch 38: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 617679.1875 - val_loss: 482841.4688\n","Epoch 39/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 646340.4375\n","Epoch 39: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 653774.5000 - val_loss: 451688.3438\n","Epoch 40/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 693934.5625\n","Epoch 40: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 691737.5000 - val_loss: 522801.1875\n","Epoch 41/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 831437.3125\n","Epoch 41: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 833193.5625 - val_loss: 435522.5938\n","Epoch 42/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 696082.4375\n","Epoch 42: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 695743.1250 - val_loss: 477025.9688\n","Epoch 43/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 642913.8125\n","Epoch 43: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 643995.4375 - val_loss: 473312.4375\n","Epoch 44/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 631527.9375\n","Epoch 44: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 637073.6250 - val_loss: 469553.3750\n","Epoch 45/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 602795.0000\n","Epoch 45: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 607026.0625 - val_loss: 498855.0938\n","Epoch 46/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 595242.4375\n","Epoch 46: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 598610.5000 - val_loss: 523364.4062\n","Epoch 47/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 584914.1875\n","Epoch 47: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 589750.5625 - val_loss: 510840.7812\n","Epoch 48/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 591609.4375\n","Epoch 48: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 595396.1250 - val_loss: 506889.1250\n","Epoch 49/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 603178.8750\n","Epoch 49: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 607517.0625 - val_loss: 573992.4375\n","Epoch 50/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 597093.4375\n","Epoch 50: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 599070.7500 - val_loss: 474821.4062\n","Epoch 51/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 628115.6875\n","Epoch 51: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 631522.2500 - val_loss: 484392.7812\n","Epoch 52/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 663267.6250\n","Epoch 52: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 662954.2500 - val_loss: 425682.5312\n","Epoch 53/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 638480.1875\n","Epoch 53: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 639993.8125 - val_loss: 502066.8125\n","Epoch 54/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 597537.7500\n","Epoch 54: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 598751.5000 - val_loss: 512002.3750\n","Epoch 55/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 611002.3750\n","Epoch 55: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 612999.6875 - val_loss: 461215.0312\n","Epoch 56/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 607593.6875\n","Epoch 56: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 610344.1875 - val_loss: 481693.7500\n","Epoch 57/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 645481.3125\n","Epoch 57: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 644171.3125 - val_loss: 487938.5938\n","Epoch 58/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 655522.6875\n","Epoch 58: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 656387.1875 - val_loss: 581713.7500\n","Epoch 59/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 608688.7500\n","Epoch 59: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 610991.6250 - val_loss: 673164.8750\n","Epoch 60/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 554850.6875\n","Epoch 60: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 562394.2500 - val_loss: 606353.4375\n","Epoch 61/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 586149.7500\n","Epoch 61: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 593169.4375 - val_loss: 545568.1875\n","Epoch 62/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 574470.7500\n","Epoch 62: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 577344.3125 - val_loss: 497902.6562\n","Epoch 63/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 602210.7500\n","Epoch 63: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 600635.1250 - val_loss: 447044.7500\n","Epoch 64/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 589544.3750\n","Epoch 64: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 588669.0000 - val_loss: 666263.4375\n","Epoch 65/1000\n","76/75 [==============================] - ETA: 0s - loss: 749923.6875\n","Epoch 65: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 17ms/step - loss: 749923.6875 - val_loss: 548031.5625\n","Epoch 66/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 738824.3750\n","Epoch 66: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 748525.7500 - val_loss: 488683.8438\n","Epoch 67/1000\n","76/75 [==============================] - ETA: 0s - loss: 655021.5625\n","Epoch 67: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 17ms/step - loss: 655021.5625 - val_loss: 502291.0312\n","Epoch 68/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 593017.3750\n","Epoch 68: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 592954.1875 - val_loss: 594195.0625\n","Epoch 69/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 576549.1875\n","Epoch 69: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 579821.6250 - val_loss: 498956.0625\n","Epoch 70/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 583836.6875\n","Epoch 70: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 580579.4375 - val_loss: 504688.9062\n","Epoch 71/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 585789.6250\n","Epoch 71: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 583964.9375 - val_loss: 513599.5938\n","Epoch 72/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 608009.5000\n","Epoch 72: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 608803.6875 - val_loss: 532705.9375\n","Epoch 73/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 613562.8125\n","Epoch 73: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 615222.5625 - val_loss: 481958.2500\n","Epoch 74/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 623650.3125\n","Epoch 74: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 622261.3125 - val_loss: 583200.0000\n","Epoch 75/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 697330.2500\n","Epoch 75: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 692996.5000 - val_loss: 651034.0000\n","Epoch 76/1000\n","76/75 [==============================] - ETA: 0s - loss: 696599.1250\n","Epoch 76: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 17ms/step - loss: 696599.1250 - val_loss: 518618.0312\n","Epoch 77/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 679765.3125\n","Epoch 77: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 17ms/step - loss: 677060.3750 - val_loss: 539248.6250\n","Epoch 78/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 640653.1875\n","Epoch 78: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 17ms/step - loss: 636010.1250 - val_loss: 552148.3750\n","Epoch 79/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 582399.6875\n","Epoch 79: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 583471.5625 - val_loss: 546845.0625\n","Epoch 80/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 593516.5625\n","Epoch 80: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 17ms/step - loss: 594404.6250 - val_loss: 566710.0625\n","Epoch 81/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 593879.0000\n","Epoch 81: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 17ms/step - loss: 600876.3750 - val_loss: 698934.3125\n","Epoch 82/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 600850.5625\n","Epoch 82: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 606729.7500 - val_loss: 708855.1250\n","Epoch 83/1000\n","76/75 [==============================] - ETA: 0s - loss: 593966.8125\n","Epoch 83: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 17ms/step - loss: 593966.8125 - val_loss: 738779.3750\n","Epoch 84/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 600924.0625\n","Epoch 84: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 17ms/step - loss: 601322.1875 - val_loss: 602039.9375\n","Epoch 85/1000\n","76/75 [==============================] - ETA: 0s - loss: 567308.4375\n","Epoch 85: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 17ms/step - loss: 567308.4375 - val_loss: 789456.3125\n","Epoch 86/1000\n","76/75 [==============================] - ETA: 0s - loss: 586203.6875\n","Epoch 86: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 586203.6875 - val_loss: 1035396.8750\n","Epoch 87/1000\n","76/75 [==============================] - ETA: 0s - loss: 562012.5000\n","Epoch 87: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 17ms/step - loss: 562012.5000 - val_loss: 849240.9375\n","Epoch 88/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 540604.6250\n","Epoch 88: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 547940.7500 - val_loss: 666246.5625\n","Epoch 89/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 561805.3125\n","Epoch 89: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 560512.0625 - val_loss: 614167.6250\n","Epoch 90/1000\n","76/75 [==============================] - ETA: 0s - loss: 544009.5000\n","Epoch 90: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 544009.5000 - val_loss: 913505.3750\n","Epoch 91/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 545031.1875\n","Epoch 91: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 544792.0625 - val_loss: 780451.9375\n","Epoch 92/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 550251.3125\n","Epoch 92: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 551018.0625 - val_loss: 994017.6875\n","Epoch 93/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 606153.6250\n","Epoch 93: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 603610.8125 - val_loss: 899537.1875\n","Epoch 94/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 600555.7500\n","Epoch 94: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 602695.1875 - val_loss: 884083.1250\n","Epoch 95/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 559062.6250\n","Epoch 95: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 563190.8125 - val_loss: 832494.1875\n","Epoch 96/1000\n","76/75 [==============================] - ETA: 0s - loss: 608998.9375\n","Epoch 96: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 17ms/step - loss: 608998.9375 - val_loss: 951940.3750\n","Epoch 97/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 607091.0625\n","Epoch 97: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 611137.4375 - val_loss: 735901.8750\n","Epoch 98/1000\n","76/75 [==============================] - ETA: 0s - loss: 558184.1875\n","Epoch 98: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 17ms/step - loss: 558184.1875 - val_loss: 867118.3750\n","Epoch 99/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 534034.1875\n","Epoch 99: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 535680.0625 - val_loss: 948115.1250\n","Epoch 100/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 538293.0625\n","Epoch 100: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 537185.4375 - val_loss: 1240739.8750\n","Epoch 101/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 597642.8125\n","Epoch 101: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 597744.0000 - val_loss: 1346850.6250\n","Epoch 102/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 593282.3125\n","Epoch 102: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 594239.2500 - val_loss: 1337046.5000\n","Epoch 103/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 552215.0000\n","Epoch 103: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 17ms/step - loss: 555670.3125 - val_loss: 932857.6250\n","Epoch 104/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 520952.6250\n","Epoch 104: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 522320.7500 - val_loss: 1012758.4375\n","Epoch 105/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 544791.0000\n","Epoch 105: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 547514.7500 - val_loss: 982194.0000\n","Epoch 106/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 509118.5938\n","Epoch 106: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 513068.7812 - val_loss: 834014.0625\n","Epoch 107/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 541533.9375\n","Epoch 107: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 544099.9375 - val_loss: 964939.1250\n","Epoch 108/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 556696.1250\n","Epoch 108: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 558515.3750 - val_loss: 846087.7500\n","Epoch 109/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 533595.5000\n","Epoch 109: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 532633.2500 - val_loss: 1081225.7500\n","Epoch 110/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 525996.0000\n","Epoch 110: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 527161.5000 - val_loss: 1048508.0625\n","Epoch 111/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 561106.0000\n","Epoch 111: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 560103.0000 - val_loss: 1061413.6250\n","Epoch 112/1000\n","76/75 [==============================] - ETA: 0s - loss: 544240.1250\n","Epoch 112: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 17ms/step - loss: 544240.1250 - val_loss: 1038388.3125\n","Epoch 113/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 535164.1250\n","Epoch 113: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 533322.0625 - val_loss: 1028704.2500\n","Epoch 114/1000\n","76/75 [==============================] - ETA: 0s - loss: 536500.9375\n","Epoch 114: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 17ms/step - loss: 536500.9375 - val_loss: 1387046.0000\n","Epoch 115/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 553484.5625\n","Epoch 115: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 17ms/step - loss: 550375.7500 - val_loss: 955321.3750\n","Epoch 116/1000\n","75/75 [============================>.] - ETA: 0s - loss: 606515.0625\n","Epoch 116: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 17ms/step - loss: 603902.7500 - val_loss: 502450.1875\n","Epoch 117/1000\n","75/75 [============================>.] - ETA: 0s - loss: 597694.4375\n","Epoch 117: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 17ms/step - loss: 593896.1875 - val_loss: 571285.9375\n","Epoch 118/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 533091.3125\n","Epoch 118: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 532523.2500 - val_loss: 766090.0000\n","Epoch 119/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 586928.6250\n","Epoch 119: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 590924.3125 - val_loss: 866403.0000\n","Epoch 120/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 634910.5625\n","Epoch 120: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 18ms/step - loss: 637495.0625 - val_loss: 918697.9375\n","Epoch 121/1000\n","75/75 [============================>.] - ETA: 0s - loss: 620270.3125\n","Epoch 121: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 19ms/step - loss: 617140.5000 - val_loss: 1187542.8750\n","Epoch 122/1000\n","74/75 [============================>.] - ETA: 0s - loss: 651392.1250\n","Epoch 122: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 18ms/step - loss: 644439.0625 - val_loss: 1206665.5000\n","Epoch 123/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 584448.7500\n","Epoch 123: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 586955.1875 - val_loss: 897202.6875\n","Epoch 124/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 511294.8750\n","Epoch 124: val_loss did not improve from 413054.40625\n","75/75 [==============================] - 1s 16ms/step - loss: 512049.7812 - val_loss: 771183.6875\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1447/1447 [00:00<00:00, 22085.73it/s]\n","100%|██████████| 1419/1419 [00:00<00:00, 21602.78it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 2135876.2500\n","Epoch 1: val_loss improved from inf to 279179.84375, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-1-1000-15.h5\n","75/75 [==============================] - 7s 24ms/step - loss: 2072668.7500 - val_loss: 279179.8438\n","Epoch 2/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 590785.9375\n","Epoch 2: val_loss improved from 279179.84375 to 114276.08594, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-1-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 571845.6250 - val_loss: 114276.0859\n","Epoch 3/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 586431.3750\n","Epoch 3: val_loss did not improve from 114276.08594\n","75/75 [==============================] - 1s 16ms/step - loss: 567191.2500 - val_loss: 158993.3750\n","Epoch 4/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 605369.0625\n","Epoch 4: val_loss did not improve from 114276.08594\n","75/75 [==============================] - 1s 16ms/step - loss: 585908.8750 - val_loss: 116914.7578\n","Epoch 5/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 602220.1875\n","Epoch 5: val_loss improved from 114276.08594 to 114060.71875, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-1-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 583151.4375 - val_loss: 114060.7188\n","Epoch 6/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 659739.7500\n","Epoch 6: val_loss improved from 114060.71875 to 85383.57031, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-1-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 639183.8750 - val_loss: 85383.5703\n","Epoch 7/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 636714.6250\n","Epoch 7: val_loss improved from 85383.57031 to 74779.61719, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-1-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 617168.8125 - val_loss: 74779.6172\n","Epoch 8/1000\n","76/75 [==============================] - ETA: 0s - loss: 617446.9375\n","Epoch 8: val_loss did not improve from 74779.61719\n","75/75 [==============================] - 1s 16ms/step - loss: 617446.9375 - val_loss: 148168.2031\n","Epoch 9/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 593532.3125\n","Epoch 9: val_loss improved from 74779.61719 to 54341.95703, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-1-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 574498.5625 - val_loss: 54341.9570\n","Epoch 10/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 625588.1875\n","Epoch 10: val_loss did not improve from 54341.95703\n","75/75 [==============================] - 1s 16ms/step - loss: 605747.4375 - val_loss: 54869.4492\n","Epoch 11/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 626010.4375\n","Epoch 11: val_loss did not improve from 54341.95703\n","75/75 [==============================] - 1s 16ms/step - loss: 606502.8125 - val_loss: 61527.5000\n","Epoch 12/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 666620.4375\n","Epoch 12: val_loss did not improve from 54341.95703\n","75/75 [==============================] - 1s 16ms/step - loss: 651164.0000 - val_loss: 247728.7031\n","Epoch 13/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 893173.5000\n","Epoch 13: val_loss did not improve from 54341.95703\n","75/75 [==============================] - 1s 16ms/step - loss: 863552.5625 - val_loss: 249930.5312\n","Epoch 14/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 739013.0625\n","Epoch 14: val_loss did not improve from 54341.95703\n","75/75 [==============================] - 1s 16ms/step - loss: 716983.9375 - val_loss: 236072.2812\n","Epoch 15/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 659429.3125\n","Epoch 15: val_loss did not improve from 54341.95703\n","75/75 [==============================] - 1s 16ms/step - loss: 637365.6875 - val_loss: 79772.9766\n","Epoch 16/1000\n","76/75 [==============================] - ETA: 0s - loss: 646240.6250\n","Epoch 16: val_loss did not improve from 54341.95703\n","75/75 [==============================] - 1s 16ms/step - loss: 646240.6250 - val_loss: 77521.6797\n","Epoch 17/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 639252.2500\n","Epoch 17: val_loss did not improve from 54341.95703\n","75/75 [==============================] - 1s 16ms/step - loss: 618133.0625 - val_loss: 59446.6055\n","Epoch 18/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 560040.1875\n","Epoch 18: val_loss did not improve from 54341.95703\n","75/75 [==============================] - 1s 16ms/step - loss: 542322.2500 - val_loss: 57249.1055\n","Epoch 19/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 517627.8438\n","Epoch 19: val_loss did not improve from 54341.95703\n","75/75 [==============================] - 1s 16ms/step - loss: 501954.7500 - val_loss: 56797.9141\n","Epoch 20/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 515144.3438\n","Epoch 20: val_loss did not improve from 54341.95703\n","75/75 [==============================] - 1s 16ms/step - loss: 500255.9688 - val_loss: 73839.9062\n","Epoch 21/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 497980.7188\n","Epoch 21: val_loss did not improve from 54341.95703\n","75/75 [==============================] - 1s 16ms/step - loss: 483248.7188 - val_loss: 138037.7969\n","Epoch 22/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 467340.9062\n","Epoch 22: val_loss did not improve from 54341.95703\n","75/75 [==============================] - 1s 16ms/step - loss: 453199.5938 - val_loss: 67965.1328\n","Epoch 23/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 501070.3750\n","Epoch 23: val_loss did not improve from 54341.95703\n","75/75 [==============================] - 1s 16ms/step - loss: 486712.2188 - val_loss: 70734.2188\n","Epoch 24/1000\n","76/75 [==============================] - ETA: 0s - loss: 472766.1250\n","Epoch 24: val_loss did not improve from 54341.95703\n","75/75 [==============================] - 1s 16ms/step - loss: 472766.1250 - val_loss: 68098.3906\n","Epoch 25/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 488568.2812\n","Epoch 25: val_loss did not improve from 54341.95703\n","75/75 [==============================] - 1s 16ms/step - loss: 473565.3750 - val_loss: 65557.3281\n","Epoch 26/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 467513.9688\n","Epoch 26: val_loss did not improve from 54341.95703\n","75/75 [==============================] - 1s 16ms/step - loss: 455010.0625 - val_loss: 76709.9609\n","Epoch 27/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 524027.5000\n","Epoch 27: val_loss did not improve from 54341.95703\n","75/75 [==============================] - 1s 16ms/step - loss: 511497.8125 - val_loss: 96897.5781\n","Epoch 28/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 583563.6250\n","Epoch 28: val_loss did not improve from 54341.95703\n","75/75 [==============================] - 1s 16ms/step - loss: 567917.8125 - val_loss: 79738.2891\n","Epoch 29/1000\n","76/75 [==============================] - ETA: 0s - loss: 533334.5000\n","Epoch 29: val_loss did not improve from 54341.95703\n","75/75 [==============================] - 1s 17ms/step - loss: 533334.5000 - val_loss: 75986.6250\n","Epoch 30/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 565275.8125\n","Epoch 30: val_loss did not improve from 54341.95703\n","75/75 [==============================] - 1s 16ms/step - loss: 551785.6250 - val_loss: 77827.9922\n","Epoch 31/1000\n","76/75 [==============================] - ETA: 0s - loss: 532125.4375\n","Epoch 31: val_loss did not improve from 54341.95703\n","75/75 [==============================] - 1s 17ms/step - loss: 532125.4375 - val_loss: 69007.0625\n","Epoch 32/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 536532.3125\n","Epoch 32: val_loss did not improve from 54341.95703\n","75/75 [==============================] - 1s 16ms/step - loss: 520830.9375 - val_loss: 69902.4688\n","Epoch 33/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 540323.5000\n","Epoch 33: val_loss did not improve from 54341.95703\n","75/75 [==============================] - 1s 16ms/step - loss: 524445.1875 - val_loss: 68516.6172\n","Epoch 34/1000\n","76/75 [==============================] - ETA: 0s - loss: 522278.5000\n","Epoch 34: val_loss did not improve from 54341.95703\n","75/75 [==============================] - 1s 16ms/step - loss: 522278.5000 - val_loss: 78039.2500\n","Epoch 35/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 537566.9375\n","Epoch 35: val_loss did not improve from 54341.95703\n","75/75 [==============================] - 1s 16ms/step - loss: 523733.3125 - val_loss: 76175.6484\n","Epoch 36/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 529944.6875\n","Epoch 36: val_loss did not improve from 54341.95703\n","75/75 [==============================] - 1s 16ms/step - loss: 514719.2500 - val_loss: 73384.2969\n","Epoch 37/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 503099.6250\n","Epoch 37: val_loss did not improve from 54341.95703\n","75/75 [==============================] - 1s 16ms/step - loss: 488715.5000 - val_loss: 78659.6094\n","Epoch 38/1000\n","76/75 [==============================] - ETA: 0s - loss: 485002.2500\n","Epoch 38: val_loss did not improve from 54341.95703\n","75/75 [==============================] - 1s 16ms/step - loss: 485002.2500 - val_loss: 73445.4375\n","Epoch 39/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 514524.8750\n","Epoch 39: val_loss did not improve from 54341.95703\n","75/75 [==============================] - 1s 16ms/step - loss: 499589.1250 - val_loss: 64168.6250\n","Epoch 40/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 523926.5000\n","Epoch 40: val_loss did not improve from 54341.95703\n","75/75 [==============================] - 1s 16ms/step - loss: 507406.6562 - val_loss: 66395.2031\n","Epoch 41/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 465164.8125\n","Epoch 41: val_loss did not improve from 54341.95703\n","75/75 [==============================] - 1s 16ms/step - loss: 450588.1875 - val_loss: 57216.5898\n","Epoch 42/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 439973.4375\n","Epoch 42: val_loss did not improve from 54341.95703\n","75/75 [==============================] - 1s 16ms/step - loss: 427482.1562 - val_loss: 64615.3594\n","Epoch 43/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 533967.7500\n","Epoch 43: val_loss improved from 54341.95703 to 53563.01953, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-1-1000-15.h5\n","75/75 [==============================] - 2s 22ms/step - loss: 517450.0312 - val_loss: 53563.0195\n","Epoch 44/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 477913.1875\n","Epoch 44: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 463621.4062 - val_loss: 68060.6016\n","Epoch 45/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 472453.1250\n","Epoch 45: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 457833.4062 - val_loss: 91426.8203\n","Epoch 46/1000\n","76/75 [==============================] - ETA: 0s - loss: 422802.2500\n","Epoch 46: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 422802.2500 - val_loss: 127054.8047\n","Epoch 47/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 438761.8750\n","Epoch 47: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 426430.8438 - val_loss: 175952.2969\n","Epoch 48/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 469493.8125\n","Epoch 48: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 454933.8750 - val_loss: 86638.8594\n","Epoch 49/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 400106.6250\n","Epoch 49: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 387688.9375 - val_loss: 106544.9844\n","Epoch 50/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 375929.0000\n","Epoch 50: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 368190.7188 - val_loss: 157870.6250\n","Epoch 51/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 381460.3125\n","Epoch 51: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 370678.9062 - val_loss: 155309.3125\n","Epoch 52/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 370039.3438\n","Epoch 52: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 364033.7812 - val_loss: 161648.9688\n","Epoch 53/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 389250.7500\n","Epoch 53: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 382377.8750 - val_loss: 212772.0156\n","Epoch 54/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 379802.5938\n","Epoch 54: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 376175.1875 - val_loss: 135865.7969\n","Epoch 55/1000\n","76/75 [==============================] - ETA: 0s - loss: 456187.0938\n","Epoch 55: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 456187.0938 - val_loss: 106644.9062\n","Epoch 56/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 350339.2812\n","Epoch 56: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 339668.7188 - val_loss: 115801.3828\n","Epoch 57/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 304102.9375\n","Epoch 57: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 295520.5000 - val_loss: 96945.1953\n","Epoch 58/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 303332.1562\n","Epoch 58: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 294726.4688 - val_loss: 83051.5312\n","Epoch 59/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 287226.3125\n","Epoch 59: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 279246.2812 - val_loss: 89142.8203\n","Epoch 60/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 275882.1562\n","Epoch 60: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 268661.8125 - val_loss: 75558.6250\n","Epoch 61/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 265650.6875\n","Epoch 61: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 259347.5938 - val_loss: 73386.4219\n","Epoch 62/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 259058.6719\n","Epoch 62: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 252077.9219 - val_loss: 82052.0781\n","Epoch 63/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 290228.8125\n","Epoch 63: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 282105.1562 - val_loss: 92754.2969\n","Epoch 64/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 289375.8438\n","Epoch 64: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 281548.6875 - val_loss: 68582.0000\n","Epoch 65/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 345706.8438\n","Epoch 65: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 336292.2812 - val_loss: 91121.8906\n","Epoch 66/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 300014.8750\n","Epoch 66: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 291826.7500 - val_loss: 94145.3594\n","Epoch 67/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 290311.9062\n","Epoch 67: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 282333.0312 - val_loss: 101640.0703\n","Epoch 68/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 266345.7812\n","Epoch 68: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 259500.2031 - val_loss: 102424.7188\n","Epoch 69/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 243055.3281\n","Epoch 69: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 236467.5156 - val_loss: 80686.2266\n","Epoch 70/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 256426.2344\n","Epoch 70: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 249202.1250 - val_loss: 92904.0547\n","Epoch 71/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 272742.6250\n","Epoch 71: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 265167.5938 - val_loss: 96834.7188\n","Epoch 72/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 263084.3750\n","Epoch 72: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 256164.0312 - val_loss: 109059.0000\n","Epoch 73/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 229022.7031\n","Epoch 73: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 222970.4062 - val_loss: 95705.4141\n","Epoch 74/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 212009.0938\n","Epoch 74: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 206586.9844 - val_loss: 100996.4141\n","Epoch 75/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 217968.2344\n","Epoch 75: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 212070.7031 - val_loss: 91710.6250\n","Epoch 76/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 227960.8281\n","Epoch 76: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 221518.7031 - val_loss: 103262.1484\n","Epoch 77/1000\n","75/75 [============================>.] - ETA: 0s - loss: 216574.2344\n","Epoch 77: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 215199.3125 - val_loss: 78486.8359\n","Epoch 78/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 214770.8125\n","Epoch 78: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 209218.4375 - val_loss: 74786.8438\n","Epoch 79/1000\n","76/75 [==============================] - ETA: 0s - loss: 217742.7344\n","Epoch 79: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 217742.7344 - val_loss: 74409.7969\n","Epoch 80/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 230360.0156\n","Epoch 80: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 224026.2031 - val_loss: 93668.2422\n","Epoch 81/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 219756.6719\n","Epoch 81: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 214932.1406 - val_loss: 136629.1562\n","Epoch 82/1000\n","76/75 [==============================] - ETA: 0s - loss: 202206.3281\n","Epoch 82: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 202206.3281 - val_loss: 103431.0625\n","Epoch 83/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 190354.2500\n","Epoch 83: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 186170.5938 - val_loss: 108085.2344\n","Epoch 84/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 175976.7500\n","Epoch 84: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 173563.4375 - val_loss: 86188.0000\n","Epoch 85/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 168766.9531\n","Epoch 85: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 166045.4844 - val_loss: 122117.8750\n","Epoch 86/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 167209.6094\n","Epoch 86: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 164017.6250 - val_loss: 99150.2188\n","Epoch 87/1000\n","76/75 [==============================] - ETA: 0s - loss: 151657.8750\n","Epoch 87: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 151657.8750 - val_loss: 115587.8516\n","Epoch 88/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 156767.3750\n","Epoch 88: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 153600.1406 - val_loss: 108489.1797\n","Epoch 89/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 159304.8281\n","Epoch 89: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 156172.5469 - val_loss: 114460.0938\n","Epoch 90/1000\n","76/75 [==============================] - ETA: 0s - loss: 157249.0312\n","Epoch 90: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 157249.0312 - val_loss: 113523.7891\n","Epoch 91/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 151525.5312\n","Epoch 91: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 148359.5312 - val_loss: 97081.8984\n","Epoch 92/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 147721.8750\n","Epoch 92: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 145134.1562 - val_loss: 106622.9453\n","Epoch 93/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 150069.9062\n","Epoch 93: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 147297.9844 - val_loss: 109347.8750\n","Epoch 94/1000\n","76/75 [==============================] - ETA: 0s - loss: 135877.6875\n","Epoch 94: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 17ms/step - loss: 135877.6875 - val_loss: 117088.2266\n","Epoch 95/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 146943.2812\n","Epoch 95: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 144028.1875 - val_loss: 86625.3672\n","Epoch 96/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 206273.2500\n","Epoch 96: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 202020.5312 - val_loss: 93109.1875\n","Epoch 97/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 156295.1094\n","Epoch 97: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 152242.9375 - val_loss: 100528.3125\n","Epoch 98/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 161891.3750\n","Epoch 98: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 158645.3281 - val_loss: 134475.3594\n","Epoch 99/1000\n","76/75 [==============================] - ETA: 0s - loss: 148822.4688\n","Epoch 99: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 148822.4688 - val_loss: 114274.5625\n","Epoch 100/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 148864.2344\n","Epoch 100: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 145489.9219 - val_loss: 102370.5469\n","Epoch 101/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 137431.1250\n","Epoch 101: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 134160.4219 - val_loss: 76499.2031\n","Epoch 102/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 153718.7344\n","Epoch 102: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 152169.4531 - val_loss: 100526.5391\n","Epoch 103/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 238172.0625\n","Epoch 103: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 236109.0938 - val_loss: 112465.7344\n","Epoch 104/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 207578.7031\n","Epoch 104: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 204663.5625 - val_loss: 112209.7812\n","Epoch 105/1000\n","75/75 [============================>.] - ETA: 0s - loss: 177789.6250\n","Epoch 105: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 177263.4531 - val_loss: 91933.4375\n","Epoch 106/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 213705.4219\n","Epoch 106: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 208119.5312 - val_loss: 114880.7500\n","Epoch 107/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 211933.5781\n","Epoch 107: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 205983.1719 - val_loss: 116243.7188\n","Epoch 108/1000\n","75/75 [============================>.] - ETA: 0s - loss: 192254.7188\n","Epoch 108: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 17ms/step - loss: 191728.6406 - val_loss: 132222.5000\n","Epoch 109/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 193203.5781\n","Epoch 109: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 17ms/step - loss: 188928.8906 - val_loss: 120665.4062\n","Epoch 110/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 185177.9375\n","Epoch 110: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 181201.0000 - val_loss: 148077.9219\n","Epoch 111/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 157293.2812\n","Epoch 111: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 154219.4375 - val_loss: 115710.6797\n","Epoch 112/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 167898.6406\n","Epoch 112: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 19ms/step - loss: 164704.1719 - val_loss: 113025.0859\n","Epoch 113/1000\n","75/75 [============================>.] - ETA: 0s - loss: 153326.1719\n","Epoch 113: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 19ms/step - loss: 152786.5938 - val_loss: 98690.9453\n","Epoch 114/1000\n","75/75 [============================>.] - ETA: 0s - loss: 156375.0312\n","Epoch 114: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 17ms/step - loss: 155453.2188 - val_loss: 98920.6875\n","Epoch 115/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 143070.1875\n","Epoch 115: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 140629.2656 - val_loss: 90401.9844\n","Epoch 116/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 134835.5000\n","Epoch 116: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 133314.8594 - val_loss: 88431.7812\n","Epoch 117/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 136360.5156\n","Epoch 117: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 133578.6250 - val_loss: 68301.4531\n","Epoch 118/1000\n","76/75 [==============================] - ETA: 0s - loss: 136736.4844\n","Epoch 118: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 17ms/step - loss: 136736.4844 - val_loss: 95199.0312\n","Epoch 119/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 139680.5625\n","Epoch 119: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 136964.7500 - val_loss: 114406.4375\n","Epoch 120/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 174178.0156\n","Epoch 120: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 169559.9062 - val_loss: 128436.8594\n","Epoch 121/1000\n","76/75 [==============================] - ETA: 0s - loss: 165263.1406\n","Epoch 121: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 17ms/step - loss: 165263.1406 - val_loss: 142911.7031\n","Epoch 122/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 143255.5312\n","Epoch 122: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 140017.2500 - val_loss: 78262.1406\n","Epoch 123/1000\n","76/75 [==============================] - ETA: 0s - loss: 123710.1094\n","Epoch 123: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 17ms/step - loss: 123710.1094 - val_loss: 100309.5703\n","Epoch 124/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 130655.4766\n","Epoch 124: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 127766.5000 - val_loss: 101754.9219\n","Epoch 125/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 181517.2500\n","Epoch 125: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 177132.0156 - val_loss: 122956.6797\n","Epoch 126/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 155138.8594\n","Epoch 126: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 151249.2656 - val_loss: 65760.0547\n","Epoch 127/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 125381.9453\n","Epoch 127: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 122843.8672 - val_loss: 69063.7500\n","Epoch 128/1000\n","76/75 [==============================] - ETA: 0s - loss: 115897.1797\n","Epoch 128: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 17ms/step - loss: 115897.1797 - val_loss: 75336.5781\n","Epoch 129/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 117277.6797\n","Epoch 129: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 115027.4688 - val_loss: 80819.3281\n","Epoch 130/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 121928.6562\n","Epoch 130: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 119165.1641 - val_loss: 77949.5156\n","Epoch 131/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 120763.9688\n","Epoch 131: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 117914.1250 - val_loss: 76793.6328\n","Epoch 132/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 112198.3203\n","Epoch 132: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 109547.0312 - val_loss: 66891.8906\n","Epoch 133/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 114145.1562\n","Epoch 133: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 111344.5391 - val_loss: 75990.1406\n","Epoch 134/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 109725.6328\n","Epoch 134: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 107067.1641 - val_loss: 96328.6641\n","Epoch 135/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 112240.3125\n","Epoch 135: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 109498.9219 - val_loss: 94057.0469\n","Epoch 136/1000\n","76/75 [==============================] - ETA: 0s - loss: 111934.6484\n","Epoch 136: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 111934.6484 - val_loss: 84827.9062\n","Epoch 137/1000\n","76/75 [==============================] - ETA: 0s - loss: 111354.3750\n","Epoch 137: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 111354.3750 - val_loss: 84127.4844\n","Epoch 138/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 106287.3828\n","Epoch 138: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 103406.6328 - val_loss: 77041.2344\n","Epoch 139/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 113800.1641\n","Epoch 139: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 112002.4453 - val_loss: 68559.1172\n","Epoch 140/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 194685.3750\n","Epoch 140: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 189245.5781 - val_loss: 134261.4531\n","Epoch 141/1000\n","76/75 [==============================] - ETA: 0s - loss: 162786.2656\n","Epoch 141: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 162786.2656 - val_loss: 136315.5938\n","Epoch 142/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 158079.8906\n","Epoch 142: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 16ms/step - loss: 154355.1719 - val_loss: 103572.1797\n","Epoch 143/1000\n","76/75 [==============================] - ETA: 0s - loss: 142953.8125\n","Epoch 143: val_loss did not improve from 53563.01953\n","75/75 [==============================] - 1s 17ms/step - loss: 142953.8125 - val_loss: 84626.6094\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1447/1447 [00:00<00:00, 24344.89it/s]\n","100%|██████████| 1419/1419 [00:00<00:00, 23313.28it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 3547482.7500\n","Epoch 1: val_loss improved from inf to 3493115.25000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-2-1000-15.h5\n","75/75 [==============================] - 6s 24ms/step - loss: 3433234.0000 - val_loss: 3493115.2500\n","Epoch 2/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 526455.0625\n","Epoch 2: val_loss improved from 3493115.25000 to 2406913.00000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-2-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 515184.2812 - val_loss: 2406913.0000\n","Epoch 3/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 352292.9375\n","Epoch 3: val_loss improved from 2406913.00000 to 1950719.37500, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-2-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 345758.2500 - val_loss: 1950719.3750\n","Epoch 4/1000\n","76/75 [==============================] - ETA: 0s - loss: 354389.4375\n","Epoch 4: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 354389.4375 - val_loss: 2370575.7500\n","Epoch 5/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 366958.9375\n","Epoch 5: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 359090.3750 - val_loss: 2460959.0000\n","Epoch 6/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 382978.3438\n","Epoch 6: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 375790.3125 - val_loss: 2392947.0000\n","Epoch 7/1000\n","76/75 [==============================] - ETA: 0s - loss: 381859.1250\n","Epoch 7: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 381859.1250 - val_loss: 2329900.2500\n","Epoch 8/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 353268.8750\n","Epoch 8: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 346804.6875 - val_loss: 2227927.7500\n","Epoch 9/1000\n","76/75 [==============================] - ETA: 0s - loss: 380182.3125\n","Epoch 9: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 380182.3125 - val_loss: 2332416.5000\n","Epoch 10/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 343508.8750\n","Epoch 10: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 337128.7188 - val_loss: 2250163.2500\n","Epoch 11/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 337126.4688\n","Epoch 11: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 330049.6875 - val_loss: 2161781.5000\n","Epoch 12/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 352436.4375\n","Epoch 12: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 345864.0000 - val_loss: 2157717.2500\n","Epoch 13/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 349755.9062\n","Epoch 13: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 342477.1875 - val_loss: 2219302.5000\n","Epoch 14/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 355457.0000\n","Epoch 14: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 347401.6875 - val_loss: 2081576.1250\n","Epoch 15/1000\n","76/75 [==============================] - ETA: 0s - loss: 338446.4688\n","Epoch 15: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 338446.4688 - val_loss: 2081117.7500\n","Epoch 16/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 350508.0938\n","Epoch 16: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 342077.0312 - val_loss: 2173854.5000\n","Epoch 17/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 354717.5625\n","Epoch 17: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 346602.7500 - val_loss: 2134154.5000\n","Epoch 18/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 346150.3125\n","Epoch 18: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 338863.8438 - val_loss: 2095804.3750\n","Epoch 19/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 334996.3750\n","Epoch 19: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 327138.9688 - val_loss: 2107659.7500\n","Epoch 20/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 338882.0625\n","Epoch 20: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 332518.4688 - val_loss: 2134266.5000\n","Epoch 21/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 352369.8438\n","Epoch 21: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 345450.1562 - val_loss: 2265161.7500\n","Epoch 22/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 347307.1250\n","Epoch 22: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 339800.7812 - val_loss: 2222080.2500\n","Epoch 23/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 353821.9062\n","Epoch 23: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 350559.0938 - val_loss: 2107077.5000\n","Epoch 24/1000\n","76/75 [==============================] - ETA: 0s - loss: 346384.8438\n","Epoch 24: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 346384.8438 - val_loss: 2064889.8750\n","Epoch 25/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 329576.2812\n","Epoch 25: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 321907.5625 - val_loss: 2206613.2500\n","Epoch 26/1000\n","76/75 [==============================] - ETA: 0s - loss: 342715.0938\n","Epoch 26: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 342715.0938 - val_loss: 2296097.5000\n","Epoch 27/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 380731.5625\n","Epoch 27: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 371429.0625 - val_loss: 2222035.5000\n","Epoch 28/1000\n","76/75 [==============================] - ETA: 0s - loss: 369433.6250\n","Epoch 28: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 17ms/step - loss: 369433.6250 - val_loss: 2206580.5000\n","Epoch 29/1000\n","76/75 [==============================] - ETA: 0s - loss: 343398.7812\n","Epoch 29: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 343398.7812 - val_loss: 2173731.5000\n","Epoch 30/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 334470.9688\n","Epoch 30: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 327693.5938 - val_loss: 2137020.2500\n","Epoch 31/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 341976.6562\n","Epoch 31: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 333349.8750 - val_loss: 2107774.2500\n","Epoch 32/1000\n","76/75 [==============================] - ETA: 0s - loss: 325183.1250\n","Epoch 32: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 325183.1250 - val_loss: 2150959.7500\n","Epoch 33/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 337865.9062\n","Epoch 33: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 330475.2812 - val_loss: 2096838.5000\n","Epoch 34/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 332284.8750\n","Epoch 34: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 327070.3750 - val_loss: 2284738.2500\n","Epoch 35/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 344614.0312\n","Epoch 35: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 337693.9375 - val_loss: 2115260.2500\n","Epoch 36/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 336506.8750\n","Epoch 36: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 328655.9062 - val_loss: 2138273.7500\n","Epoch 37/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 333993.4375\n","Epoch 37: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 325752.7812 - val_loss: 2124222.7500\n","Epoch 38/1000\n","76/75 [==============================] - ETA: 0s - loss: 352714.2188\n","Epoch 38: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 17ms/step - loss: 352714.2188 - val_loss: 2184659.0000\n","Epoch 39/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 351724.8438\n","Epoch 39: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 344342.0625 - val_loss: 2096671.5000\n","Epoch 40/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 334665.9062\n","Epoch 40: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 326914.1562 - val_loss: 2158209.2500\n","Epoch 41/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 327623.7812\n","Epoch 41: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 17ms/step - loss: 319799.0938 - val_loss: 2094935.3750\n","Epoch 42/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 336973.6250\n","Epoch 42: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 328643.2188 - val_loss: 2150828.5000\n","Epoch 43/1000\n","76/75 [==============================] - ETA: 0s - loss: 341596.9688\n","Epoch 43: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 17ms/step - loss: 341596.9688 - val_loss: 1994388.2500\n","Epoch 44/1000\n","76/75 [==============================] - ETA: 0s - loss: 319053.4062\n","Epoch 44: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 17ms/step - loss: 319053.4062 - val_loss: 2113703.0000\n","Epoch 45/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 334781.1250\n","Epoch 45: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 327220.0625 - val_loss: 2106830.7500\n","Epoch 46/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 335300.7812\n","Epoch 46: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 17ms/step - loss: 328366.2812 - val_loss: 2138660.2500\n","Epoch 47/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 341383.6250\n","Epoch 47: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 334539.9375 - val_loss: 2192559.5000\n","Epoch 48/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 341948.5938\n","Epoch 48: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 17ms/step - loss: 347158.8750 - val_loss: 2545778.2500\n","Epoch 49/1000\n","76/75 [==============================] - ETA: 0s - loss: 320617.6562\n","Epoch 49: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 17ms/step - loss: 320617.6562 - val_loss: 2409706.7500\n","Epoch 50/1000\n","76/75 [==============================] - ETA: 0s - loss: 319815.8438\n","Epoch 50: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 17ms/step - loss: 319815.8438 - val_loss: 2337432.5000\n","Epoch 51/1000\n","76/75 [==============================] - ETA: 0s - loss: 316766.7188\n","Epoch 51: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 17ms/step - loss: 316766.7188 - val_loss: 2065602.0000\n","Epoch 52/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 285600.7500\n","Epoch 52: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 17ms/step - loss: 282123.0625 - val_loss: 2399334.7500\n","Epoch 53/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 334722.0625\n","Epoch 53: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 329328.4062 - val_loss: 2339185.7500\n","Epoch 54/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 358766.6562\n","Epoch 54: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 17ms/step - loss: 350716.8750 - val_loss: 2133691.5000\n","Epoch 55/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 349723.9375\n","Epoch 55: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 343370.8125 - val_loss: 2129448.0000\n","Epoch 56/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 343289.0000\n","Epoch 56: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 337755.4688 - val_loss: 2299585.0000\n","Epoch 57/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 356964.7188\n","Epoch 57: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 17ms/step - loss: 349873.4375 - val_loss: 2124145.7500\n","Epoch 58/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 338235.9062\n","Epoch 58: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 331603.1250 - val_loss: 2137562.5000\n","Epoch 59/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 339035.8438\n","Epoch 59: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 332848.6562 - val_loss: 2205841.2500\n","Epoch 60/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 343167.1875\n","Epoch 60: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 336619.3125 - val_loss: 2193888.2500\n","Epoch 61/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 355305.7188\n","Epoch 61: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 346753.5625 - val_loss: 1985733.0000\n","Epoch 62/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 342813.0000\n","Epoch 62: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 335154.0938 - val_loss: 2032970.6250\n","Epoch 63/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 331224.5625\n","Epoch 63: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 324088.1250 - val_loss: 2064397.1250\n","Epoch 64/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 321538.9688\n","Epoch 64: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 314429.4688 - val_loss: 1971190.7500\n","Epoch 65/1000\n","76/75 [==============================] - ETA: 0s - loss: 306070.8125\n","Epoch 65: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 306070.8125 - val_loss: 2019202.0000\n","Epoch 66/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 343433.5938\n","Epoch 66: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 336444.0938 - val_loss: 2049064.3750\n","Epoch 67/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 342069.7500\n","Epoch 67: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 333641.8438 - val_loss: 2073822.3750\n","Epoch 68/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 331147.0000\n","Epoch 68: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 323320.9062 - val_loss: 1990845.2500\n","Epoch 69/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 321179.4062\n","Epoch 69: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 313499.6875 - val_loss: 2056852.5000\n","Epoch 70/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 338459.3750\n","Epoch 70: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 330952.8750 - val_loss: 1960742.5000\n","Epoch 71/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 315205.1875\n","Epoch 71: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 308608.2188 - val_loss: 2029957.8750\n","Epoch 72/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 315383.7812\n","Epoch 72: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 307580.7500 - val_loss: 2027105.6250\n","Epoch 73/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 320188.5000\n","Epoch 73: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 313053.8750 - val_loss: 1980516.2500\n","Epoch 74/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 329579.9062\n","Epoch 74: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 17ms/step - loss: 321990.0312 - val_loss: 2013018.3750\n","Epoch 75/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 308188.5312\n","Epoch 75: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 300369.9375 - val_loss: 1963796.7500\n","Epoch 76/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 310344.0312\n","Epoch 76: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 303365.7812 - val_loss: 2037489.1250\n","Epoch 77/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 315501.8125\n","Epoch 77: val_loss did not improve from 1950719.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 308299.5312 - val_loss: 1953107.1250\n","Epoch 78/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 322779.2812\n","Epoch 78: val_loss improved from 1950719.37500 to 1937827.62500, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-2-1000-15.h5\n","75/75 [==============================] - 2s 22ms/step - loss: 315166.8125 - val_loss: 1937827.6250\n","Epoch 79/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 324807.2812\n","Epoch 79: val_loss did not improve from 1937827.62500\n","75/75 [==============================] - 1s 16ms/step - loss: 316436.7500 - val_loss: 1971080.1250\n","Epoch 80/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 317893.8438\n","Epoch 80: val_loss did not improve from 1937827.62500\n","75/75 [==============================] - 1s 17ms/step - loss: 310236.1562 - val_loss: 2007290.8750\n","Epoch 81/1000\n","76/75 [==============================] - ETA: 0s - loss: 309510.4375\n","Epoch 81: val_loss did not improve from 1937827.62500\n","75/75 [==============================] - 1s 16ms/step - loss: 309510.4375 - val_loss: 1940998.7500\n","Epoch 82/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 313664.7812\n","Epoch 82: val_loss did not improve from 1937827.62500\n","75/75 [==============================] - 1s 16ms/step - loss: 306059.3438 - val_loss: 1960136.7500\n","Epoch 83/1000\n","76/75 [==============================] - ETA: 0s - loss: 308476.4062\n","Epoch 83: val_loss did not improve from 1937827.62500\n","75/75 [==============================] - 1s 17ms/step - loss: 308476.4062 - val_loss: 1951534.3750\n","Epoch 84/1000\n","75/75 [============================>.] - ETA: 0s - loss: 318312.5000\n","Epoch 84: val_loss improved from 1937827.62500 to 1923027.87500, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-2-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 316792.5938 - val_loss: 1923027.8750\n","Epoch 85/1000\n","74/75 [============================>.] - ETA: 0s - loss: 314732.4062\n","Epoch 85: val_loss improved from 1923027.87500 to 1915805.50000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-2-1000-15.h5\n","75/75 [==============================] - 2s 20ms/step - loss: 310227.6250 - val_loss: 1915805.5000\n","Epoch 86/1000\n","76/75 [==============================] - ETA: 0s - loss: 309532.5312\n","Epoch 86: val_loss improved from 1915805.50000 to 1844333.62500, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-2-1000-15.h5\n","75/75 [==============================] - 2s 21ms/step - loss: 309532.5312 - val_loss: 1844333.6250\n","Epoch 87/1000\n","75/75 [============================>.] - ETA: 0s - loss: 331951.0312\n","Epoch 87: val_loss did not improve from 1844333.62500\n","75/75 [==============================] - 1s 16ms/step - loss: 331939.5938 - val_loss: 2230207.0000\n","Epoch 88/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 323610.6562\n","Epoch 88: val_loss did not improve from 1844333.62500\n","75/75 [==============================] - 1s 17ms/step - loss: 316746.4688 - val_loss: 2017770.6250\n","Epoch 89/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 315958.3125\n","Epoch 89: val_loss did not improve from 1844333.62500\n","75/75 [==============================] - 1s 16ms/step - loss: 308433.4375 - val_loss: 1958815.7500\n","Epoch 90/1000\n","76/75 [==============================] - ETA: 0s - loss: 325533.0625\n","Epoch 90: val_loss did not improve from 1844333.62500\n","75/75 [==============================] - 1s 16ms/step - loss: 325533.0625 - val_loss: 1910255.5000\n","Epoch 91/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 322921.0000\n","Epoch 91: val_loss improved from 1844333.62500 to 1837168.12500, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-2-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 314197.4375 - val_loss: 1837168.1250\n","Epoch 92/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 311138.8438\n","Epoch 92: val_loss did not improve from 1837168.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 303251.2188 - val_loss: 1915459.1250\n","Epoch 93/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 347586.5625\n","Epoch 93: val_loss did not improve from 1837168.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 338366.6562 - val_loss: 2191485.2500\n","Epoch 94/1000\n","76/75 [==============================] - ETA: 0s - loss: 361922.4375\n","Epoch 94: val_loss did not improve from 1837168.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 361922.4375 - val_loss: 2119912.7500\n","Epoch 95/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 361332.5000\n","Epoch 95: val_loss did not improve from 1837168.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 351335.2812 - val_loss: 2135225.2500\n","Epoch 96/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 343650.1250\n","Epoch 96: val_loss did not improve from 1837168.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 335379.0938 - val_loss: 2153551.5000\n","Epoch 97/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 351880.8438\n","Epoch 97: val_loss did not improve from 1837168.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 342704.0312 - val_loss: 2172143.2500\n","Epoch 98/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 362787.8125\n","Epoch 98: val_loss did not improve from 1837168.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 353325.0312 - val_loss: 2152962.0000\n","Epoch 99/1000\n","76/75 [==============================] - ETA: 0s - loss: 320415.5312\n","Epoch 99: val_loss did not improve from 1837168.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 320415.5312 - val_loss: 2101002.7500\n","Epoch 100/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 346476.3750\n","Epoch 100: val_loss did not improve from 1837168.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 338670.6562 - val_loss: 2036534.1250\n","Epoch 101/1000\n","76/75 [==============================] - ETA: 0s - loss: 330372.5312\n","Epoch 101: val_loss did not improve from 1837168.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 330372.5312 - val_loss: 2049324.6250\n","Epoch 102/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 341955.6875\n","Epoch 102: val_loss did not improve from 1837168.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 333320.6875 - val_loss: 2017423.1250\n","Epoch 103/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 335653.1250\n","Epoch 103: val_loss did not improve from 1837168.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 327397.0312 - val_loss: 1969681.3750\n","Epoch 104/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 355796.7812\n","Epoch 104: val_loss did not improve from 1837168.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 347135.1875 - val_loss: 2059694.0000\n","Epoch 105/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 340470.0312\n","Epoch 105: val_loss did not improve from 1837168.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 332389.3438 - val_loss: 2008911.3750\n","Epoch 106/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 353583.0938\n","Epoch 106: val_loss did not improve from 1837168.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 344448.4375 - val_loss: 1981657.8750\n","Epoch 107/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 355622.8750\n","Epoch 107: val_loss did not improve from 1837168.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 346139.5312 - val_loss: 1990128.8750\n","Epoch 108/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 353199.6250\n","Epoch 108: val_loss did not improve from 1837168.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 344534.0625 - val_loss: 1974106.6250\n","Epoch 109/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 363178.8125\n","Epoch 109: val_loss did not improve from 1837168.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 353480.0938 - val_loss: 2005430.2500\n","Epoch 110/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 351034.5938\n","Epoch 110: val_loss did not improve from 1837168.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 342382.7188 - val_loss: 1983498.3750\n","Epoch 111/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 348868.5000\n","Epoch 111: val_loss did not improve from 1837168.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 341248.3438 - val_loss: 1978436.2500\n","Epoch 112/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 345930.8125\n","Epoch 112: val_loss did not improve from 1837168.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 337760.2500 - val_loss: 1923426.7500\n","Epoch 113/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 350058.5000\n","Epoch 113: val_loss did not improve from 1837168.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 341197.1250 - val_loss: 1873257.2500\n","Epoch 114/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 337755.1250\n","Epoch 114: val_loss did not improve from 1837168.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 330316.3125 - val_loss: 1879881.5000\n","Epoch 115/1000\n","76/75 [==============================] - ETA: 0s - loss: 327058.7812\n","Epoch 115: val_loss did not improve from 1837168.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 327058.7812 - val_loss: 1896899.1250\n","Epoch 116/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 343777.6250\n","Epoch 116: val_loss did not improve from 1837168.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 335997.1562 - val_loss: 1897494.1250\n","Epoch 117/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 336722.5938\n","Epoch 117: val_loss did not improve from 1837168.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 328483.3125 - val_loss: 1879605.2500\n","Epoch 118/1000\n","76/75 [==============================] - ETA: 0s - loss: 317904.5625\n","Epoch 118: val_loss improved from 1837168.12500 to 1808293.62500, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-2-1000-15.h5\n","75/75 [==============================] - 2s 23ms/step - loss: 317904.5625 - val_loss: 1808293.6250\n","Epoch 119/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 334331.9062\n","Epoch 119: val_loss improved from 1808293.62500 to 1677428.50000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-2-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 326080.4375 - val_loss: 1677428.5000\n","Epoch 120/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 313263.6562\n","Epoch 120: val_loss did not improve from 1677428.50000\n","75/75 [==============================] - 1s 16ms/step - loss: 305521.4062 - val_loss: 1854084.7500\n","Epoch 121/1000\n","76/75 [==============================] - ETA: 0s - loss: 308797.0625\n","Epoch 121: val_loss did not improve from 1677428.50000\n","75/75 [==============================] - 1s 16ms/step - loss: 308797.0625 - val_loss: 1753055.2500\n","Epoch 122/1000\n","76/75 [==============================] - ETA: 0s - loss: 370017.4688\n","Epoch 122: val_loss improved from 1677428.50000 to 1507238.37500, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-2-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 370017.4688 - val_loss: 1507238.3750\n","Epoch 123/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 399103.4688\n","Epoch 123: val_loss did not improve from 1507238.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 393564.0938 - val_loss: 2036915.1250\n","Epoch 124/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 351924.1875\n","Epoch 124: val_loss did not improve from 1507238.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 343750.3125 - val_loss: 1841696.8750\n","Epoch 125/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 313928.1562\n","Epoch 125: val_loss did not improve from 1507238.37500\n","75/75 [==============================] - 1s 17ms/step - loss: 306000.7812 - val_loss: 1870914.2500\n","Epoch 126/1000\n","76/75 [==============================] - ETA: 0s - loss: 319542.8125\n","Epoch 126: val_loss did not improve from 1507238.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 319542.8125 - val_loss: 1817916.0000\n","Epoch 127/1000\n","76/75 [==============================] - ETA: 0s - loss: 320095.7812\n","Epoch 127: val_loss did not improve from 1507238.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 320095.7812 - val_loss: 1860261.1250\n","Epoch 128/1000\n","76/75 [==============================] - ETA: 0s - loss: 351761.4375\n","Epoch 128: val_loss did not improve from 1507238.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 351761.4375 - val_loss: 1795829.7500\n","Epoch 129/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 320077.3438\n","Epoch 129: val_loss did not improve from 1507238.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 312381.3438 - val_loss: 1818022.2500\n","Epoch 130/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 325711.2188\n","Epoch 130: val_loss did not improve from 1507238.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 317552.6250 - val_loss: 1769162.6250\n","Epoch 131/1000\n","76/75 [==============================] - ETA: 0s - loss: 336008.6250\n","Epoch 131: val_loss did not improve from 1507238.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 336008.6250 - val_loss: 1710280.6250\n","Epoch 132/1000\n","76/75 [==============================] - ETA: 0s - loss: 315595.9062\n","Epoch 132: val_loss did not improve from 1507238.37500\n","75/75 [==============================] - 1s 17ms/step - loss: 315595.9062 - val_loss: 1662647.8750\n","Epoch 133/1000\n","76/75 [==============================] - ETA: 0s - loss: 324603.7500\n","Epoch 133: val_loss did not improve from 1507238.37500\n","75/75 [==============================] - 1s 17ms/step - loss: 324603.7500 - val_loss: 1616412.5000\n","Epoch 134/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 335714.0000\n","Epoch 134: val_loss did not improve from 1507238.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 328667.5000 - val_loss: 1607066.8750\n","Epoch 135/1000\n","76/75 [==============================] - ETA: 0s - loss: 323858.5938\n","Epoch 135: val_loss did not improve from 1507238.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 323858.5938 - val_loss: 1622880.1250\n","Epoch 136/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 306802.6562\n","Epoch 136: val_loss did not improve from 1507238.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 299465.6250 - val_loss: 1527649.8750\n","Epoch 137/1000\n","76/75 [==============================] - ETA: 0s - loss: 319592.9062\n","Epoch 137: val_loss did not improve from 1507238.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 319592.9062 - val_loss: 1569422.3750\n","Epoch 138/1000\n","76/75 [==============================] - ETA: 0s - loss: 334657.7500\n","Epoch 138: val_loss did not improve from 1507238.37500\n","75/75 [==============================] - 1s 17ms/step - loss: 334657.7500 - val_loss: 1668420.8750\n","Epoch 139/1000\n","76/75 [==============================] - ETA: 0s - loss: 325423.7188\n","Epoch 139: val_loss did not improve from 1507238.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 325423.7188 - val_loss: 1630944.1250\n","Epoch 140/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 325367.2812\n","Epoch 140: val_loss did not improve from 1507238.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 318587.9062 - val_loss: 1609968.2500\n","Epoch 141/1000\n","76/75 [==============================] - ETA: 0s - loss: 324149.4688\n","Epoch 141: val_loss improved from 1507238.37500 to 1470377.12500, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-2-1000-15.h5\n","75/75 [==============================] - 2s 22ms/step - loss: 324149.4688 - val_loss: 1470377.1250\n","Epoch 142/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 329941.4688\n","Epoch 142: val_loss did not improve from 1470377.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 323307.9062 - val_loss: 1629643.3750\n","Epoch 143/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 311354.0000\n","Epoch 143: val_loss did not improve from 1470377.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 304693.8750 - val_loss: 1557365.8750\n","Epoch 144/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 328919.4062\n","Epoch 144: val_loss did not improve from 1470377.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 319899.3750 - val_loss: 1618712.1250\n","Epoch 145/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 332311.9062\n","Epoch 145: val_loss did not improve from 1470377.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 327446.7188 - val_loss: 1598156.8750\n","Epoch 146/1000\n","75/75 [============================>.] - ETA: 0s - loss: 318245.1875\n","Epoch 146: val_loss did not improve from 1470377.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 316195.7500 - val_loss: 1541719.7500\n","Epoch 147/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 315593.8125\n","Epoch 147: val_loss did not improve from 1470377.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 309134.6875 - val_loss: 1533562.0000\n","Epoch 148/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 313739.3750\n","Epoch 148: val_loss improved from 1470377.12500 to 1360784.25000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-2-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 310179.8750 - val_loss: 1360784.2500\n","Epoch 149/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 328629.2812\n","Epoch 149: val_loss did not improve from 1360784.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 320928.7188 - val_loss: 1555639.5000\n","Epoch 150/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 352938.6562\n","Epoch 150: val_loss did not improve from 1360784.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 345296.9375 - val_loss: 1545605.1250\n","Epoch 151/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 340601.5625\n","Epoch 151: val_loss did not improve from 1360784.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 334869.4375 - val_loss: 1609307.2500\n","Epoch 152/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 362934.4375\n","Epoch 152: val_loss did not improve from 1360784.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 356435.6562 - val_loss: 1541636.8750\n","Epoch 153/1000\n","76/75 [==============================] - ETA: 0s - loss: 312701.5938\n","Epoch 153: val_loss did not improve from 1360784.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 312701.5938 - val_loss: 1381723.3750\n","Epoch 154/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 307365.6875\n","Epoch 154: val_loss did not improve from 1360784.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 304059.3125 - val_loss: 1385376.7500\n","Epoch 155/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 330154.7500\n","Epoch 155: val_loss did not improve from 1360784.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 325116.2812 - val_loss: 1431300.1250\n","Epoch 156/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 324324.6250\n","Epoch 156: val_loss improved from 1360784.25000 to 1282576.25000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-2-1000-15.h5\n","75/75 [==============================] - 2s 22ms/step - loss: 319411.0312 - val_loss: 1282576.2500\n","Epoch 157/1000\n","76/75 [==============================] - ETA: 0s - loss: 309889.9375\n","Epoch 157: val_loss did not improve from 1282576.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 309889.9375 - val_loss: 1296345.0000\n","Epoch 158/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 332247.2188\n","Epoch 158: val_loss did not improve from 1282576.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 325147.2812 - val_loss: 1459170.8750\n","Epoch 159/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 326955.8750\n","Epoch 159: val_loss did not improve from 1282576.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 322131.6250 - val_loss: 1448509.5000\n","Epoch 160/1000\n","76/75 [==============================] - ETA: 0s - loss: 319972.8125\n","Epoch 160: val_loss did not improve from 1282576.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 319972.8125 - val_loss: 1333059.6250\n","Epoch 161/1000\n","75/75 [============================>.] - ETA: 0s - loss: 315800.7812\n","Epoch 161: val_loss did not improve from 1282576.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 313751.7188 - val_loss: 1302581.2500\n","Epoch 162/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 363371.5625\n","Epoch 162: val_loss did not improve from 1282576.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 355811.4688 - val_loss: 1362017.3750\n","Epoch 163/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 320124.6250\n","Epoch 163: val_loss did not improve from 1282576.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 317078.9062 - val_loss: 1437703.3750\n","Epoch 164/1000\n","76/75 [==============================] - ETA: 0s - loss: 350216.5625\n","Epoch 164: val_loss did not improve from 1282576.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 350216.5625 - val_loss: 1575681.7500\n","Epoch 165/1000\n","74/75 [============================>.] - ETA: 0s - loss: 300296.3438\n","Epoch 165: val_loss did not improve from 1282576.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 297063.7500 - val_loss: 1482688.2500\n","Epoch 166/1000\n","76/75 [==============================] - ETA: 0s - loss: 302803.5625\n","Epoch 166: val_loss did not improve from 1282576.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 302803.5625 - val_loss: 1466397.0000\n","Epoch 167/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 309955.5938\n","Epoch 167: val_loss did not improve from 1282576.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 303072.9062 - val_loss: 1518799.1250\n","Epoch 168/1000\n","76/75 [==============================] - ETA: 0s - loss: 293744.7188\n","Epoch 168: val_loss did not improve from 1282576.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 293744.7188 - val_loss: 1560966.7500\n","Epoch 169/1000\n","76/75 [==============================] - ETA: 0s - loss: 279852.8125\n","Epoch 169: val_loss did not improve from 1282576.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 279852.8125 - val_loss: 1439908.7500\n","Epoch 170/1000\n","76/75 [==============================] - ETA: 0s - loss: 289121.2500\n","Epoch 170: val_loss did not improve from 1282576.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 289121.2500 - val_loss: 1461797.7500\n","Epoch 171/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 298316.7812\n","Epoch 171: val_loss did not improve from 1282576.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 294034.2812 - val_loss: 1526482.1250\n","Epoch 172/1000\n","76/75 [==============================] - ETA: 0s - loss: 278027.8438\n","Epoch 172: val_loss did not improve from 1282576.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 278027.8438 - val_loss: 1467126.6250\n","Epoch 173/1000\n","76/75 [==============================] - ETA: 0s - loss: 284567.5000\n","Epoch 173: val_loss did not improve from 1282576.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 284567.5000 - val_loss: 1371468.2500\n","Epoch 174/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 274290.4688\n","Epoch 174: val_loss did not improve from 1282576.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 268760.3438 - val_loss: 1453028.3750\n","Epoch 175/1000\n","76/75 [==============================] - ETA: 0s - loss: 284870.0312\n","Epoch 175: val_loss did not improve from 1282576.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 284870.0312 - val_loss: 1394606.2500\n","Epoch 176/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 275395.7500\n","Epoch 176: val_loss did not improve from 1282576.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 269639.9688 - val_loss: 1360857.7500\n","Epoch 177/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 273698.4375\n","Epoch 177: val_loss did not improve from 1282576.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 267270.7812 - val_loss: 1406999.8750\n","Epoch 178/1000\n","76/75 [==============================] - ETA: 0s - loss: 264865.4688\n","Epoch 178: val_loss did not improve from 1282576.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 264865.4688 - val_loss: 1353273.7500\n","Epoch 179/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 273701.1562\n","Epoch 179: val_loss did not improve from 1282576.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 266892.8438 - val_loss: 1362917.7500\n","Epoch 180/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 261421.4062\n","Epoch 180: val_loss did not improve from 1282576.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 258410.5938 - val_loss: 1390053.1250\n","Epoch 181/1000\n","76/75 [==============================] - ETA: 0s - loss: 269703.3125\n","Epoch 181: val_loss did not improve from 1282576.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 269703.3125 - val_loss: 1373276.5000\n","Epoch 182/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 281107.1562\n","Epoch 182: val_loss did not improve from 1282576.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 274861.2500 - val_loss: 1286822.1250\n","Epoch 183/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 265614.4375\n","Epoch 183: val_loss did not improve from 1282576.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 258691.3594 - val_loss: 1339449.6250\n","Epoch 184/1000\n","76/75 [==============================] - ETA: 0s - loss: 266758.7500\n","Epoch 184: val_loss did not improve from 1282576.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 266758.7500 - val_loss: 1366436.0000\n","Epoch 185/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 283996.8125\n","Epoch 185: val_loss did not improve from 1282576.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 277661.1875 - val_loss: 1363114.2500\n","Epoch 186/1000\n","76/75 [==============================] - ETA: 0s - loss: 261522.6719\n","Epoch 186: val_loss improved from 1282576.25000 to 1272162.50000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-2-1000-15.h5\n","75/75 [==============================] - 2s 22ms/step - loss: 261522.6719 - val_loss: 1272162.5000\n","Epoch 187/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 295241.8750\n","Epoch 187: val_loss did not improve from 1272162.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 289517.4375 - val_loss: 1389542.0000\n","Epoch 188/1000\n","76/75 [==============================] - ETA: 0s - loss: 287543.8750\n","Epoch 188: val_loss did not improve from 1272162.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 287543.8750 - val_loss: 1367690.0000\n","Epoch 189/1000\n","75/75 [============================>.] - ETA: 0s - loss: 281265.1250\n","Epoch 189: val_loss did not improve from 1272162.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 279795.4688 - val_loss: 1417396.6250\n","Epoch 190/1000\n","76/75 [==============================] - ETA: 0s - loss: 282450.4688\n","Epoch 190: val_loss did not improve from 1272162.50000\n","75/75 [==============================] - 1s 16ms/step - loss: 282450.4688 - val_loss: 1511954.3750\n","Epoch 191/1000\n","76/75 [==============================] - ETA: 0s - loss: 329877.3438\n","Epoch 191: val_loss did not improve from 1272162.50000\n","75/75 [==============================] - 1s 16ms/step - loss: 329877.3438 - val_loss: 1536475.6250\n","Epoch 192/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 303230.8750\n","Epoch 192: val_loss improved from 1272162.50000 to 1269525.62500, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-2-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 296981.4375 - val_loss: 1269525.6250\n","Epoch 193/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 343623.8125\n","Epoch 193: val_loss did not improve from 1269525.62500\n","75/75 [==============================] - 1s 16ms/step - loss: 336299.6562 - val_loss: 1295553.6250\n","Epoch 194/1000\n","76/75 [==============================] - ETA: 0s - loss: 287775.6562\n","Epoch 194: val_loss did not improve from 1269525.62500\n","75/75 [==============================] - 1s 16ms/step - loss: 287775.6562 - val_loss: 1298060.7500\n","Epoch 195/1000\n","76/75 [==============================] - ETA: 0s - loss: 280174.8438\n","Epoch 195: val_loss did not improve from 1269525.62500\n","75/75 [==============================] - 1s 17ms/step - loss: 280174.8438 - val_loss: 1278231.3750\n","Epoch 196/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 288791.2812\n","Epoch 196: val_loss improved from 1269525.62500 to 1239537.12500, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-2-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 282633.9375 - val_loss: 1239537.1250\n","Epoch 197/1000\n","75/75 [============================>.] - ETA: 0s - loss: 293678.8750\n","Epoch 197: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 291791.3750 - val_loss: 1474757.8750\n","Epoch 198/1000\n","76/75 [==============================] - ETA: 0s - loss: 374436.7500\n","Epoch 198: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 374436.7500 - val_loss: 1409164.8750\n","Epoch 199/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 326809.5312\n","Epoch 199: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 322465.5625 - val_loss: 1505306.5000\n","Epoch 200/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 346748.8750\n","Epoch 200: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 340591.3750 - val_loss: 1533147.0000\n","Epoch 201/1000\n","74/75 [============================>.] - ETA: 0s - loss: 387415.2500\n","Epoch 201: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 380953.5000 - val_loss: 1674634.8750\n","Epoch 202/1000\n","76/75 [==============================] - ETA: 0s - loss: 351298.3438\n","Epoch 202: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 351298.3438 - val_loss: 1459677.0000\n","Epoch 203/1000\n","76/75 [==============================] - ETA: 0s - loss: 342066.5938\n","Epoch 203: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 342066.5938 - val_loss: 1417534.5000\n","Epoch 204/1000\n","75/75 [============================>.] - ETA: 0s - loss: 333093.4062\n","Epoch 204: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 18ms/step - loss: 330753.2188 - val_loss: 1424633.7500\n","Epoch 205/1000\n","76/75 [==============================] - ETA: 0s - loss: 305752.4062\n","Epoch 205: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 19ms/step - loss: 305752.4062 - val_loss: 1522509.1250\n","Epoch 206/1000\n","74/75 [============================>.] - ETA: 0s - loss: 331582.4375\n","Epoch 206: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 19ms/step - loss: 326171.2812 - val_loss: 1508757.3750\n","Epoch 207/1000\n","75/75 [============================>.] - ETA: 0s - loss: 300790.6250\n","Epoch 207: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 298520.9375 - val_loss: 1265618.8750\n","Epoch 208/1000\n","76/75 [==============================] - ETA: 0s - loss: 288359.3438\n","Epoch 208: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 288359.3438 - val_loss: 1314880.3750\n","Epoch 209/1000\n","76/75 [==============================] - ETA: 0s - loss: 266678.9688\n","Epoch 209: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 266678.9688 - val_loss: 1385069.7500\n","Epoch 210/1000\n","76/75 [==============================] - ETA: 0s - loss: 259403.8438\n","Epoch 210: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 259403.8438 - val_loss: 1425344.5000\n","Epoch 211/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 280973.5312\n","Epoch 211: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 275164.4688 - val_loss: 1444458.6250\n","Epoch 212/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 287227.3125\n","Epoch 212: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 280617.6562 - val_loss: 1417155.7500\n","Epoch 213/1000\n","76/75 [==============================] - ETA: 0s - loss: 270249.0625\n","Epoch 213: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 270249.0625 - val_loss: 1304800.8750\n","Epoch 214/1000\n","76/75 [==============================] - ETA: 0s - loss: 266124.6562\n","Epoch 214: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 266124.6562 - val_loss: 1295396.5000\n","Epoch 215/1000\n","76/75 [==============================] - ETA: 0s - loss: 277044.4062\n","Epoch 215: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 277044.4062 - val_loss: 1272546.0000\n","Epoch 216/1000\n","76/75 [==============================] - ETA: 0s - loss: 265641.3125\n","Epoch 216: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 265641.3125 - val_loss: 1264279.3750\n","Epoch 217/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 279420.0625\n","Epoch 217: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 274460.7812 - val_loss: 1239565.6250\n","Epoch 218/1000\n","76/75 [==============================] - ETA: 0s - loss: 329782.1875\n","Epoch 218: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 329782.1875 - val_loss: 1506355.6250\n","Epoch 219/1000\n","76/75 [==============================] - ETA: 0s - loss: 361316.0625\n","Epoch 219: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 361316.0625 - val_loss: 1418008.8750\n","Epoch 220/1000\n","76/75 [==============================] - ETA: 0s - loss: 330932.2188\n","Epoch 220: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 330932.2188 - val_loss: 1495103.7500\n","Epoch 221/1000\n","76/75 [==============================] - ETA: 0s - loss: 309442.5000\n","Epoch 221: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 309442.5000 - val_loss: 1379394.8750\n","Epoch 222/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 320504.8750\n","Epoch 222: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 316026.8438 - val_loss: 1346934.2500\n","Epoch 223/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 304233.5000\n","Epoch 223: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 300269.8750 - val_loss: 1376330.0000\n","Epoch 224/1000\n","76/75 [==============================] - ETA: 0s - loss: 290292.4062\n","Epoch 224: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 290292.4062 - val_loss: 1313348.5000\n","Epoch 225/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 305903.1250\n","Epoch 225: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 300149.8125 - val_loss: 1354274.2500\n","Epoch 226/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 309531.1875\n","Epoch 226: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 304549.0312 - val_loss: 1402275.5000\n","Epoch 227/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 315223.8438\n","Epoch 227: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 310006.6875 - val_loss: 1445339.8750\n","Epoch 228/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 319168.6875\n","Epoch 228: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 314548.0000 - val_loss: 1485086.2500\n","Epoch 229/1000\n","76/75 [==============================] - ETA: 0s - loss: 321483.8125\n","Epoch 229: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 321483.8125 - val_loss: 1471844.7500\n","Epoch 230/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 320579.8125\n","Epoch 230: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 317449.3750 - val_loss: 1383819.6250\n","Epoch 231/1000\n","74/75 [============================>.] - ETA: 0s - loss: 324585.4375\n","Epoch 231: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 319303.4375 - val_loss: 1419746.5000\n","Epoch 232/1000\n","76/75 [==============================] - ETA: 0s - loss: 285037.8438\n","Epoch 232: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 285037.8438 - val_loss: 1329687.3750\n","Epoch 233/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 317449.3438\n","Epoch 233: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 312176.8750 - val_loss: 1430883.1250\n","Epoch 234/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 316609.0938\n","Epoch 234: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 313362.6250 - val_loss: 1358984.2500\n","Epoch 235/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 328686.5312\n","Epoch 235: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 323052.8438 - val_loss: 1438495.1250\n","Epoch 236/1000\n","76/75 [==============================] - ETA: 0s - loss: 295427.0625\n","Epoch 236: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 295427.0625 - val_loss: 1412174.1250\n","Epoch 237/1000\n","76/75 [==============================] - ETA: 0s - loss: 296600.6562\n","Epoch 237: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 296600.6562 - val_loss: 1336269.8750\n","Epoch 238/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 285973.2500\n","Epoch 238: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 279055.2500 - val_loss: 1331392.1250\n","Epoch 239/1000\n","76/75 [==============================] - ETA: 0s - loss: 280300.4062\n","Epoch 239: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 280300.4062 - val_loss: 1332153.6250\n","Epoch 240/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 302313.2500\n","Epoch 240: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 295089.6562 - val_loss: 1413555.5000\n","Epoch 241/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 293993.9688\n","Epoch 241: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 287208.6562 - val_loss: 1392727.6250\n","Epoch 242/1000\n","74/75 [============================>.] - ETA: 0s - loss: 282509.6250\n","Epoch 242: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 278493.8125 - val_loss: 1303782.3750\n","Epoch 243/1000\n","76/75 [==============================] - ETA: 0s - loss: 279017.1562\n","Epoch 243: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 279017.1562 - val_loss: 1318663.7500\n","Epoch 244/1000\n","76/75 [==============================] - ETA: 0s - loss: 275236.0000\n","Epoch 244: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 275236.0000 - val_loss: 1280455.3750\n","Epoch 245/1000\n","76/75 [==============================] - ETA: 0s - loss: 285343.5312\n","Epoch 245: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 285343.5312 - val_loss: 1326642.5000\n","Epoch 246/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 298740.9688\n","Epoch 246: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 294478.7500 - val_loss: 1348213.8750\n","Epoch 247/1000\n","76/75 [==============================] - ETA: 0s - loss: 309354.6250\n","Epoch 247: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 309354.6250 - val_loss: 1373576.0000\n","Epoch 248/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 306325.0625\n","Epoch 248: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 302100.5000 - val_loss: 1382498.5000\n","Epoch 249/1000\n","76/75 [==============================] - ETA: 0s - loss: 303497.8438\n","Epoch 249: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 303497.8438 - val_loss: 1442055.7500\n","Epoch 250/1000\n","76/75 [==============================] - ETA: 0s - loss: 291456.8438\n","Epoch 250: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 291456.8438 - val_loss: 1360001.2500\n","Epoch 251/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 292000.8125\n","Epoch 251: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 286261.0938 - val_loss: 1336828.5000\n","Epoch 252/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 282414.0938\n","Epoch 252: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 277142.7500 - val_loss: 1329921.7500\n","Epoch 253/1000\n","76/75 [==============================] - ETA: 0s - loss: 299459.5000\n","Epoch 253: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 299459.5000 - val_loss: 1333986.0000\n","Epoch 254/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 288676.6250\n","Epoch 254: val_loss did not improve from 1239537.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 283418.6250 - val_loss: 1249822.7500\n","Epoch 255/1000\n","76/75 [==============================] - ETA: 0s - loss: 265970.1875\n","Epoch 255: val_loss improved from 1239537.12500 to 1195891.75000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-2-1000-15.h5\n","75/75 [==============================] - 2s 22ms/step - loss: 265970.1875 - val_loss: 1195891.7500\n","Epoch 256/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 296464.2500\n","Epoch 256: val_loss improved from 1195891.75000 to 1167706.87500, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-2-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 290608.0312 - val_loss: 1167706.8750\n","Epoch 257/1000\n","75/75 [============================>.] - ETA: 0s - loss: 294797.8438\n","Epoch 257: val_loss did not improve from 1167706.87500\n","75/75 [==============================] - 1s 17ms/step - loss: 293751.2188 - val_loss: 1207647.6250\n","Epoch 258/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 302096.3750\n","Epoch 258: val_loss did not improve from 1167706.87500\n","75/75 [==============================] - 1s 16ms/step - loss: 295321.2812 - val_loss: 1172105.0000\n","Epoch 259/1000\n","76/75 [==============================] - ETA: 0s - loss: 292748.8438\n","Epoch 259: val_loss improved from 1167706.87500 to 1159964.50000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-2-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 292748.8438 - val_loss: 1159964.5000\n","Epoch 260/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 283873.7188\n","Epoch 260: val_loss did not improve from 1159964.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 280474.2812 - val_loss: 1174063.6250\n","Epoch 261/1000\n","76/75 [==============================] - ETA: 0s - loss: 308651.9062\n","Epoch 261: val_loss did not improve from 1159964.50000\n","75/75 [==============================] - 1s 16ms/step - loss: 308651.9062 - val_loss: 1250540.0000\n","Epoch 262/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 281961.2500\n","Epoch 262: val_loss did not improve from 1159964.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 279736.6562 - val_loss: 1282765.0000\n","Epoch 263/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 297322.4062\n","Epoch 263: val_loss improved from 1159964.50000 to 1133459.75000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-2-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 300148.5625 - val_loss: 1133459.7500\n","Epoch 264/1000\n","76/75 [==============================] - ETA: 0s - loss: 313466.9688\n","Epoch 264: val_loss improved from 1133459.75000 to 1087934.75000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-2-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 313466.9688 - val_loss: 1087934.7500\n","Epoch 265/1000\n","74/75 [============================>.] - ETA: 0s - loss: 331910.5000\n","Epoch 265: val_loss did not improve from 1087934.75000\n","75/75 [==============================] - 1s 17ms/step - loss: 331957.5312 - val_loss: 1406965.6250\n","Epoch 266/1000\n","76/75 [==============================] - ETA: 0s - loss: 352918.7500\n","Epoch 266: val_loss did not improve from 1087934.75000\n","75/75 [==============================] - 1s 17ms/step - loss: 352918.7500 - val_loss: 1244576.6250\n","Epoch 267/1000\n","76/75 [==============================] - ETA: 0s - loss: 340823.4375\n","Epoch 267: val_loss did not improve from 1087934.75000\n","75/75 [==============================] - 1s 17ms/step - loss: 340823.4375 - val_loss: 1528029.1250\n","Epoch 268/1000\n","76/75 [==============================] - ETA: 0s - loss: 376386.8438\n","Epoch 268: val_loss did not improve from 1087934.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 376386.8438 - val_loss: 1377327.3750\n","Epoch 269/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 356718.7500\n","Epoch 269: val_loss did not improve from 1087934.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 359263.7188 - val_loss: 1154912.1250\n","Epoch 270/1000\n","75/75 [============================>.] - ETA: 0s - loss: 326055.7500\n","Epoch 270: val_loss improved from 1087934.75000 to 1072258.75000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-2-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 327823.6250 - val_loss: 1072258.7500\n","Epoch 271/1000\n","75/75 [============================>.] - ETA: 0s - loss: 333555.2188\n","Epoch 271: val_loss did not improve from 1072258.75000\n","75/75 [==============================] - 1s 17ms/step - loss: 332231.9062 - val_loss: 1113575.5000\n","Epoch 272/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 294627.9688\n","Epoch 272: val_loss improved from 1072258.75000 to 1036165.18750, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-2-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 292452.1250 - val_loss: 1036165.1875\n","Epoch 273/1000\n","76/75 [==============================] - ETA: 0s - loss: 306188.5312\n","Epoch 273: val_loss improved from 1036165.18750 to 925280.31250, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-2-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 306188.5312 - val_loss: 925280.3125\n","Epoch 274/1000\n","76/75 [==============================] - ETA: 0s - loss: 296440.5000\n","Epoch 274: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 296440.5000 - val_loss: 1029844.5000\n","Epoch 275/1000\n","76/75 [==============================] - ETA: 0s - loss: 298673.5312\n","Epoch 275: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 16ms/step - loss: 298673.5312 - val_loss: 1018310.5625\n","Epoch 276/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 295675.2812\n","Epoch 276: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 295459.3750 - val_loss: 986842.5625\n","Epoch 277/1000\n","75/75 [============================>.] - ETA: 0s - loss: 291131.5000\n","Epoch 277: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 289641.4062 - val_loss: 986602.2500\n","Epoch 278/1000\n","75/75 [============================>.] - ETA: 0s - loss: 278614.9062\n","Epoch 278: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 277248.0938 - val_loss: 977361.5625\n","Epoch 279/1000\n","76/75 [==============================] - ETA: 0s - loss: 322352.4062\n","Epoch 279: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 322352.4062 - val_loss: 1255382.2500\n","Epoch 280/1000\n","75/75 [============================>.] - ETA: 0s - loss: 334271.4062\n","Epoch 280: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 333283.8750 - val_loss: 1243286.2500\n","Epoch 281/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 304778.3438\n","Epoch 281: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 16ms/step - loss: 307452.0312 - val_loss: 1168825.2500\n","Epoch 282/1000\n","76/75 [==============================] - ETA: 0s - loss: 289667.2500\n","Epoch 282: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 289667.2500 - val_loss: 1192716.2500\n","Epoch 283/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 323916.8750\n","Epoch 283: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 322840.5625 - val_loss: 1165058.8750\n","Epoch 284/1000\n","76/75 [==============================] - ETA: 0s - loss: 378659.4688\n","Epoch 284: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 378659.4688 - val_loss: 1396767.3750\n","Epoch 285/1000\n","74/75 [============================>.] - ETA: 0s - loss: 405147.3438\n","Epoch 285: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 397692.2500 - val_loss: 1222666.2500\n","Epoch 286/1000\n","74/75 [============================>.] - ETA: 0s - loss: 371307.9062\n","Epoch 286: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 365371.7812 - val_loss: 1264523.6250\n","Epoch 287/1000\n","76/75 [==============================] - ETA: 0s - loss: 369487.7188\n","Epoch 287: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 369487.7188 - val_loss: 1305439.0000\n","Epoch 288/1000\n","76/75 [==============================] - ETA: 0s - loss: 339137.4062\n","Epoch 288: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 339137.4062 - val_loss: 1141976.6250\n","Epoch 289/1000\n","74/75 [============================>.] - ETA: 0s - loss: 338337.3750\n","Epoch 289: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 332817.0000 - val_loss: 1221087.3750\n","Epoch 290/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 396234.8438\n","Epoch 290: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 393811.7812 - val_loss: 1814674.5000\n","Epoch 291/1000\n","76/75 [==============================] - ETA: 0s - loss: 448322.3125\n","Epoch 291: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 448322.3125 - val_loss: 1649682.6250\n","Epoch 292/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 391125.2812\n","Epoch 292: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 382773.7812 - val_loss: 1302692.0000\n","Epoch 293/1000\n","75/75 [============================>.] - ETA: 0s - loss: 384207.8125\n","Epoch 293: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 381921.8438 - val_loss: 1657927.8750\n","Epoch 294/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 403300.7500\n","Epoch 294: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 396604.1562 - val_loss: 1445435.0000\n","Epoch 295/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 389912.6250\n","Epoch 295: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 383136.2188 - val_loss: 1373917.5000\n","Epoch 296/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 392102.9375\n","Epoch 296: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 385265.2500 - val_loss: 1346579.0000\n","Epoch 297/1000\n","74/75 [============================>.] - ETA: 0s - loss: 349105.3125\n","Epoch 297: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 344488.0938 - val_loss: 1317463.0000\n","Epoch 298/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 350384.6250\n","Epoch 298: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 16ms/step - loss: 345495.0938 - val_loss: 1267333.2500\n","Epoch 299/1000\n","75/75 [============================>.] - ETA: 0s - loss: 340731.1562\n","Epoch 299: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 338441.0938 - val_loss: 1297575.3750\n","Epoch 300/1000\n","76/75 [==============================] - ETA: 0s - loss: 322186.3438\n","Epoch 300: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 322186.3438 - val_loss: 1232029.2500\n","Epoch 301/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 320971.9375\n","Epoch 301: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 16ms/step - loss: 316557.2188 - val_loss: 1124290.0000\n","Epoch 302/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 330334.8750\n","Epoch 302: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 16ms/step - loss: 324794.5938 - val_loss: 1101880.6250\n","Epoch 303/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 299302.4375\n","Epoch 303: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 16ms/step - loss: 296672.3125 - val_loss: 1174463.1250\n","Epoch 304/1000\n","76/75 [==============================] - ETA: 0s - loss: 309330.9688\n","Epoch 304: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 309330.9688 - val_loss: 1134901.1250\n","Epoch 305/1000\n","76/75 [==============================] - ETA: 0s - loss: 349853.0938\n","Epoch 305: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 349853.0938 - val_loss: 1272029.0000\n","Epoch 306/1000\n","75/75 [============================>.] - ETA: 0s - loss: 356573.2188\n","Epoch 306: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 353979.0000 - val_loss: 1213820.5000\n","Epoch 307/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 336799.7188\n","Epoch 307: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 16ms/step - loss: 332841.5000 - val_loss: 1140460.8750\n","Epoch 308/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 365345.4062\n","Epoch 308: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 363699.1875 - val_loss: 1193975.5000\n","Epoch 309/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 367458.2188\n","Epoch 309: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 16ms/step - loss: 360194.7812 - val_loss: 1256072.7500\n","Epoch 310/1000\n","76/75 [==============================] - ETA: 0s - loss: 351473.5938\n","Epoch 310: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 351473.5938 - val_loss: 1249038.1250\n","Epoch 311/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 379623.3125\n","Epoch 311: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 16ms/step - loss: 374872.1562 - val_loss: 1307905.8750\n","Epoch 312/1000\n","76/75 [==============================] - ETA: 0s - loss: 387179.2188\n","Epoch 312: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 387179.2188 - val_loss: 1238396.7500\n","Epoch 313/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 385197.6250\n","Epoch 313: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 379674.4688 - val_loss: 1301912.6250\n","Epoch 314/1000\n","76/75 [==============================] - ETA: 0s - loss: 372276.0938\n","Epoch 314: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 372276.0938 - val_loss: 1271652.0000\n","Epoch 315/1000\n","75/75 [============================>.] - ETA: 0s - loss: 369284.6250\n","Epoch 315: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 366614.8125 - val_loss: 1134394.8750\n","Epoch 316/1000\n","76/75 [==============================] - ETA: 0s - loss: 361485.7500\n","Epoch 316: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 361485.7500 - val_loss: 1184496.6250\n","Epoch 317/1000\n","76/75 [==============================] - ETA: 0s - loss: 362065.0312\n","Epoch 317: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 362065.0312 - val_loss: 1063853.1250\n","Epoch 318/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 348542.0312\n","Epoch 318: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 348787.8438 - val_loss: 1062275.1250\n","Epoch 319/1000\n","75/75 [============================>.] - ETA: 0s - loss: 352664.0938\n","Epoch 319: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 350259.3438 - val_loss: 1067269.1250\n","Epoch 320/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 395187.7188\n","Epoch 320: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 16ms/step - loss: 391882.1562 - val_loss: 1087304.1250\n","Epoch 321/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 363683.0625\n","Epoch 321: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 361458.0938 - val_loss: 1289408.5000\n","Epoch 322/1000\n","76/75 [==============================] - ETA: 0s - loss: 352436.0312\n","Epoch 322: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 352436.0312 - val_loss: 1249310.2500\n","Epoch 323/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 366984.8438\n","Epoch 323: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 360379.6875 - val_loss: 1212475.0000\n","Epoch 324/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 400946.3438\n","Epoch 324: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 399121.6250 - val_loss: 1089312.1250\n","Epoch 325/1000\n","74/75 [============================>.] - ETA: 0s - loss: 399590.5000\n","Epoch 325: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 18ms/step - loss: 395526.5000 - val_loss: 1143695.5000\n","Epoch 326/1000\n","75/75 [============================>.] - ETA: 0s - loss: 391106.6562\n","Epoch 326: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 20ms/step - loss: 390680.0625 - val_loss: 1204985.7500\n","Epoch 327/1000\n","76/75 [==============================] - ETA: 0s - loss: 412693.3750\n","Epoch 327: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 19ms/step - loss: 412693.3750 - val_loss: 1096746.7500\n","Epoch 328/1000\n","74/75 [============================>.] - ETA: 0s - loss: 375635.2812\n","Epoch 328: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 374523.0312 - val_loss: 1011747.2500\n","Epoch 329/1000\n","76/75 [==============================] - ETA: 0s - loss: 374821.4375\n","Epoch 329: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 374821.4375 - val_loss: 1024039.8750\n","Epoch 330/1000\n","76/75 [==============================] - ETA: 0s - loss: 382984.9688\n","Epoch 330: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 382984.9688 - val_loss: 991324.1875\n","Epoch 331/1000\n","75/75 [============================>.] - ETA: 0s - loss: 389841.6875\n","Epoch 331: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 387877.1562 - val_loss: 1119690.6250\n","Epoch 332/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 390986.5938\n","Epoch 332: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 390467.6250 - val_loss: 1055814.2500\n","Epoch 333/1000\n","76/75 [==============================] - ETA: 0s - loss: 361856.2500\n","Epoch 333: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 16ms/step - loss: 361856.2500 - val_loss: 1039654.3125\n","Epoch 334/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 367914.1562\n","Epoch 334: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 16ms/step - loss: 364271.8438 - val_loss: 1087759.1250\n","Epoch 335/1000\n","76/75 [==============================] - ETA: 0s - loss: 353499.4062\n","Epoch 335: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 353499.4062 - val_loss: 1037001.8125\n","Epoch 336/1000\n","76/75 [==============================] - ETA: 0s - loss: 366286.7812\n","Epoch 336: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 366286.7812 - val_loss: 1106126.7500\n","Epoch 337/1000\n","76/75 [==============================] - ETA: 0s - loss: 383724.0938\n","Epoch 337: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 383724.0938 - val_loss: 976728.0000\n","Epoch 338/1000\n","76/75 [==============================] - ETA: 0s - loss: 373588.9688\n","Epoch 338: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 373588.9688 - val_loss: 1066478.7500\n","Epoch 339/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 360161.2188\n","Epoch 339: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 357003.2500 - val_loss: 1068854.2500\n","Epoch 340/1000\n","74/75 [============================>.] - ETA: 0s - loss: 385091.5938\n","Epoch 340: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 379503.9062 - val_loss: 1045420.1875\n","Epoch 341/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 370247.0000\n","Epoch 341: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 16ms/step - loss: 368567.6875 - val_loss: 1028191.0000\n","Epoch 342/1000\n","76/75 [==============================] - ETA: 0s - loss: 360030.6875\n","Epoch 342: val_loss did not improve from 925280.31250\n","75/75 [==============================] - 1s 17ms/step - loss: 360030.6875 - val_loss: 967999.6875\n","Epoch 343/1000\n","76/75 [==============================] - ETA: 0s - loss: 347779.1562\n","Epoch 343: val_loss improved from 925280.31250 to 906662.62500, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-2-1000-15.h5\n","75/75 [==============================] - 2s 22ms/step - loss: 347779.1562 - val_loss: 906662.6250\n","Epoch 344/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 346051.5312\n","Epoch 344: val_loss did not improve from 906662.62500\n","75/75 [==============================] - 1s 17ms/step - loss: 344664.8125 - val_loss: 974840.8125\n","Epoch 345/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 367877.6875\n","Epoch 345: val_loss did not improve from 906662.62500\n","75/75 [==============================] - 1s 17ms/step - loss: 363998.9375 - val_loss: 1030579.5000\n","Epoch 346/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 357355.0312\n","Epoch 346: val_loss did not improve from 906662.62500\n","75/75 [==============================] - 1s 16ms/step - loss: 354707.1250 - val_loss: 1021077.0625\n","Epoch 347/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 370527.4375\n","Epoch 347: val_loss did not improve from 906662.62500\n","75/75 [==============================] - 1s 16ms/step - loss: 372254.8750 - val_loss: 908082.1875\n","Epoch 348/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 349791.5938\n","Epoch 348: val_loss did not improve from 906662.62500\n","75/75 [==============================] - 1s 16ms/step - loss: 346496.2188 - val_loss: 961952.2500\n","Epoch 349/1000\n","76/75 [==============================] - ETA: 0s - loss: 346997.5938\n","Epoch 349: val_loss did not improve from 906662.62500\n","75/75 [==============================] - 1s 17ms/step - loss: 346997.5938 - val_loss: 976768.3125\n","Epoch 350/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 349254.5938\n","Epoch 350: val_loss improved from 906662.62500 to 901195.43750, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-2-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 346020.0625 - val_loss: 901195.4375\n","Epoch 351/1000\n","75/75 [============================>.] - ETA: 0s - loss: 326846.5625\n","Epoch 351: val_loss did not improve from 901195.43750\n","75/75 [==============================] - 1s 17ms/step - loss: 324870.5625 - val_loss: 925775.3125\n","Epoch 352/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 351034.8125\n","Epoch 352: val_loss did not improve from 901195.43750\n","75/75 [==============================] - 1s 16ms/step - loss: 347872.0000 - val_loss: 932372.6250\n","Epoch 353/1000\n","76/75 [==============================] - ETA: 0s - loss: 338169.1250\n","Epoch 353: val_loss did not improve from 901195.43750\n","75/75 [==============================] - 1s 17ms/step - loss: 338169.1250 - val_loss: 982670.5625\n","Epoch 354/1000\n","76/75 [==============================] - ETA: 0s - loss: 338781.6875\n","Epoch 354: val_loss did not improve from 901195.43750\n","75/75 [==============================] - 1s 17ms/step - loss: 338781.6875 - val_loss: 948524.7500\n","Epoch 355/1000\n","75/75 [============================>.] - ETA: 0s - loss: 340201.3438\n","Epoch 355: val_loss improved from 901195.43750 to 866144.12500, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-2-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 340505.2188 - val_loss: 866144.1250\n","Epoch 356/1000\n","75/75 [============================>.] - ETA: 0s - loss: 364904.8125\n","Epoch 356: val_loss did not improve from 866144.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 366379.0312 - val_loss: 970723.3750\n","Epoch 357/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 346350.0938\n","Epoch 357: val_loss did not improve from 866144.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 343852.9062 - val_loss: 925316.6875\n","Epoch 358/1000\n","76/75 [==============================] - ETA: 0s - loss: 347707.5000\n","Epoch 358: val_loss improved from 866144.12500 to 848299.50000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-2-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 347707.5000 - val_loss: 848299.5000\n","Epoch 359/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 361335.9375\n","Epoch 359: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 357639.3438 - val_loss: 942794.5000\n","Epoch 360/1000\n","76/75 [==============================] - ETA: 0s - loss: 330447.0938\n","Epoch 360: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 330447.0938 - val_loss: 1029204.0625\n","Epoch 361/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 340550.2500\n","Epoch 361: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 336793.5000 - val_loss: 1068674.0000\n","Epoch 362/1000\n","76/75 [==============================] - ETA: 0s - loss: 317509.6250\n","Epoch 362: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 317509.6250 - val_loss: 1041125.4375\n","Epoch 363/1000\n","75/75 [============================>.] - ETA: 0s - loss: 365299.2500\n","Epoch 363: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 364529.4062 - val_loss: 1134746.0000\n","Epoch 364/1000\n","76/75 [==============================] - ETA: 0s - loss: 332647.1875\n","Epoch 364: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 332647.1875 - val_loss: 1041896.3125\n","Epoch 365/1000\n","76/75 [==============================] - ETA: 0s - loss: 360548.2500\n","Epoch 365: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 360548.2500 - val_loss: 1041664.0000\n","Epoch 366/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 302882.7188\n","Epoch 366: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 16ms/step - loss: 300524.7812 - val_loss: 1021232.6875\n","Epoch 367/1000\n","76/75 [==============================] - ETA: 0s - loss: 304288.6250\n","Epoch 367: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 304288.6250 - val_loss: 929172.0000\n","Epoch 368/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 297920.1250\n","Epoch 368: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 296730.9688 - val_loss: 956429.3125\n","Epoch 369/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 270119.1562\n","Epoch 369: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 273237.2188 - val_loss: 937721.4375\n","Epoch 370/1000\n","76/75 [==============================] - ETA: 0s - loss: 279811.4375\n","Epoch 370: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 279811.4375 - val_loss: 1026264.0000\n","Epoch 371/1000\n","75/75 [============================>.] - ETA: 0s - loss: 276322.7188\n","Epoch 371: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 274712.9375 - val_loss: 921048.5000\n","Epoch 372/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 272348.0312\n","Epoch 372: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 16ms/step - loss: 275300.6562 - val_loss: 1014440.0000\n","Epoch 373/1000\n","76/75 [==============================] - ETA: 0s - loss: 292686.9062\n","Epoch 373: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 292686.9062 - val_loss: 1084276.6250\n","Epoch 374/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 290725.3438\n","Epoch 374: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 16ms/step - loss: 290098.0312 - val_loss: 996039.1250\n","Epoch 375/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 260200.0000\n","Epoch 375: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 258259.5156 - val_loss: 1010672.8750\n","Epoch 376/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 262591.0000\n","Epoch 376: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 16ms/step - loss: 262446.2812 - val_loss: 975965.0625\n","Epoch 377/1000\n","75/75 [============================>.] - ETA: 0s - loss: 293801.3438\n","Epoch 377: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 292118.3750 - val_loss: 927788.3750\n","Epoch 378/1000\n","75/75 [============================>.] - ETA: 0s - loss: 298168.3438\n","Epoch 378: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 296590.4062 - val_loss: 906170.0000\n","Epoch 379/1000\n","76/75 [==============================] - ETA: 0s - loss: 293907.5312\n","Epoch 379: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 293907.5312 - val_loss: 863976.4375\n","Epoch 380/1000\n","76/75 [==============================] - ETA: 0s - loss: 278601.6562\n","Epoch 380: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 278601.6562 - val_loss: 851130.1250\n","Epoch 381/1000\n","76/75 [==============================] - ETA: 0s - loss: 283445.5938\n","Epoch 381: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 283445.5938 - val_loss: 886984.0625\n","Epoch 382/1000\n","76/75 [==============================] - ETA: 0s - loss: 276026.8438\n","Epoch 382: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 276026.8438 - val_loss: 993256.4375\n","Epoch 383/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 278945.6250\n","Epoch 383: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 284496.5625 - val_loss: 1027803.9375\n","Epoch 384/1000\n","76/75 [==============================] - ETA: 0s - loss: 383315.8125\n","Epoch 384: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 383315.8125 - val_loss: 1167155.3750\n","Epoch 385/1000\n","76/75 [==============================] - ETA: 0s - loss: 293549.4062\n","Epoch 385: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 293549.4062 - val_loss: 1145829.8750\n","Epoch 386/1000\n","76/75 [==============================] - ETA: 0s - loss: 316986.4375\n","Epoch 386: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 316986.4375 - val_loss: 1236630.0000\n","Epoch 387/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 290828.1250\n","Epoch 387: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 286520.6562 - val_loss: 1084790.1250\n","Epoch 388/1000\n","75/75 [============================>.] - ETA: 0s - loss: 329103.3438\n","Epoch 388: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 327232.4375 - val_loss: 1069798.7500\n","Epoch 389/1000\n","76/75 [==============================] - ETA: 0s - loss: 337280.4062\n","Epoch 389: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 337280.4062 - val_loss: 1029829.4375\n","Epoch 390/1000\n","76/75 [==============================] - ETA: 0s - loss: 292421.0625\n","Epoch 390: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 292421.0625 - val_loss: 975556.9375\n","Epoch 391/1000\n","76/75 [==============================] - ETA: 0s - loss: 276360.1875\n","Epoch 391: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 276360.1875 - val_loss: 960792.8750\n","Epoch 392/1000\n","76/75 [==============================] - ETA: 0s - loss: 291253.2500\n","Epoch 392: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 291253.2500 - val_loss: 1323456.7500\n","Epoch 393/1000\n","76/75 [==============================] - ETA: 0s - loss: 359285.7812\n","Epoch 393: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 359285.7812 - val_loss: 1930346.6250\n","Epoch 394/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 366284.7188\n","Epoch 394: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 16ms/step - loss: 359424.2500 - val_loss: 2174012.0000\n","Epoch 395/1000\n","74/75 [============================>.] - ETA: 0s - loss: 379674.9688\n","Epoch 395: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 373444.4062 - val_loss: 2239482.0000\n","Epoch 396/1000\n","75/75 [============================>.] - ETA: 0s - loss: 319572.4375\n","Epoch 396: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 317745.8750 - val_loss: 1964284.1250\n","Epoch 397/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 339293.0000\n","Epoch 397: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 16ms/step - loss: 331712.5000 - val_loss: 1950514.7500\n","Epoch 398/1000\n","76/75 [==============================] - ETA: 0s - loss: 333701.6562\n","Epoch 398: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 333701.6562 - val_loss: 1951873.6250\n","Epoch 399/1000\n","75/75 [============================>.] - ETA: 0s - loss: 348441.3438\n","Epoch 399: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 346687.3438 - val_loss: 2083211.2500\n","Epoch 400/1000\n","75/75 [============================>.] - ETA: 0s - loss: 360810.4062\n","Epoch 400: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 358951.6562 - val_loss: 1772863.8750\n","Epoch 401/1000\n","76/75 [==============================] - ETA: 0s - loss: 333012.7188\n","Epoch 401: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 333012.7188 - val_loss: 1752498.2500\n","Epoch 402/1000\n","75/75 [============================>.] - ETA: 0s - loss: 322469.3438\n","Epoch 402: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 320971.3438 - val_loss: 1271742.0000\n","Epoch 403/1000\n","75/75 [============================>.] - ETA: 0s - loss: 309744.5938\n","Epoch 403: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 307515.6250 - val_loss: 1347814.3750\n","Epoch 404/1000\n","76/75 [==============================] - ETA: 0s - loss: 322517.1875\n","Epoch 404: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 322517.1875 - val_loss: 1248751.2500\n","Epoch 405/1000\n","76/75 [==============================] - ETA: 0s - loss: 338166.5000\n","Epoch 405: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 338166.5000 - val_loss: 1310534.0000\n","Epoch 406/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 335239.5938\n","Epoch 406: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 16ms/step - loss: 328578.8125 - val_loss: 1258966.2500\n","Epoch 407/1000\n","74/75 [============================>.] - ETA: 0s - loss: 325981.6250\n","Epoch 407: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 320818.9062 - val_loss: 1249791.0000\n","Epoch 408/1000\n","75/75 [============================>.] - ETA: 0s - loss: 321690.0625\n","Epoch 408: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 320105.1250 - val_loss: 1869572.1250\n","Epoch 409/1000\n","76/75 [==============================] - ETA: 0s - loss: 364674.7188\n","Epoch 409: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 16ms/step - loss: 364674.7188 - val_loss: 1904371.8750\n","Epoch 410/1000\n","74/75 [============================>.] - ETA: 0s - loss: 326028.1875\n","Epoch 410: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 320812.4062 - val_loss: 1843261.1250\n","Epoch 411/1000\n","76/75 [==============================] - ETA: 0s - loss: 331854.5000\n","Epoch 411: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 331854.5000 - val_loss: 1975814.5000\n","Epoch 412/1000\n","76/75 [==============================] - ETA: 0s - loss: 330903.9375\n","Epoch 412: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 330903.9375 - val_loss: 1954386.5000\n","Epoch 413/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 322918.6250\n","Epoch 413: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 314446.7500 - val_loss: 1866339.7500\n","Epoch 414/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 321741.1562\n","Epoch 414: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 16ms/step - loss: 313559.5000 - val_loss: 1845859.1250\n","Epoch 415/1000\n","75/75 [============================>.] - ETA: 0s - loss: 318036.9375\n","Epoch 415: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 316314.2500 - val_loss: 1763760.8750\n","Epoch 416/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 323114.5000\n","Epoch 416: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 314605.9688 - val_loss: 1631927.1250\n","Epoch 417/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 319536.6562\n","Epoch 417: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 311940.7500 - val_loss: 1615228.0000\n","Epoch 418/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 342143.5625\n","Epoch 418: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 334172.5625 - val_loss: 1777832.1250\n","Epoch 419/1000\n","76/75 [==============================] - ETA: 0s - loss: 346741.4688\n","Epoch 419: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 346741.4688 - val_loss: 1102387.7500\n","Epoch 420/1000\n","76/75 [==============================] - ETA: 0s - loss: 338350.7812\n","Epoch 420: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 338350.7812 - val_loss: 1324073.0000\n","Epoch 421/1000\n","76/75 [==============================] - ETA: 0s - loss: 307081.7500\n","Epoch 421: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 307081.7500 - val_loss: 1154915.8750\n","Epoch 422/1000\n","76/75 [==============================] - ETA: 0s - loss: 302827.9688\n","Epoch 422: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 302827.9688 - val_loss: 1623819.2500\n","Epoch 423/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 332611.0938\n","Epoch 423: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 325622.3438 - val_loss: 1631444.3750\n","Epoch 424/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 326135.8750\n","Epoch 424: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 16ms/step - loss: 320165.0000 - val_loss: 1519312.7500\n","Epoch 425/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 323099.7500\n","Epoch 425: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 16ms/step - loss: 318767.9688 - val_loss: 1541388.2500\n","Epoch 426/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 334436.3125\n","Epoch 426: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 16ms/step - loss: 327188.4062 - val_loss: 1507918.2500\n","Epoch 427/1000\n","76/75 [==============================] - ETA: 0s - loss: 316993.4375\n","Epoch 427: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 316993.4375 - val_loss: 1556617.7500\n","Epoch 428/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 318884.5625\n","Epoch 428: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 312581.7188 - val_loss: 1489260.2500\n","Epoch 429/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 302069.8125\n","Epoch 429: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 293918.7188 - val_loss: 1320774.0000\n","Epoch 430/1000\n","76/75 [==============================] - ETA: 0s - loss: 290492.5625\n","Epoch 430: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 290492.5625 - val_loss: 1273351.2500\n","Epoch 431/1000\n","76/75 [==============================] - ETA: 0s - loss: 311510.1250\n","Epoch 431: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 311510.1250 - val_loss: 1391719.3750\n","Epoch 432/1000\n","75/75 [============================>.] - ETA: 0s - loss: 308278.8125\n","Epoch 432: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 306525.3125 - val_loss: 1303044.6250\n","Epoch 433/1000\n","76/75 [==============================] - ETA: 0s - loss: 310594.2812\n","Epoch 433: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 310594.2812 - val_loss: 1324600.5000\n","Epoch 434/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 305685.1875\n","Epoch 434: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 299486.3438 - val_loss: 1551270.0000\n","Epoch 435/1000\n","74/75 [============================>.] - ETA: 0s - loss: 305111.8438\n","Epoch 435: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 300106.1250 - val_loss: 1561509.0000\n","Epoch 436/1000\n","76/75 [==============================] - ETA: 0s - loss: 281696.0625\n","Epoch 436: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 281696.0625 - val_loss: 1765338.5000\n","Epoch 437/1000\n","76/75 [==============================] - ETA: 0s - loss: 289624.1250\n","Epoch 437: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 289624.1250 - val_loss: 1658265.7500\n","Epoch 438/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 305295.8125\n","Epoch 438: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 298544.6875 - val_loss: 1647342.2500\n","Epoch 439/1000\n","76/75 [==============================] - ETA: 0s - loss: 304232.6250\n","Epoch 439: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 304232.6250 - val_loss: 1633157.1250\n","Epoch 440/1000\n","76/75 [==============================] - ETA: 0s - loss: 295937.3125\n","Epoch 440: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 295937.3125 - val_loss: 1683440.2500\n","Epoch 441/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 318696.3438\n","Epoch 441: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 311572.3125 - val_loss: 1492806.7500\n","Epoch 442/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 312044.5625\n","Epoch 442: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 305917.6875 - val_loss: 1429051.1250\n","Epoch 443/1000\n","75/75 [============================>.] - ETA: 0s - loss: 280052.4688\n","Epoch 443: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 278824.1875 - val_loss: 1395259.7500\n","Epoch 444/1000\n","76/75 [==============================] - ETA: 0s - loss: 300610.9062\n","Epoch 444: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 300610.9062 - val_loss: 1079199.3750\n","Epoch 445/1000\n","76/75 [==============================] - ETA: 0s - loss: 320311.8438\n","Epoch 445: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 320311.8438 - val_loss: 1226810.3750\n","Epoch 446/1000\n","76/75 [==============================] - ETA: 0s - loss: 329748.0938\n","Epoch 446: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 329748.0938 - val_loss: 1443585.5000\n","Epoch 447/1000\n","74/75 [============================>.] - ETA: 0s - loss: 320749.8438\n","Epoch 447: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 18ms/step - loss: 315548.8125 - val_loss: 1187626.1250\n","Epoch 448/1000\n","75/75 [============================>.] - ETA: 0s - loss: 318973.6562\n","Epoch 448: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 20ms/step - loss: 316616.0000 - val_loss: 1112529.2500\n","Epoch 449/1000\n","74/75 [============================>.] - ETA: 0s - loss: 299081.5312\n","Epoch 449: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 19ms/step - loss: 293771.3750 - val_loss: 1040744.6875\n","Epoch 450/1000\n","76/75 [==============================] - ETA: 0s - loss: 329021.1250\n","Epoch 450: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 329021.1250 - val_loss: 1302009.2500\n","Epoch 451/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 338109.5312\n","Epoch 451: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 330500.5625 - val_loss: 1076883.3750\n","Epoch 452/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 308828.2812\n","Epoch 452: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 304827.3750 - val_loss: 1015076.7500\n","Epoch 453/1000\n","76/75 [==============================] - ETA: 0s - loss: 279901.1875\n","Epoch 453: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 279901.1875 - val_loss: 892480.3125\n","Epoch 454/1000\n","75/75 [============================>.] - ETA: 0s - loss: 285043.3750\n","Epoch 454: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 286103.5312 - val_loss: 1018815.8750\n","Epoch 455/1000\n","75/75 [============================>.] - ETA: 0s - loss: 300348.0625\n","Epoch 455: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 298149.4375 - val_loss: 981071.5625\n","Epoch 456/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 296440.1250\n","Epoch 456: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 293316.4688 - val_loss: 1025476.5000\n","Epoch 457/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 280461.0000\n","Epoch 457: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 278656.0000 - val_loss: 979082.1250\n","Epoch 458/1000\n","76/75 [==============================] - ETA: 0s - loss: 286784.0625\n","Epoch 458: val_loss did not improve from 848299.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 286784.0625 - val_loss: 958006.0625\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1447/1447 [00:00<00:00, 23220.47it/s]\n","100%|██████████| 1419/1419 [00:00<00:00, 22640.69it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 3409542.2500\n","Epoch 1: val_loss improved from inf to 1000997.06250, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-3-1000-15.h5\n","75/75 [==============================] - 6s 24ms/step - loss: 3302406.2500 - val_loss: 1000997.0625\n","Epoch 2/1000\n","76/75 [==============================] - ETA: 0s - loss: 343335.0625\n","Epoch 2: val_loss did not improve from 1000997.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 343335.0625 - val_loss: 2049624.6250\n","Epoch 3/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 362299.4375\n","Epoch 3: val_loss did not improve from 1000997.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 355314.8125 - val_loss: 1885296.6250\n","Epoch 4/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 341962.0625\n","Epoch 4: val_loss did not improve from 1000997.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 336003.0000 - val_loss: 1675027.0000\n","Epoch 5/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 342002.8125\n","Epoch 5: val_loss did not improve from 1000997.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 335211.0312 - val_loss: 1552300.8750\n","Epoch 6/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 335281.3438\n","Epoch 6: val_loss did not improve from 1000997.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 328562.8750 - val_loss: 1649785.7500\n","Epoch 7/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 333276.0938\n","Epoch 7: val_loss did not improve from 1000997.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 326498.3125 - val_loss: 1550608.1250\n","Epoch 8/1000\n","76/75 [==============================] - ETA: 0s - loss: 311455.6875\n","Epoch 8: val_loss did not improve from 1000997.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 311455.6875 - val_loss: 1463314.7500\n","Epoch 9/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 330134.1562\n","Epoch 9: val_loss did not improve from 1000997.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 323737.5938 - val_loss: 1485600.5000\n","Epoch 10/1000\n","76/75 [==============================] - ETA: 0s - loss: 295142.9688\n","Epoch 10: val_loss did not improve from 1000997.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 295142.9688 - val_loss: 1238593.7500\n","Epoch 11/1000\n","76/75 [==============================] - ETA: 0s - loss: 317169.7188\n","Epoch 11: val_loss did not improve from 1000997.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 317169.7188 - val_loss: 1608722.3750\n","Epoch 12/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 309182.1875\n","Epoch 12: val_loss did not improve from 1000997.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 304604.9688 - val_loss: 1496593.1250\n","Epoch 13/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 247750.6094\n","Epoch 13: val_loss did not improve from 1000997.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 244297.7500 - val_loss: 1152289.2500\n","Epoch 14/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 260462.6719\n","Epoch 14: val_loss did not improve from 1000997.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 257491.3906 - val_loss: 1105442.2500\n","Epoch 15/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 267814.5312\n","Epoch 15: val_loss did not improve from 1000997.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 263699.2812 - val_loss: 1154413.6250\n","Epoch 16/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 239376.7500\n","Epoch 16: val_loss improved from 1000997.06250 to 942262.00000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-3-1000-15.h5\n","75/75 [==============================] - 2s 22ms/step - loss: 236498.0469 - val_loss: 942262.0000\n","Epoch 17/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 243648.0156\n","Epoch 17: val_loss improved from 942262.00000 to 888841.31250, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-3-1000-15.h5\n","75/75 [==============================] - 1s 17ms/step - loss: 241328.9375 - val_loss: 888841.3125\n","Epoch 18/1000\n","76/75 [==============================] - ETA: 0s - loss: 224709.2656\n","Epoch 18: val_loss did not improve from 888841.31250\n","75/75 [==============================] - 1s 16ms/step - loss: 224709.2656 - val_loss: 1012423.4375\n","Epoch 19/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 249895.8594\n","Epoch 19: val_loss did not improve from 888841.31250\n","75/75 [==============================] - 1s 16ms/step - loss: 247963.8281 - val_loss: 1002068.9375\n","Epoch 20/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 216085.5000\n","Epoch 20: val_loss did not improve from 888841.31250\n","75/75 [==============================] - 1s 16ms/step - loss: 215871.6094 - val_loss: 889847.0000\n","Epoch 21/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 208966.3750\n","Epoch 21: val_loss did not improve from 888841.31250\n","75/75 [==============================] - 1s 16ms/step - loss: 208787.1406 - val_loss: 890113.1875\n","Epoch 22/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 189894.6875\n","Epoch 22: val_loss did not improve from 888841.31250\n","75/75 [==============================] - 1s 16ms/step - loss: 188210.5469 - val_loss: 1252044.3750\n","Epoch 23/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 181039.0312\n","Epoch 23: val_loss did not improve from 888841.31250\n","75/75 [==============================] - 1s 16ms/step - loss: 180125.0938 - val_loss: 1235986.8750\n","Epoch 24/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 185218.1250\n","Epoch 24: val_loss did not improve from 888841.31250\n","75/75 [==============================] - 1s 16ms/step - loss: 183776.2656 - val_loss: 1183704.8750\n","Epoch 25/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 179964.3438\n","Epoch 25: val_loss did not improve from 888841.31250\n","75/75 [==============================] - 1s 16ms/step - loss: 177784.5000 - val_loss: 1287092.0000\n","Epoch 26/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 184385.5781\n","Epoch 26: val_loss did not improve from 888841.31250\n","75/75 [==============================] - 1s 16ms/step - loss: 183854.4062 - val_loss: 1320983.2500\n","Epoch 27/1000\n","76/75 [==============================] - ETA: 0s - loss: 172636.4062\n","Epoch 27: val_loss did not improve from 888841.31250\n","75/75 [==============================] - 1s 16ms/step - loss: 172636.4062 - val_loss: 1294370.1250\n","Epoch 28/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 171958.4375\n","Epoch 28: val_loss did not improve from 888841.31250\n","75/75 [==============================] - 1s 16ms/step - loss: 171652.0938 - val_loss: 1330601.7500\n","Epoch 29/1000\n","76/75 [==============================] - ETA: 0s - loss: 180177.1250\n","Epoch 29: val_loss did not improve from 888841.31250\n","75/75 [==============================] - 1s 16ms/step - loss: 180177.1250 - val_loss: 1265246.1250\n","Epoch 30/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 173882.4531\n","Epoch 30: val_loss did not improve from 888841.31250\n","75/75 [==============================] - 1s 16ms/step - loss: 174971.9375 - val_loss: 973251.8125\n","Epoch 31/1000\n","76/75 [==============================] - ETA: 0s - loss: 199176.1250\n","Epoch 31: val_loss did not improve from 888841.31250\n","75/75 [==============================] - 1s 16ms/step - loss: 199176.1250 - val_loss: 919307.6250\n","Epoch 32/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 201953.9062\n","Epoch 32: val_loss improved from 888841.31250 to 737314.43750, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-3-1000-15.h5\n","75/75 [==============================] - 2s 21ms/step - loss: 207430.4219 - val_loss: 737314.4375\n","Epoch 33/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 203298.3594\n","Epoch 33: val_loss did not improve from 737314.43750\n","75/75 [==============================] - 1s 16ms/step - loss: 203909.5625 - val_loss: 829493.5000\n","Epoch 34/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 177439.6250\n","Epoch 34: val_loss did not improve from 737314.43750\n","75/75 [==============================] - 1s 16ms/step - loss: 180042.7031 - val_loss: 899832.1250\n","Epoch 35/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 167873.6875\n","Epoch 35: val_loss did not improve from 737314.43750\n","75/75 [==============================] - 1s 16ms/step - loss: 169263.8906 - val_loss: 897175.1250\n","Epoch 36/1000\n","76/75 [==============================] - ETA: 0s - loss: 150921.6875\n","Epoch 36: val_loss did not improve from 737314.43750\n","75/75 [==============================] - 1s 16ms/step - loss: 150921.6875 - val_loss: 1221559.8750\n","Epoch 37/1000\n","74/75 [============================>.] - ETA: 0s - loss: 152489.2188\n","Epoch 37: val_loss did not improve from 737314.43750\n","75/75 [==============================] - 1s 17ms/step - loss: 150651.1250 - val_loss: 1672295.3750\n","Epoch 38/1000\n","76/75 [==============================] - ETA: 0s - loss: 172844.2500\n","Epoch 38: val_loss did not improve from 737314.43750\n","75/75 [==============================] - 1s 17ms/step - loss: 172844.2500 - val_loss: 1902868.7500\n","Epoch 39/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 164359.1406\n","Epoch 39: val_loss did not improve from 737314.43750\n","75/75 [==============================] - 1s 17ms/step - loss: 163859.1406 - val_loss: 2005105.7500\n","Epoch 40/1000\n","76/75 [==============================] - ETA: 0s - loss: 166563.5469\n","Epoch 40: val_loss did not improve from 737314.43750\n","75/75 [==============================] - 1s 17ms/step - loss: 166563.5469 - val_loss: 2025706.3750\n","Epoch 41/1000\n","76/75 [==============================] - ETA: 0s - loss: 218271.8125\n","Epoch 41: val_loss did not improve from 737314.43750\n","75/75 [==============================] - 1s 16ms/step - loss: 218271.8125 - val_loss: 2331834.2500\n","Epoch 42/1000\n","76/75 [==============================] - ETA: 0s - loss: 196576.0156\n","Epoch 42: val_loss did not improve from 737314.43750\n","75/75 [==============================] - 1s 17ms/step - loss: 196576.0156 - val_loss: 1801759.3750\n","Epoch 43/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 168422.3750\n","Epoch 43: val_loss did not improve from 737314.43750\n","75/75 [==============================] - 1s 17ms/step - loss: 166483.8125 - val_loss: 1488067.3750\n","Epoch 44/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 187373.9531\n","Epoch 44: val_loss did not improve from 737314.43750\n","75/75 [==============================] - 1s 17ms/step - loss: 185643.1562 - val_loss: 1528372.8750\n","Epoch 45/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 182357.0781\n","Epoch 45: val_loss did not improve from 737314.43750\n","75/75 [==============================] - 1s 17ms/step - loss: 181334.6875 - val_loss: 1289511.7500\n","Epoch 46/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 160874.0312\n","Epoch 46: val_loss did not improve from 737314.43750\n","75/75 [==============================] - 1s 16ms/step - loss: 167439.4062 - val_loss: 1081021.8750\n","Epoch 47/1000\n","76/75 [==============================] - ETA: 0s - loss: 227726.4375\n","Epoch 47: val_loss did not improve from 737314.43750\n","75/75 [==============================] - 1s 16ms/step - loss: 227726.4375 - val_loss: 906330.6250\n","Epoch 48/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 234295.0938\n","Epoch 48: val_loss did not improve from 737314.43750\n","75/75 [==============================] - 1s 16ms/step - loss: 233458.8125 - val_loss: 802576.5625\n","Epoch 49/1000\n","76/75 [==============================] - ETA: 0s - loss: 196205.5312\n","Epoch 49: val_loss improved from 737314.43750 to 709769.37500, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-3-1000-15.h5\n","75/75 [==============================] - 2s 22ms/step - loss: 196205.5312 - val_loss: 709769.3750\n","Epoch 50/1000\n","76/75 [==============================] - ETA: 0s - loss: 163678.2500\n","Epoch 50: val_loss did not improve from 709769.37500\n","75/75 [==============================] - 1s 17ms/step - loss: 163678.2500 - val_loss: 731591.9375\n","Epoch 51/1000\n","76/75 [==============================] - ETA: 0s - loss: 143366.5781\n","Epoch 51: val_loss did not improve from 709769.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 143366.5781 - val_loss: 773238.3750\n","Epoch 52/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 138552.6875\n","Epoch 52: val_loss did not improve from 709769.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 139701.5469 - val_loss: 854801.0000\n","Epoch 53/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 134357.3438\n","Epoch 53: val_loss did not improve from 709769.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 135353.8906 - val_loss: 964516.6250\n","Epoch 54/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 142141.6875\n","Epoch 54: val_loss did not improve from 709769.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 141173.9219 - val_loss: 1199662.3750\n","Epoch 55/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 142078.1875\n","Epoch 55: val_loss did not improve from 709769.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 140727.0312 - val_loss: 1348661.7500\n","Epoch 56/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 135095.5781\n","Epoch 56: val_loss did not improve from 709769.37500\n","75/75 [==============================] - 1s 17ms/step - loss: 134890.3125 - val_loss: 1281473.1250\n","Epoch 57/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 151194.1719\n","Epoch 57: val_loss did not improve from 709769.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 150109.1094 - val_loss: 1190532.5000\n","Epoch 58/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 164570.4688\n","Epoch 58: val_loss did not improve from 709769.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 163378.8594 - val_loss: 1297035.5000\n","Epoch 59/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 147330.7031\n","Epoch 59: val_loss did not improve from 709769.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 147197.1094 - val_loss: 1171908.8750\n","Epoch 60/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 152819.5000\n","Epoch 60: val_loss did not improve from 709769.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 152275.4062 - val_loss: 1069238.2500\n","Epoch 61/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 146331.0312\n","Epoch 61: val_loss did not improve from 709769.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 146312.9375 - val_loss: 1023050.5000\n","Epoch 62/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 152001.0312\n","Epoch 62: val_loss did not improve from 709769.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 151836.0312 - val_loss: 988468.7500\n","Epoch 63/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 148141.1875\n","Epoch 63: val_loss did not improve from 709769.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 147788.1562 - val_loss: 1229764.6250\n","Epoch 64/1000\n","76/75 [==============================] - ETA: 0s - loss: 135928.7031\n","Epoch 64: val_loss did not improve from 709769.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 135928.7031 - val_loss: 1012090.1250\n","Epoch 65/1000\n","76/75 [==============================] - ETA: 0s - loss: 143313.7344\n","Epoch 65: val_loss did not improve from 709769.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 143313.7344 - val_loss: 1114006.7500\n","Epoch 66/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 154309.6406\n","Epoch 66: val_loss did not improve from 709769.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 153103.6562 - val_loss: 1157700.5000\n","Epoch 67/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 134563.7188\n","Epoch 67: val_loss did not improve from 709769.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 134716.6406 - val_loss: 1095059.1250\n","Epoch 68/1000\n","76/75 [==============================] - ETA: 0s - loss: 145050.0781\n","Epoch 68: val_loss did not improve from 709769.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 145050.0781 - val_loss: 988672.8750\n","Epoch 69/1000\n","76/75 [==============================] - ETA: 0s - loss: 142770.2344\n","Epoch 69: val_loss did not improve from 709769.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 142770.2344 - val_loss: 998292.6250\n","Epoch 70/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 141574.9219\n","Epoch 70: val_loss did not improve from 709769.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 140853.0312 - val_loss: 1086811.5000\n","Epoch 71/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 135254.6094\n","Epoch 71: val_loss did not improve from 709769.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 135833.7500 - val_loss: 1011379.0625\n","Epoch 72/1000\n","76/75 [==============================] - ETA: 0s - loss: 135068.9375\n","Epoch 72: val_loss did not improve from 709769.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 135068.9375 - val_loss: 888932.6875\n","Epoch 73/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 135434.4531\n","Epoch 73: val_loss did not improve from 709769.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 135983.6406 - val_loss: 779960.6250\n","Epoch 74/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 140949.1406\n","Epoch 74: val_loss did not improve from 709769.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 141610.0000 - val_loss: 1012712.3125\n","Epoch 75/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 148428.0312\n","Epoch 75: val_loss did not improve from 709769.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 147960.4062 - val_loss: 1274444.6250\n","Epoch 76/1000\n","76/75 [==============================] - ETA: 0s - loss: 141936.9219\n","Epoch 76: val_loss did not improve from 709769.37500\n","75/75 [==============================] - 1s 17ms/step - loss: 141936.9219 - val_loss: 1064640.5000\n","Epoch 77/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 139586.7500\n","Epoch 77: val_loss did not improve from 709769.37500\n","75/75 [==============================] - 1s 17ms/step - loss: 139741.7812 - val_loss: 945258.3750\n","Epoch 78/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 140266.7188\n","Epoch 78: val_loss did not improve from 709769.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 140766.0000 - val_loss: 981869.8750\n","Epoch 79/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 144977.1719\n","Epoch 79: val_loss did not improve from 709769.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 144160.5938 - val_loss: 914936.7500\n","Epoch 80/1000\n","75/75 [============================>.] - ETA: 0s - loss: 161034.3438\n","Epoch 80: val_loss did not improve from 709769.37500\n","75/75 [==============================] - 1s 18ms/step - loss: 159972.7031 - val_loss: 875027.9375\n","Epoch 81/1000\n","75/75 [============================>.] - ETA: 0s - loss: 165791.9062\n","Epoch 81: val_loss did not improve from 709769.37500\n","75/75 [==============================] - 1s 19ms/step - loss: 164765.2812 - val_loss: 1027844.8750\n","Epoch 82/1000\n","75/75 [============================>.] - ETA: 0s - loss: 146537.7500\n","Epoch 82: val_loss did not improve from 709769.37500\n","75/75 [==============================] - 1s 18ms/step - loss: 145920.9688 - val_loss: 954373.4375\n","Epoch 83/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 135629.7344\n","Epoch 83: val_loss did not improve from 709769.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 135951.8438 - val_loss: 920058.4375\n","Epoch 84/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 140034.1562\n","Epoch 84: val_loss did not improve from 709769.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 139383.4844 - val_loss: 909960.6875\n","Epoch 85/1000\n","75/75 [============================>.] - ETA: 0s - loss: 146743.2812\n","Epoch 85: val_loss did not improve from 709769.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 145845.3750 - val_loss: 761547.1875\n","Epoch 86/1000\n","76/75 [==============================] - ETA: 0s - loss: 134973.6719\n","Epoch 86: val_loss improved from 709769.37500 to 700824.18750, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-3-1000-15.h5\n","75/75 [==============================] - 2s 22ms/step - loss: 134973.6719 - val_loss: 700824.1875\n","Epoch 87/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 144842.9531\n","Epoch 87: val_loss did not improve from 700824.18750\n","75/75 [==============================] - 1s 17ms/step - loss: 147476.3125 - val_loss: 799571.0625\n","Epoch 88/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 131097.5000\n","Epoch 88: val_loss did not improve from 700824.18750\n","75/75 [==============================] - 1s 16ms/step - loss: 136778.4531 - val_loss: 760560.6875\n","Epoch 89/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 140404.7812\n","Epoch 89: val_loss improved from 700824.18750 to 540904.06250, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-3-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 141581.5625 - val_loss: 540904.0625\n","Epoch 90/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 142651.6250\n","Epoch 90: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 141639.7969 - val_loss: 1017964.8750\n","Epoch 91/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 138984.6719\n","Epoch 91: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 138622.0156 - val_loss: 964111.6875\n","Epoch 92/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 134340.8906\n","Epoch 92: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 133610.1094 - val_loss: 949578.2500\n","Epoch 93/1000\n","76/75 [==============================] - ETA: 0s - loss: 129920.0156\n","Epoch 93: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 129920.0156 - val_loss: 1150598.6250\n","Epoch 94/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 122449.3359\n","Epoch 94: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 123639.0312 - val_loss: 887438.5000\n","Epoch 95/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 133618.5625\n","Epoch 95: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 136086.9844 - val_loss: 1036780.9375\n","Epoch 96/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 135406.0156\n","Epoch 96: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 134819.1562 - val_loss: 949850.3750\n","Epoch 97/1000\n","76/75 [==============================] - ETA: 0s - loss: 138396.4531\n","Epoch 97: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 138396.4531 - val_loss: 933055.8750\n","Epoch 98/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 155903.5781\n","Epoch 98: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 154183.8594 - val_loss: 1355578.8750\n","Epoch 99/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 134049.6875\n","Epoch 99: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 17ms/step - loss: 133353.3594 - val_loss: 1158245.6250\n","Epoch 100/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 133240.6562\n","Epoch 100: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 132812.1250 - val_loss: 1196896.0000\n","Epoch 101/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 144514.5156\n","Epoch 101: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 143471.1094 - val_loss: 1166364.8750\n","Epoch 102/1000\n","76/75 [==============================] - ETA: 0s - loss: 134036.8750\n","Epoch 102: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 134036.8750 - val_loss: 1125644.2500\n","Epoch 103/1000\n","76/75 [==============================] - ETA: 0s - loss: 134907.0469\n","Epoch 103: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 134907.0469 - val_loss: 1118931.2500\n","Epoch 104/1000\n","76/75 [==============================] - ETA: 0s - loss: 135727.6406\n","Epoch 104: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 17ms/step - loss: 135727.6406 - val_loss: 1095059.3750\n","Epoch 105/1000\n","75/75 [============================>.] - ETA: 0s - loss: 137688.6562\n","Epoch 105: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 17ms/step - loss: 136865.7188 - val_loss: 964574.6250\n","Epoch 106/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 150010.0469\n","Epoch 106: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 149724.0000 - val_loss: 775112.5000\n","Epoch 107/1000\n","75/75 [============================>.] - ETA: 0s - loss: 152812.9375\n","Epoch 107: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 17ms/step - loss: 151857.3750 - val_loss: 868108.7500\n","Epoch 108/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 140561.5469\n","Epoch 108: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 141721.4688 - val_loss: 721109.6250\n","Epoch 109/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 133161.7344\n","Epoch 109: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 133969.5312 - val_loss: 745378.1250\n","Epoch 110/1000\n","74/75 [============================>.] - ETA: 0s - loss: 154600.5781\n","Epoch 110: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 17ms/step - loss: 153493.1406 - val_loss: 616561.8125\n","Epoch 111/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 144389.9688\n","Epoch 111: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 143770.5781 - val_loss: 835004.6875\n","Epoch 112/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 136775.2969\n","Epoch 112: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 136374.0000 - val_loss: 966317.6250\n","Epoch 113/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 136802.1406\n","Epoch 113: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 136574.0781 - val_loss: 985650.0000\n","Epoch 114/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 128681.1797\n","Epoch 114: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 128641.4375 - val_loss: 880150.8125\n","Epoch 115/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 140949.5625\n","Epoch 115: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 144213.0000 - val_loss: 638307.5625\n","Epoch 116/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 137682.8125\n","Epoch 116: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 136922.2812 - val_loss: 758581.0625\n","Epoch 117/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 150295.0156\n","Epoch 117: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 151610.2031 - val_loss: 655839.8125\n","Epoch 118/1000\n","75/75 [============================>.] - ETA: 0s - loss: 140464.7188\n","Epoch 118: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 140285.9062 - val_loss: 676427.8750\n","Epoch 119/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 136868.2656\n","Epoch 119: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 137499.5938 - val_loss: 734598.4375\n","Epoch 120/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 135532.4219\n","Epoch 120: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 136773.7812 - val_loss: 708856.2500\n","Epoch 121/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 124769.8438\n","Epoch 121: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 127913.0781 - val_loss: 647416.8750\n","Epoch 122/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 133647.9375\n","Epoch 122: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 133563.8594 - val_loss: 690871.4375\n","Epoch 123/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 129695.1562\n","Epoch 123: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 129978.5547 - val_loss: 908291.1250\n","Epoch 124/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 126630.2109\n","Epoch 124: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 125917.8438 - val_loss: 868489.1250\n","Epoch 125/1000\n","76/75 [==============================] - ETA: 0s - loss: 128420.6094\n","Epoch 125: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 128420.6094 - val_loss: 940469.5625\n","Epoch 126/1000\n","76/75 [==============================] - ETA: 0s - loss: 128336.4688\n","Epoch 126: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 128336.4688 - val_loss: 950020.6250\n","Epoch 127/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 121053.4609\n","Epoch 127: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 120997.2969 - val_loss: 1030948.1875\n","Epoch 128/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 126025.0625\n","Epoch 128: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 125145.9375 - val_loss: 1087941.8750\n","Epoch 129/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 130796.5938\n","Epoch 129: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 132206.3594 - val_loss: 972381.7500\n","Epoch 130/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 126482.5391\n","Epoch 130: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 126229.8906 - val_loss: 918015.8750\n","Epoch 131/1000\n","76/75 [==============================] - ETA: 0s - loss: 135067.1094\n","Epoch 131: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 135067.1094 - val_loss: 947769.3750\n","Epoch 132/1000\n","76/75 [==============================] - ETA: 0s - loss: 155689.7188\n","Epoch 132: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 155689.7188 - val_loss: 1039406.6250\n","Epoch 133/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 140355.9688\n","Epoch 133: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 140371.7656 - val_loss: 1000864.5625\n","Epoch 134/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 144420.9688\n","Epoch 134: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 143004.7812 - val_loss: 988021.6250\n","Epoch 135/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 140349.2656\n","Epoch 135: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 139157.4062 - val_loss: 959173.8750\n","Epoch 136/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 129950.7266\n","Epoch 136: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 129239.8516 - val_loss: 1090256.2500\n","Epoch 137/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 131973.9688\n","Epoch 137: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 132971.7969 - val_loss: 947345.0000\n","Epoch 138/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 127720.6172\n","Epoch 138: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 128865.9453 - val_loss: 899412.5625\n","Epoch 139/1000\n","76/75 [==============================] - ETA: 0s - loss: 130374.5547\n","Epoch 139: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 130374.5547 - val_loss: 880086.3125\n","Epoch 140/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 125879.0156\n","Epoch 140: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 127113.2578 - val_loss: 809128.0625\n","Epoch 141/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 126928.6562\n","Epoch 141: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 128271.1250 - val_loss: 919740.8125\n","Epoch 142/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 125401.2031\n","Epoch 142: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 125558.1641 - val_loss: 728920.4375\n","Epoch 143/1000\n","76/75 [==============================] - ETA: 0s - loss: 124220.4609\n","Epoch 143: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 124220.4609 - val_loss: 811893.5625\n","Epoch 144/1000\n","75/75 [============================>.] - ETA: 0s - loss: 125545.8672\n","Epoch 144: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 17ms/step - loss: 124941.4062 - val_loss: 750630.0000\n","Epoch 145/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 141508.6719\n","Epoch 145: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 142158.4219 - val_loss: 798272.5625\n","Epoch 146/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 140063.7969\n","Epoch 146: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 141607.7969 - val_loss: 853303.1250\n","Epoch 147/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 128528.6719\n","Epoch 147: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 127904.5938 - val_loss: 756726.1250\n","Epoch 148/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 141793.8438\n","Epoch 148: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 141837.8281 - val_loss: 920588.3125\n","Epoch 149/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 127849.3203\n","Epoch 149: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 127529.0234 - val_loss: 1057870.7500\n","Epoch 150/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 133987.3594\n","Epoch 150: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 134160.0156 - val_loss: 1137190.2500\n","Epoch 151/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 128835.5469\n","Epoch 151: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 129113.8984 - val_loss: 1157004.8750\n","Epoch 152/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 134366.4219\n","Epoch 152: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 134472.4219 - val_loss: 1042937.9375\n","Epoch 153/1000\n","76/75 [==============================] - ETA: 0s - loss: 130753.7656\n","Epoch 153: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 130753.7656 - val_loss: 992402.0000\n","Epoch 154/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 146827.4531\n","Epoch 154: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 146564.5625 - val_loss: 899521.2500\n","Epoch 155/1000\n","76/75 [==============================] - ETA: 0s - loss: 133165.7656\n","Epoch 155: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 133165.7656 - val_loss: 792111.3125\n","Epoch 156/1000\n","76/75 [==============================] - ETA: 0s - loss: 140114.5469\n","Epoch 156: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 140114.5469 - val_loss: 778762.1875\n","Epoch 157/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 129764.4531\n","Epoch 157: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 128958.5469 - val_loss: 916537.1250\n","Epoch 158/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 140565.9219\n","Epoch 158: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 142205.7656 - val_loss: 910624.3125\n","Epoch 159/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 139695.6562\n","Epoch 159: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 140931.6875 - val_loss: 667243.6875\n","Epoch 160/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 134340.1094\n","Epoch 160: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 134149.3438 - val_loss: 560871.7500\n","Epoch 161/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 131633.0938\n","Epoch 161: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 132578.9844 - val_loss: 927937.1250\n","Epoch 162/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 126732.4609\n","Epoch 162: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 125885.7578 - val_loss: 964119.8750\n","Epoch 163/1000\n","76/75 [==============================] - ETA: 0s - loss: 127618.0703\n","Epoch 163: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 127618.0703 - val_loss: 801252.9375\n","Epoch 164/1000\n","76/75 [==============================] - ETA: 0s - loss: 125021.1562\n","Epoch 164: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 125021.1562 - val_loss: 905431.9375\n","Epoch 165/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 122895.0938\n","Epoch 165: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 122585.5547 - val_loss: 861953.0000\n","Epoch 166/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 121684.7812\n","Epoch 166: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 121333.9609 - val_loss: 762954.7500\n","Epoch 167/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 118106.2500\n","Epoch 167: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 118666.6875 - val_loss: 830649.6875\n","Epoch 168/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 124170.8594\n","Epoch 168: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 125131.2891 - val_loss: 833188.1875\n","Epoch 169/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 127693.7656\n","Epoch 169: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 128981.8203 - val_loss: 770442.1250\n","Epoch 170/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 129980.7109\n","Epoch 170: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 129246.0156 - val_loss: 847500.3125\n","Epoch 171/1000\n","76/75 [==============================] - ETA: 0s - loss: 126978.4922\n","Epoch 171: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 126978.4922 - val_loss: 696623.0625\n","Epoch 172/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 129943.6875\n","Epoch 172: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 132273.4531 - val_loss: 660354.8750\n","Epoch 173/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 152191.2031\n","Epoch 173: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 151062.1250 - val_loss: 903845.5000\n","Epoch 174/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 128154.1094\n","Epoch 174: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 127439.2109 - val_loss: 939425.3125\n","Epoch 175/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 124178.2344\n","Epoch 175: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 123537.1719 - val_loss: 1045279.8750\n","Epoch 176/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 126783.3594\n","Epoch 176: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 128233.4453 - val_loss: 932528.1875\n","Epoch 177/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 126792.8125\n","Epoch 177: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 129213.6328 - val_loss: 753973.0625\n","Epoch 178/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 126297.7891\n","Epoch 178: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 126320.3125 - val_loss: 814719.1875\n","Epoch 179/1000\n","74/75 [============================>.] - ETA: 0s - loss: 127382.1094\n","Epoch 179: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 17ms/step - loss: 125785.0391 - val_loss: 870841.2500\n","Epoch 180/1000\n","76/75 [==============================] - ETA: 0s - loss: 129567.6172\n","Epoch 180: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 129567.6172 - val_loss: 728027.3125\n","Epoch 181/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 142406.4375\n","Epoch 181: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 141827.0156 - val_loss: 911085.3125\n","Epoch 182/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 128967.8438\n","Epoch 182: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 130399.6719 - val_loss: 828521.2500\n","Epoch 183/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 122219.0312\n","Epoch 183: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 122778.3047 - val_loss: 784284.8750\n","Epoch 184/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 123026.8359\n","Epoch 184: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 122587.8281 - val_loss: 814668.8750\n","Epoch 185/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 128382.2344\n","Epoch 185: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 127192.5000 - val_loss: 693395.3125\n","Epoch 186/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 131309.7500\n","Epoch 186: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 130565.1562 - val_loss: 934940.3750\n","Epoch 187/1000\n","76/75 [==============================] - ETA: 0s - loss: 119852.9375\n","Epoch 187: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 119852.9375 - val_loss: 768172.8125\n","Epoch 188/1000\n","76/75 [==============================] - ETA: 0s - loss: 125363.2812\n","Epoch 188: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 17ms/step - loss: 125363.2812 - val_loss: 696435.8750\n","Epoch 189/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 125612.5781\n","Epoch 189: val_loss did not improve from 540904.06250\n","75/75 [==============================] - 1s 16ms/step - loss: 125321.6406 - val_loss: 824434.8125\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1447/1447 [00:00<00:00, 22233.55it/s]\n","100%|██████████| 1419/1419 [00:00<00:00, 21997.28it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/1000\n","76/75 [==============================] - ETA: 0s - loss: 3013067.0000\n","Epoch 1: val_loss improved from inf to 423290.93750, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-4-1000-15.h5\n","75/75 [==============================] - 6s 25ms/step - loss: 3013067.0000 - val_loss: 423290.9375\n","Epoch 2/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 708849.1250\n","Epoch 2: val_loss improved from 423290.93750 to 409200.62500, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-4-1000-15.h5\n","75/75 [==============================] - 1s 17ms/step - loss: 691984.7500 - val_loss: 409200.6250\n","Epoch 3/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 593640.2500\n","Epoch 3: val_loss improved from 409200.62500 to 360658.15625, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-4-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 581468.8750 - val_loss: 360658.1562\n","Epoch 4/1000\n","76/75 [==============================] - ETA: 0s - loss: 596048.9375\n","Epoch 4: val_loss did not improve from 360658.15625\n","75/75 [==============================] - 1s 16ms/step - loss: 596048.9375 - val_loss: 384263.5625\n","Epoch 5/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 572106.5625\n","Epoch 5: val_loss did not improve from 360658.15625\n","75/75 [==============================] - 1s 16ms/step - loss: 563206.7500 - val_loss: 391072.1562\n","Epoch 6/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 536435.0625\n","Epoch 6: val_loss did not improve from 360658.15625\n","75/75 [==============================] - 1s 16ms/step - loss: 528837.5625 - val_loss: 411407.2812\n","Epoch 7/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 476972.4688\n","Epoch 7: val_loss did not improve from 360658.15625\n","75/75 [==============================] - 1s 16ms/step - loss: 471160.9375 - val_loss: 437208.3125\n","Epoch 8/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 455702.2812\n","Epoch 8: val_loss did not improve from 360658.15625\n","75/75 [==============================] - 1s 16ms/step - loss: 447002.4062 - val_loss: 379013.3125\n","Epoch 9/1000\n","76/75 [==============================] - ETA: 0s - loss: 445077.4375\n","Epoch 9: val_loss improved from 360658.15625 to 290469.53125, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-4-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 445077.4375 - val_loss: 290469.5312\n","Epoch 10/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 364646.8438\n","Epoch 10: val_loss did not improve from 290469.53125\n","75/75 [==============================] - 1s 16ms/step - loss: 370471.7812 - val_loss: 456382.5312\n","Epoch 11/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 306433.7500\n","Epoch 11: val_loss improved from 290469.53125 to 283791.18750, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-4-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 300557.8750 - val_loss: 283791.1875\n","Epoch 12/1000\n","74/75 [============================>.] - ETA: 0s - loss: 252297.5469\n","Epoch 12: val_loss improved from 283791.18750 to 262350.46875, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-4-1000-15.h5\n","75/75 [==============================] - 2s 20ms/step - loss: 250038.1094 - val_loss: 262350.4688\n","Epoch 13/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 238132.2188\n","Epoch 13: val_loss improved from 262350.46875 to 253267.01562, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-4-1000-15.h5\n","75/75 [==============================] - 2s 20ms/step - loss: 234662.6875 - val_loss: 253267.0156\n","Epoch 14/1000\n","75/75 [============================>.] - ETA: 0s - loss: 232993.0156\n","Epoch 14: val_loss improved from 253267.01562 to 175779.50000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-4-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 231832.8906 - val_loss: 175779.5000\n","Epoch 15/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 214316.9531\n","Epoch 15: val_loss did not improve from 175779.50000\n","75/75 [==============================] - 1s 16ms/step - loss: 211404.8438 - val_loss: 181002.2969\n","Epoch 16/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 212437.2812\n","Epoch 16: val_loss improved from 175779.50000 to 147891.26562, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-4-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 207482.3125 - val_loss: 147891.2656\n","Epoch 17/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 207969.0312\n","Epoch 17: val_loss did not improve from 147891.26562\n","75/75 [==============================] - 1s 16ms/step - loss: 204170.3906 - val_loss: 156401.4688\n","Epoch 18/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 185101.0625\n","Epoch 18: val_loss improved from 147891.26562 to 127272.89062, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-4-1000-15.h5\n","75/75 [==============================] - 1s 17ms/step - loss: 181919.7656 - val_loss: 127272.8906\n","Epoch 19/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 217141.4219\n","Epoch 19: val_loss did not improve from 127272.89062\n","75/75 [==============================] - 1s 16ms/step - loss: 214486.7344 - val_loss: 202816.9219\n","Epoch 20/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 216780.1250\n","Epoch 20: val_loss did not improve from 127272.89062\n","75/75 [==============================] - 1s 16ms/step - loss: 212798.5625 - val_loss: 142150.0156\n","Epoch 21/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 230167.7188\n","Epoch 21: val_loss did not improve from 127272.89062\n","75/75 [==============================] - 1s 17ms/step - loss: 225859.3281 - val_loss: 154656.0781\n","Epoch 22/1000\n","76/75 [==============================] - ETA: 0s - loss: 199076.2031\n","Epoch 22: val_loss improved from 127272.89062 to 117449.12500, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-4-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 199076.2031 - val_loss: 117449.1250\n","Epoch 23/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 188908.0625\n","Epoch 23: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 186070.5781 - val_loss: 163275.2344\n","Epoch 24/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 192126.6094\n","Epoch 24: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 188898.2969 - val_loss: 151215.2656\n","Epoch 25/1000\n","74/75 [============================>.] - ETA: 0s - loss: 187685.5156\n","Epoch 25: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 185218.8750 - val_loss: 121462.0156\n","Epoch 26/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 169715.9219\n","Epoch 26: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 166806.2812 - val_loss: 141660.3906\n","Epoch 27/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 179778.2656\n","Epoch 27: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 176822.4844 - val_loss: 193690.7812\n","Epoch 28/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 173502.3438\n","Epoch 28: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 170197.8750 - val_loss: 128294.2266\n","Epoch 29/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 176541.0938\n","Epoch 29: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 174316.9219 - val_loss: 211444.1406\n","Epoch 30/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 167026.8906\n","Epoch 30: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 164904.5312 - val_loss: 156972.6562\n","Epoch 31/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 160850.3594\n","Epoch 31: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 159968.9688 - val_loss: 202638.0781\n","Epoch 32/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 153528.3750\n","Epoch 32: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 152132.6719 - val_loss: 199607.4062\n","Epoch 33/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 157372.1406\n","Epoch 33: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 156760.0312 - val_loss: 199424.5000\n","Epoch 34/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 154422.4844\n","Epoch 34: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 153118.5312 - val_loss: 190548.0625\n","Epoch 35/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 149876.0938\n","Epoch 35: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 147767.8906 - val_loss: 197796.0781\n","Epoch 36/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 156169.4375\n","Epoch 36: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 154806.7031 - val_loss: 204489.6406\n","Epoch 37/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 141023.2969\n","Epoch 37: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 140486.6875 - val_loss: 195131.9375\n","Epoch 38/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 157754.5312\n","Epoch 38: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 155643.3125 - val_loss: 296810.1562\n","Epoch 39/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 157395.3125\n","Epoch 39: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 155829.1250 - val_loss: 195891.4375\n","Epoch 40/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 167021.8438\n","Epoch 40: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 165261.0469 - val_loss: 528456.5625\n","Epoch 41/1000\n","76/75 [==============================] - ETA: 0s - loss: 193267.1250\n","Epoch 41: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 193267.1250 - val_loss: 219321.2344\n","Epoch 42/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 173049.1094\n","Epoch 42: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 171686.0312 - val_loss: 272382.8438\n","Epoch 43/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 161583.0469\n","Epoch 43: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 158808.4688 - val_loss: 220225.2188\n","Epoch 44/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 154051.4688\n","Epoch 44: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 151578.2812 - val_loss: 260499.7500\n","Epoch 45/1000\n","76/75 [==============================] - ETA: 0s - loss: 152078.3906\n","Epoch 45: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 152078.3906 - val_loss: 218163.7344\n","Epoch 46/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 177043.2500\n","Epoch 46: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 173720.5156 - val_loss: 375091.4062\n","Epoch 47/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 159783.4219\n","Epoch 47: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 158224.3438 - val_loss: 157600.7188\n","Epoch 48/1000\n","76/75 [==============================] - ETA: 0s - loss: 166024.2969\n","Epoch 48: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 166024.2969 - val_loss: 319890.5625\n","Epoch 49/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 153305.3281\n","Epoch 49: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 150688.7031 - val_loss: 150170.5781\n","Epoch 50/1000\n","76/75 [==============================] - ETA: 0s - loss: 145237.8750\n","Epoch 50: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 145237.8750 - val_loss: 231938.6250\n","Epoch 51/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 133644.4844\n","Epoch 51: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 133794.5625 - val_loss: 232480.1094\n","Epoch 52/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 129610.6484\n","Epoch 52: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 128766.2344 - val_loss: 201641.5469\n","Epoch 53/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 134596.8125\n","Epoch 53: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 133088.1406 - val_loss: 196756.2812\n","Epoch 54/1000\n","76/75 [==============================] - ETA: 0s - loss: 128214.6562\n","Epoch 54: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 128214.6562 - val_loss: 223139.3750\n","Epoch 55/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 136704.0156\n","Epoch 55: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 134996.5312 - val_loss: 231061.1406\n","Epoch 56/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 152172.4688\n","Epoch 56: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 149130.0625 - val_loss: 237237.8906\n","Epoch 57/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 141627.2188\n","Epoch 57: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 139770.6094 - val_loss: 291965.0312\n","Epoch 58/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 158372.2500\n","Epoch 58: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 156526.5625 - val_loss: 204628.2656\n","Epoch 59/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 150217.3438\n","Epoch 59: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 147416.6562 - val_loss: 233712.5469\n","Epoch 60/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 155471.3438\n","Epoch 60: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 153321.4844 - val_loss: 235052.6406\n","Epoch 61/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 144732.7969\n","Epoch 61: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 143033.5781 - val_loss: 291269.0000\n","Epoch 62/1000\n","76/75 [==============================] - ETA: 0s - loss: 138340.0156\n","Epoch 62: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 138340.0156 - val_loss: 208502.5312\n","Epoch 63/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 132423.1875\n","Epoch 63: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 131339.2344 - val_loss: 252224.5625\n","Epoch 64/1000\n","76/75 [==============================] - ETA: 0s - loss: 131840.4688\n","Epoch 64: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 131840.4688 - val_loss: 198733.2188\n","Epoch 65/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 130151.9766\n","Epoch 65: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 127366.1875 - val_loss: 213682.2656\n","Epoch 66/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 125674.8594\n","Epoch 66: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 125087.4219 - val_loss: 169764.9219\n","Epoch 67/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 132572.6875\n","Epoch 67: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 130429.6484 - val_loss: 218013.5469\n","Epoch 68/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 126183.3906\n","Epoch 68: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 125502.0703 - val_loss: 185035.5000\n","Epoch 69/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 123775.6797\n","Epoch 69: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 123313.3672 - val_loss: 176830.6719\n","Epoch 70/1000\n","76/75 [==============================] - ETA: 0s - loss: 118645.0312\n","Epoch 70: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 118645.0312 - val_loss: 145865.1094\n","Epoch 71/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 124424.9141\n","Epoch 71: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 122501.3984 - val_loss: 233971.2344\n","Epoch 72/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 125488.9922\n","Epoch 72: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 124844.7734 - val_loss: 150355.9844\n","Epoch 73/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 130464.5703\n","Epoch 73: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 128639.7031 - val_loss: 258116.9844\n","Epoch 74/1000\n","76/75 [==============================] - ETA: 0s - loss: 141891.8125\n","Epoch 74: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 141891.8125 - val_loss: 224683.6562\n","Epoch 75/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 129729.7812\n","Epoch 75: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 128638.9688 - val_loss: 261493.4062\n","Epoch 76/1000\n","76/75 [==============================] - ETA: 0s - loss: 124416.0391\n","Epoch 76: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 124416.0391 - val_loss: 189726.7344\n","Epoch 77/1000\n","76/75 [==============================] - ETA: 0s - loss: 132050.8125\n","Epoch 77: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 132050.8125 - val_loss: 214961.5000\n","Epoch 78/1000\n","76/75 [==============================] - ETA: 0s - loss: 135605.2188\n","Epoch 78: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 135605.2188 - val_loss: 169506.7344\n","Epoch 79/1000\n","75/75 [============================>.] - ETA: 0s - loss: 128810.9688\n","Epoch 79: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 128187.7578 - val_loss: 261636.2500\n","Epoch 80/1000\n","76/75 [==============================] - ETA: 0s - loss: 148383.6875\n","Epoch 80: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 148383.6875 - val_loss: 293734.2500\n","Epoch 81/1000\n","76/75 [==============================] - ETA: 0s - loss: 135635.2188\n","Epoch 81: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 135635.2188 - val_loss: 193003.1562\n","Epoch 82/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 132189.8125\n","Epoch 82: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 129848.5312 - val_loss: 167625.3125\n","Epoch 83/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 124759.3906\n","Epoch 83: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 122861.1406 - val_loss: 226531.2344\n","Epoch 84/1000\n","75/75 [============================>.] - ETA: 0s - loss: 139506.4062\n","Epoch 84: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 138845.1250 - val_loss: 185539.7344\n","Epoch 85/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 128670.7422\n","Epoch 85: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 127397.2266 - val_loss: 310829.3125\n","Epoch 86/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 157328.2656\n","Epoch 86: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 154879.4531 - val_loss: 348070.5625\n","Epoch 87/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 158020.6875\n","Epoch 87: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 155000.7812 - val_loss: 293943.5625\n","Epoch 88/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 163823.2188\n","Epoch 88: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 160523.4219 - val_loss: 344103.5625\n","Epoch 89/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 149167.9062\n","Epoch 89: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 149628.3906 - val_loss: 234105.4688\n","Epoch 90/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 131147.1406\n","Epoch 90: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 129862.0156 - val_loss: 138617.1406\n","Epoch 91/1000\n","76/75 [==============================] - ETA: 0s - loss: 113826.2422\n","Epoch 91: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 113826.2422 - val_loss: 142166.4062\n","Epoch 92/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 123903.5625\n","Epoch 92: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 121346.2969 - val_loss: 182948.3125\n","Epoch 93/1000\n","75/75 [============================>.] - ETA: 0s - loss: 116443.8125\n","Epoch 93: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 116130.4844 - val_loss: 153296.8125\n","Epoch 94/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 113343.2344\n","Epoch 94: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 112535.8438 - val_loss: 273914.0938\n","Epoch 95/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 150831.6562\n","Epoch 95: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 147467.5625 - val_loss: 252096.3438\n","Epoch 96/1000\n","76/75 [==============================] - ETA: 0s - loss: 144049.4531\n","Epoch 96: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 144049.4531 - val_loss: 239580.1094\n","Epoch 97/1000\n","76/75 [==============================] - ETA: 0s - loss: 128016.5625\n","Epoch 97: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 128016.5625 - val_loss: 190453.4844\n","Epoch 98/1000\n","76/75 [==============================] - ETA: 0s - loss: 127982.8438\n","Epoch 98: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 127982.8438 - val_loss: 175746.3438\n","Epoch 99/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 129150.3906\n","Epoch 99: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 127341.4219 - val_loss: 197765.7500\n","Epoch 100/1000\n","76/75 [==============================] - ETA: 0s - loss: 117656.8359\n","Epoch 100: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 117656.8359 - val_loss: 201973.5312\n","Epoch 101/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 117552.6875\n","Epoch 101: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 17ms/step - loss: 117001.6953 - val_loss: 163361.8594\n","Epoch 102/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 108098.9844\n","Epoch 102: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 106690.2188 - val_loss: 163887.3594\n","Epoch 103/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 117050.3750\n","Epoch 103: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 114864.9766 - val_loss: 144730.0625\n","Epoch 104/1000\n","76/75 [==============================] - ETA: 0s - loss: 110873.3125\n","Epoch 104: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 110873.3125 - val_loss: 180242.6406\n","Epoch 105/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 115172.6953\n","Epoch 105: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 114427.2109 - val_loss: 182100.3125\n","Epoch 106/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 112867.0859\n","Epoch 106: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 111269.9531 - val_loss: 160512.7812\n","Epoch 107/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 126079.3438\n","Epoch 107: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 124112.3281 - val_loss: 392300.0000\n","Epoch 108/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 196046.5469\n","Epoch 108: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 191846.2812 - val_loss: 599695.6875\n","Epoch 109/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 493003.8750\n","Epoch 109: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 480624.4375 - val_loss: 164493.2969\n","Epoch 110/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 218654.3125\n","Epoch 110: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 213951.8594 - val_loss: 294056.5938\n","Epoch 111/1000\n","76/75 [==============================] - ETA: 0s - loss: 164502.2500\n","Epoch 111: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 164502.2500 - val_loss: 208213.5000\n","Epoch 112/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 131015.6406\n","Epoch 112: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 129646.8438 - val_loss: 143160.7188\n","Epoch 113/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 128724.4453\n","Epoch 113: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 126602.6328 - val_loss: 145003.5156\n","Epoch 114/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 115178.6953\n","Epoch 114: val_loss did not improve from 117449.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 113625.9844 - val_loss: 131959.2500\n","Epoch 115/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 116821.6406\n","Epoch 115: val_loss improved from 117449.12500 to 114207.60938, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-4-1000-15.h5\n","75/75 [==============================] - 2s 22ms/step - loss: 115119.8594 - val_loss: 114207.6094\n","Epoch 116/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 120114.1953\n","Epoch 116: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 118104.9844 - val_loss: 125335.1094\n","Epoch 117/1000\n","76/75 [==============================] - ETA: 0s - loss: 117293.7500\n","Epoch 117: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 117293.7500 - val_loss: 115830.8672\n","Epoch 118/1000\n","76/75 [==============================] - ETA: 0s - loss: 116425.8281\n","Epoch 118: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 17ms/step - loss: 116425.8281 - val_loss: 128062.9688\n","Epoch 119/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 115714.2969\n","Epoch 119: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 114590.6953 - val_loss: 145274.8438\n","Epoch 120/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 124821.1562\n","Epoch 120: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 122584.3281 - val_loss: 128602.6641\n","Epoch 121/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 119901.6328\n","Epoch 121: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 117865.2422 - val_loss: 134612.7344\n","Epoch 122/1000\n","75/75 [============================>.] - ETA: 0s - loss: 112427.9141\n","Epoch 122: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 111634.5625 - val_loss: 119732.7734\n","Epoch 123/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 116582.6250\n","Epoch 123: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 114330.4688 - val_loss: 138982.8594\n","Epoch 124/1000\n","76/75 [==============================] - ETA: 0s - loss: 113956.3672\n","Epoch 124: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 17ms/step - loss: 113956.3672 - val_loss: 140843.1250\n","Epoch 125/1000\n","76/75 [==============================] - ETA: 0s - loss: 112319.2031\n","Epoch 125: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 17ms/step - loss: 112319.2031 - val_loss: 122781.9453\n","Epoch 126/1000\n","75/75 [============================>.] - ETA: 0s - loss: 119120.4531\n","Epoch 126: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 118261.5703 - val_loss: 218906.7969\n","Epoch 127/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 148806.6094\n","Epoch 127: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 147130.8906 - val_loss: 167835.5156\n","Epoch 128/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 130808.4453\n","Epoch 128: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 129070.5312 - val_loss: 150903.4375\n","Epoch 129/1000\n","76/75 [==============================] - ETA: 0s - loss: 127377.0156\n","Epoch 129: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 17ms/step - loss: 127377.0156 - val_loss: 149846.4844\n","Epoch 130/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 113156.6094\n","Epoch 130: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 112940.7656 - val_loss: 120149.2812\n","Epoch 131/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 124901.2891\n","Epoch 131: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 122884.1016 - val_loss: 188952.1719\n","Epoch 132/1000\n","76/75 [==============================] - ETA: 0s - loss: 108724.5312\n","Epoch 132: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 108724.5312 - val_loss: 135028.2188\n","Epoch 133/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 125620.0625\n","Epoch 133: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 124127.9141 - val_loss: 141788.2188\n","Epoch 134/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 118808.1250\n","Epoch 134: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 117929.5547 - val_loss: 184906.9219\n","Epoch 135/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 133697.3750\n","Epoch 135: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 132500.0938 - val_loss: 138774.7812\n","Epoch 136/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 122705.1250\n","Epoch 136: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 121543.8203 - val_loss: 150431.3438\n","Epoch 137/1000\n","76/75 [==============================] - ETA: 0s - loss: 124413.5859\n","Epoch 137: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 17ms/step - loss: 124413.5859 - val_loss: 168279.3125\n","Epoch 138/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 129533.6875\n","Epoch 138: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 127981.2969 - val_loss: 216021.2656\n","Epoch 139/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 145813.5156\n","Epoch 139: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 144283.3750 - val_loss: 143568.7969\n","Epoch 140/1000\n","75/75 [============================>.] - ETA: 0s - loss: 149596.5625\n","Epoch 140: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 19ms/step - loss: 149045.5000 - val_loss: 247786.8125\n","Epoch 141/1000\n","76/75 [==============================] - ETA: 0s - loss: 295725.5000\n","Epoch 141: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 19ms/step - loss: 295725.5000 - val_loss: 469637.1250\n","Epoch 142/1000\n","75/75 [============================>.] - ETA: 0s - loss: 1143627.2500\n","Epoch 142: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 18ms/step - loss: 1146934.6250 - val_loss: 2260154.2500\n","Epoch 143/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 833554.7500\n","Epoch 143: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 817620.7500 - val_loss: 773030.0625\n","Epoch 144/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 672926.0000\n","Epoch 144: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 678452.7500 - val_loss: 589372.1875\n","Epoch 145/1000\n","76/75 [==============================] - ETA: 0s - loss: 578608.9375\n","Epoch 145: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 17ms/step - loss: 578608.9375 - val_loss: 584491.6250\n","Epoch 146/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 562287.3125\n","Epoch 146: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 577032.7500 - val_loss: 774164.7500\n","Epoch 147/1000\n","76/75 [==============================] - ETA: 0s - loss: 532968.5000\n","Epoch 147: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 532968.5000 - val_loss: 727566.8125\n","Epoch 148/1000\n","76/75 [==============================] - ETA: 0s - loss: 523434.8125\n","Epoch 148: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 523434.8125 - val_loss: 661419.6250\n","Epoch 149/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 451562.4375\n","Epoch 149: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 458279.9688 - val_loss: 747714.3750\n","Epoch 150/1000\n","76/75 [==============================] - ETA: 0s - loss: 470386.0938\n","Epoch 150: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 470386.0938 - val_loss: 701904.8125\n","Epoch 151/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 438911.7812\n","Epoch 151: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 17ms/step - loss: 435831.3438 - val_loss: 561297.7500\n","Epoch 152/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 453529.0000\n","Epoch 152: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 448011.4062 - val_loss: 760080.8125\n","Epoch 153/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 448884.5938\n","Epoch 153: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 439852.1562 - val_loss: 481367.7812\n","Epoch 154/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 365153.0938\n","Epoch 154: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 359381.2812 - val_loss: 579646.8750\n","Epoch 155/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 331801.0938\n","Epoch 155: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 326125.8438 - val_loss: 544316.9375\n","Epoch 156/1000\n","76/75 [==============================] - ETA: 0s - loss: 314885.1250\n","Epoch 156: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 314885.1250 - val_loss: 521172.0625\n","Epoch 157/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 269333.0000\n","Epoch 157: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 268241.4375 - val_loss: 498811.2812\n","Epoch 158/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 270747.9062\n","Epoch 158: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 17ms/step - loss: 267509.7812 - val_loss: 495711.4375\n","Epoch 159/1000\n","76/75 [==============================] - ETA: 0s - loss: 231619.5625\n","Epoch 159: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 17ms/step - loss: 231619.5625 - val_loss: 415195.8125\n","Epoch 160/1000\n","76/75 [==============================] - ETA: 0s - loss: 220551.7188\n","Epoch 160: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 220551.7188 - val_loss: 383007.5312\n","Epoch 161/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 234479.5469\n","Epoch 161: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 231427.5000 - val_loss: 402363.5000\n","Epoch 162/1000\n","76/75 [==============================] - ETA: 0s - loss: 217741.1406\n","Epoch 162: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 217741.1406 - val_loss: 395843.2812\n","Epoch 163/1000\n","76/75 [==============================] - ETA: 0s - loss: 208571.1719\n","Epoch 163: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 208571.1719 - val_loss: 470209.3438\n","Epoch 164/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 218136.3438\n","Epoch 164: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 215126.2656 - val_loss: 518481.5625\n","Epoch 165/1000\n","76/75 [==============================] - ETA: 0s - loss: 205250.5938\n","Epoch 165: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 205250.5938 - val_loss: 467024.7500\n","Epoch 166/1000\n","76/75 [==============================] - ETA: 0s - loss: 206431.7812\n","Epoch 166: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 206431.7812 - val_loss: 403237.2812\n","Epoch 167/1000\n","76/75 [==============================] - ETA: 0s - loss: 203280.1719\n","Epoch 167: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 203280.1719 - val_loss: 312774.5312\n","Epoch 168/1000\n","76/75 [==============================] - ETA: 0s - loss: 190269.3281\n","Epoch 168: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 190269.3281 - val_loss: 378768.2500\n","Epoch 169/1000\n","76/75 [==============================] - ETA: 0s - loss: 175054.8750\n","Epoch 169: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 175054.8750 - val_loss: 319115.7812\n","Epoch 170/1000\n","76/75 [==============================] - ETA: 0s - loss: 181597.7656\n","Epoch 170: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 181597.7656 - val_loss: 310766.7812\n","Epoch 171/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 178813.5469\n","Epoch 171: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 178590.3438 - val_loss: 371253.5000\n","Epoch 172/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 173630.3750\n","Epoch 172: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 17ms/step - loss: 171762.0781 - val_loss: 349320.0938\n","Epoch 173/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 170183.7656\n","Epoch 173: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 169100.1875 - val_loss: 398753.2500\n","Epoch 174/1000\n","75/75 [============================>.] - ETA: 0s - loss: 162494.7969\n","Epoch 174: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 161584.1250 - val_loss: 314367.8750\n","Epoch 175/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 171382.9062\n","Epoch 175: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 171009.9531 - val_loss: 227807.3594\n","Epoch 176/1000\n","76/75 [==============================] - ETA: 0s - loss: 166721.1094\n","Epoch 176: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 166721.1094 - val_loss: 240591.3750\n","Epoch 177/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 177450.7031\n","Epoch 177: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 177315.5312 - val_loss: 238546.6719\n","Epoch 178/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 171328.6719\n","Epoch 178: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 169869.6406 - val_loss: 227906.2969\n","Epoch 179/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 168082.5938\n","Epoch 179: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 167392.7969 - val_loss: 250101.6562\n","Epoch 180/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 168686.5938\n","Epoch 180: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 166191.1094 - val_loss: 277570.1875\n","Epoch 181/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 160407.7344\n","Epoch 181: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 158150.1094 - val_loss: 247774.9062\n","Epoch 182/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 161454.8281\n","Epoch 182: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 160134.6250 - val_loss: 238780.5938\n","Epoch 183/1000\n","76/75 [==============================] - ETA: 0s - loss: 157344.8281\n","Epoch 183: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 157344.8281 - val_loss: 282852.1562\n","Epoch 184/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 166844.8438\n","Epoch 184: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 17ms/step - loss: 166763.9688 - val_loss: 258666.9062\n","Epoch 185/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 167986.4688\n","Epoch 185: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 167624.4844 - val_loss: 231897.4531\n","Epoch 186/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 174402.1406\n","Epoch 186: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 172500.3906 - val_loss: 180094.2656\n","Epoch 187/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 161278.8594\n","Epoch 187: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 161351.9219 - val_loss: 176755.8594\n","Epoch 188/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 163350.5156\n","Epoch 188: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 160870.2188 - val_loss: 212237.7188\n","Epoch 189/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 167395.5312\n","Epoch 189: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 167576.6406 - val_loss: 202797.6094\n","Epoch 190/1000\n","75/75 [============================>.] - ETA: 0s - loss: 153109.2812\n","Epoch 190: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 17ms/step - loss: 152704.5000 - val_loss: 195815.0938\n","Epoch 191/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 150217.0781\n","Epoch 191: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 147948.6406 - val_loss: 193705.5000\n","Epoch 192/1000\n","76/75 [==============================] - ETA: 0s - loss: 144323.0938\n","Epoch 192: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 144323.0938 - val_loss: 210298.4844\n","Epoch 193/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 162403.9375\n","Epoch 193: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 160651.0312 - val_loss: 179503.0781\n","Epoch 194/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 146684.0625\n","Epoch 194: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 145357.4531 - val_loss: 174177.1250\n","Epoch 195/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 152945.2969\n","Epoch 195: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 151862.0938 - val_loss: 245452.1719\n","Epoch 196/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 143025.3750\n","Epoch 196: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 142907.9531 - val_loss: 161557.5312\n","Epoch 197/1000\n","75/75 [============================>.] - ETA: 0s - loss: 157247.1250\n","Epoch 197: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 17ms/step - loss: 156371.6719 - val_loss: 188185.3438\n","Epoch 198/1000\n","76/75 [==============================] - ETA: 0s - loss: 142137.4844\n","Epoch 198: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 17ms/step - loss: 142137.4844 - val_loss: 173739.1250\n","Epoch 199/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 151275.6875\n","Epoch 199: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 148461.0625 - val_loss: 215629.9219\n","Epoch 200/1000\n","76/75 [==============================] - ETA: 0s - loss: 143401.3281\n","Epoch 200: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 143401.3281 - val_loss: 195056.3281\n","Epoch 201/1000\n","75/75 [============================>.] - ETA: 0s - loss: 136029.4062\n","Epoch 201: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 135617.2656 - val_loss: 234374.2969\n","Epoch 202/1000\n","75/75 [============================>.] - ETA: 0s - loss: 143938.7500\n","Epoch 202: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 17ms/step - loss: 143587.6250 - val_loss: 187490.1875\n","Epoch 203/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 143127.0156\n","Epoch 203: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 141805.7656 - val_loss: 216785.2344\n","Epoch 204/1000\n","76/75 [==============================] - ETA: 0s - loss: 149722.9688\n","Epoch 204: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 17ms/step - loss: 149722.9688 - val_loss: 181327.9062\n","Epoch 205/1000\n","75/75 [============================>.] - ETA: 0s - loss: 138042.9844\n","Epoch 205: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 17ms/step - loss: 138046.1406 - val_loss: 201864.5625\n","Epoch 206/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 138244.4219\n","Epoch 206: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 138312.2812 - val_loss: 179127.7031\n","Epoch 207/1000\n","76/75 [==============================] - ETA: 0s - loss: 139408.2812\n","Epoch 207: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 17ms/step - loss: 139408.2812 - val_loss: 224074.1719\n","Epoch 208/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 134207.6094\n","Epoch 208: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 17ms/step - loss: 133573.2344 - val_loss: 176001.1719\n","Epoch 209/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 146767.7500\n","Epoch 209: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 144801.9062 - val_loss: 198539.2812\n","Epoch 210/1000\n","76/75 [==============================] - ETA: 0s - loss: 138170.3125\n","Epoch 210: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 17ms/step - loss: 138170.3125 - val_loss: 201143.1406\n","Epoch 211/1000\n","76/75 [==============================] - ETA: 0s - loss: 140309.0625\n","Epoch 211: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 17ms/step - loss: 140309.0625 - val_loss: 274716.3750\n","Epoch 212/1000\n","76/75 [==============================] - ETA: 0s - loss: 147109.3438\n","Epoch 212: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 17ms/step - loss: 147109.3438 - val_loss: 224222.7031\n","Epoch 213/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 158367.4219\n","Epoch 213: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 157106.3125 - val_loss: 360184.4062\n","Epoch 214/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 167908.7969\n","Epoch 214: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 165963.5156 - val_loss: 258655.5469\n","Epoch 215/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 189493.7500\n","Epoch 215: val_loss did not improve from 114207.60938\n","75/75 [==============================] - 1s 16ms/step - loss: 185961.5156 - val_loss: 274941.5938\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1447/1447 [00:00<00:00, 22780.58it/s]\n","100%|██████████| 1419/1419 [00:00<00:00, 21680.77it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/1000\n","76/75 [==============================] - ETA: 0s - loss: 14109885.0000\n","Epoch 1: val_loss improved from inf to 3633237.75000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-5-1000-15.h5\n","75/75 [==============================] - 6s 25ms/step - loss: 14109885.0000 - val_loss: 3633237.7500\n","Epoch 2/1000\n","76/75 [==============================] - ETA: 0s - loss: 1844066.6250\n","Epoch 2: val_loss improved from 3633237.75000 to 1616361.37500, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-5-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 1844066.6250 - val_loss: 1616361.3750\n","Epoch 3/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 1428398.0000\n","Epoch 3: val_loss improved from 1616361.37500 to 1436909.12500, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-5-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 1535201.1250 - val_loss: 1436909.1250\n","Epoch 4/1000\n","76/75 [==============================] - ETA: 0s - loss: 2130756.5000\n","Epoch 4: val_loss did not improve from 1436909.12500\n","75/75 [==============================] - 1s 16ms/step - loss: 2130756.5000 - val_loss: 1450394.8750\n","Epoch 5/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 1381496.3750\n","Epoch 5: val_loss improved from 1436909.12500 to 1361782.50000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-5-1000-15.h5\n","75/75 [==============================] - 1s 17ms/step - loss: 1488106.7500 - val_loss: 1361782.5000\n","Epoch 6/1000\n","75/75 [============================>.] - ETA: 0s - loss: 1410171.2500\n","Epoch 6: val_loss improved from 1361782.50000 to 1183517.62500, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-5-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 1411514.8750 - val_loss: 1183517.6250\n","Epoch 7/1000\n","76/75 [==============================] - ETA: 0s - loss: 1426872.1250\n","Epoch 7: val_loss did not improve from 1183517.62500\n","75/75 [==============================] - 1s 16ms/step - loss: 1426872.1250 - val_loss: 1323045.5000\n","Epoch 8/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 1423582.3750\n","Epoch 8: val_loss did not improve from 1183517.62500\n","75/75 [==============================] - 1s 16ms/step - loss: 1504125.1250 - val_loss: 1194520.6250\n","Epoch 9/1000\n","75/75 [============================>.] - ETA: 0s - loss: 1359099.5000\n","Epoch 9: val_loss improved from 1183517.62500 to 1070807.00000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-5-1000-15.h5\n","75/75 [==============================] - 1s 19ms/step - loss: 1361532.7500 - val_loss: 1070807.0000\n","Epoch 10/1000\n","75/75 [============================>.] - ETA: 0s - loss: 1172069.6250\n","Epoch 10: val_loss improved from 1070807.00000 to 968193.00000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-5-1000-15.h5\n","75/75 [==============================] - 2s 20ms/step - loss: 1174956.1250 - val_loss: 968193.0000\n","Epoch 11/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 916106.7500\n","Epoch 11: val_loss improved from 968193.00000 to 676025.43750, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-5-1000-15.h5\n","75/75 [==============================] - 1s 19ms/step - loss: 996448.4375 - val_loss: 676025.4375\n","Epoch 12/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 1047853.1250\n","Epoch 12: val_loss did not improve from 676025.43750\n","75/75 [==============================] - 1s 16ms/step - loss: 1103690.7500 - val_loss: 934013.6875\n","Epoch 13/1000\n","76/75 [==============================] - ETA: 0s - loss: 840066.0000\n","Epoch 13: val_loss did not improve from 676025.43750\n","75/75 [==============================] - 1s 16ms/step - loss: 840066.0000 - val_loss: 732365.0000\n","Epoch 14/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 842781.3750\n","Epoch 14: val_loss did not improve from 676025.43750\n","75/75 [==============================] - 1s 16ms/step - loss: 910396.3125 - val_loss: 783080.1250\n","Epoch 15/1000\n","76/75 [==============================] - ETA: 0s - loss: 780089.9375\n","Epoch 15: val_loss did not improve from 676025.43750\n","75/75 [==============================] - 1s 16ms/step - loss: 780089.9375 - val_loss: 698050.0625\n","Epoch 16/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 726255.1250\n","Epoch 16: val_loss did not improve from 676025.43750\n","75/75 [==============================] - 1s 16ms/step - loss: 769150.5625 - val_loss: 729206.3125\n","Epoch 17/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 609117.2500\n","Epoch 17: val_loss did not improve from 676025.43750\n","75/75 [==============================] - 1s 16ms/step - loss: 643548.0000 - val_loss: 900787.0000\n","Epoch 18/1000\n","76/75 [==============================] - ETA: 0s - loss: 692242.8125\n","Epoch 18: val_loss did not improve from 676025.43750\n","75/75 [==============================] - 1s 16ms/step - loss: 692242.8125 - val_loss: 1008991.4375\n","Epoch 19/1000\n","76/75 [==============================] - ETA: 0s - loss: 640733.1875\n","Epoch 19: val_loss did not improve from 676025.43750\n","75/75 [==============================] - 1s 16ms/step - loss: 640733.1875 - val_loss: 1020828.3750\n","Epoch 20/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 594147.0000\n","Epoch 20: val_loss did not improve from 676025.43750\n","75/75 [==============================] - 1s 16ms/step - loss: 631228.1250 - val_loss: 1109504.8750\n","Epoch 21/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 758907.9375\n","Epoch 21: val_loss did not improve from 676025.43750\n","75/75 [==============================] - 1s 16ms/step - loss: 799823.2500 - val_loss: 1668568.3750\n","Epoch 22/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 795676.1875\n","Epoch 22: val_loss did not improve from 676025.43750\n","75/75 [==============================] - 1s 16ms/step - loss: 834655.1250 - val_loss: 1475389.7500\n","Epoch 23/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 637726.3125\n","Epoch 23: val_loss did not improve from 676025.43750\n","75/75 [==============================] - 1s 16ms/step - loss: 659792.8750 - val_loss: 1283085.1250\n","Epoch 24/1000\n","75/75 [============================>.] - ETA: 0s - loss: 666320.0625\n","Epoch 24: val_loss did not improve from 676025.43750\n","75/75 [==============================] - 1s 17ms/step - loss: 669033.1875 - val_loss: 764565.5625\n","Epoch 25/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 502573.1250\n","Epoch 25: val_loss did not improve from 676025.43750\n","75/75 [==============================] - 1s 16ms/step - loss: 521526.3438 - val_loss: 877509.1250\n","Epoch 26/1000\n","76/75 [==============================] - ETA: 0s - loss: 499301.0000\n","Epoch 26: val_loss did not improve from 676025.43750\n","75/75 [==============================] - 1s 16ms/step - loss: 499301.0000 - val_loss: 1037485.0625\n","Epoch 27/1000\n","76/75 [==============================] - ETA: 0s - loss: 540272.7500\n","Epoch 27: val_loss did not improve from 676025.43750\n","75/75 [==============================] - 1s 16ms/step - loss: 540272.7500 - val_loss: 1223547.1250\n","Epoch 28/1000\n","76/75 [==============================] - ETA: 0s - loss: 598476.8750\n","Epoch 28: val_loss did not improve from 676025.43750\n","75/75 [==============================] - 1s 16ms/step - loss: 598476.8750 - val_loss: 1563993.3750\n","Epoch 29/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 575412.8125\n","Epoch 29: val_loss did not improve from 676025.43750\n","75/75 [==============================] - 1s 16ms/step - loss: 589855.8125 - val_loss: 1306736.2500\n","Epoch 30/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 565442.0000\n","Epoch 30: val_loss did not improve from 676025.43750\n","75/75 [==============================] - 1s 16ms/step - loss: 582281.5625 - val_loss: 1256375.3750\n","Epoch 31/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 532388.8750\n","Epoch 31: val_loss did not improve from 676025.43750\n","75/75 [==============================] - 1s 16ms/step - loss: 555590.3750 - val_loss: 1444816.3750\n","Epoch 32/1000\n","76/75 [==============================] - ETA: 0s - loss: 668348.8125\n","Epoch 32: val_loss did not improve from 676025.43750\n","75/75 [==============================] - 1s 16ms/step - loss: 668348.8125 - val_loss: 1679274.7500\n","Epoch 33/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 644570.0625\n","Epoch 33: val_loss did not improve from 676025.43750\n","75/75 [==============================] - 1s 16ms/step - loss: 659607.8125 - val_loss: 950457.2500\n","Epoch 34/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 559426.3125\n","Epoch 34: val_loss did not improve from 676025.43750\n","75/75 [==============================] - 1s 16ms/step - loss: 588400.3125 - val_loss: 1066261.1250\n","Epoch 35/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 579303.6875\n","Epoch 35: val_loss did not improve from 676025.43750\n","75/75 [==============================] - 1s 16ms/step - loss: 594627.9375 - val_loss: 888875.5000\n","Epoch 36/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 539921.3125\n","Epoch 36: val_loss did not improve from 676025.43750\n","75/75 [==============================] - 1s 16ms/step - loss: 565481.5625 - val_loss: 1169359.7500\n","Epoch 37/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 574245.8750\n","Epoch 37: val_loss did not improve from 676025.43750\n","75/75 [==============================] - 1s 16ms/step - loss: 610502.0000 - val_loss: 2428953.5000\n","Epoch 38/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 673192.3125\n","Epoch 38: val_loss did not improve from 676025.43750\n","75/75 [==============================] - 1s 16ms/step - loss: 684180.9375 - val_loss: 2075447.8750\n","Epoch 39/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 622407.3125\n","Epoch 39: val_loss did not improve from 676025.43750\n","75/75 [==============================] - 1s 16ms/step - loss: 630623.4375 - val_loss: 1816319.5000\n","Epoch 40/1000\n","75/75 [============================>.] - ETA: 0s - loss: 548664.6250\n","Epoch 40: val_loss did not improve from 676025.43750\n","75/75 [==============================] - 1s 16ms/step - loss: 552874.9375 - val_loss: 1661152.8750\n","Epoch 41/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 589700.5625\n","Epoch 41: val_loss did not improve from 676025.43750\n","75/75 [==============================] - 1s 16ms/step - loss: 605237.0000 - val_loss: 2188365.0000\n","Epoch 42/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 591255.5625\n","Epoch 42: val_loss did not improve from 676025.43750\n","75/75 [==============================] - 1s 16ms/step - loss: 596200.7500 - val_loss: 1853727.5000\n","Epoch 43/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 619070.6875\n","Epoch 43: val_loss did not improve from 676025.43750\n","75/75 [==============================] - 1s 16ms/step - loss: 633243.3750 - val_loss: 2281562.5000\n","Epoch 44/1000\n","76/75 [==============================] - ETA: 0s - loss: 560079.0625\n","Epoch 44: val_loss did not improve from 676025.43750\n","75/75 [==============================] - 1s 16ms/step - loss: 560079.0625 - val_loss: 2115855.0000\n","Epoch 45/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 540014.3750\n","Epoch 45: val_loss did not improve from 676025.43750\n","75/75 [==============================] - 1s 16ms/step - loss: 550872.7500 - val_loss: 2319566.5000\n","Epoch 46/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 540470.3750\n","Epoch 46: val_loss did not improve from 676025.43750\n","75/75 [==============================] - 1s 16ms/step - loss: 549722.2500 - val_loss: 862088.5000\n","Epoch 47/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 480947.9375\n","Epoch 47: val_loss did not improve from 676025.43750\n","75/75 [==============================] - 1s 16ms/step - loss: 493178.7500 - val_loss: 769424.8125\n","Epoch 48/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 511926.8125\n","Epoch 48: val_loss improved from 676025.43750 to 573667.25000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-5-1000-15.h5\n","75/75 [==============================] - 2s 22ms/step - loss: 527068.6250 - val_loss: 573667.2500\n","Epoch 49/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 627156.2500\n","Epoch 49: val_loss improved from 573667.25000 to 527111.00000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-5-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 633872.8750 - val_loss: 527111.0000\n","Epoch 50/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 524432.8750\n","Epoch 50: val_loss improved from 527111.00000 to 513984.34375, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-5-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 531653.1875 - val_loss: 513984.3438\n","Epoch 51/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 509902.5000\n","Epoch 51: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 523855.5000 - val_loss: 654625.3750\n","Epoch 52/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 494804.5000\n","Epoch 52: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 509305.2812 - val_loss: 1088852.6250\n","Epoch 53/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 517070.1875\n","Epoch 53: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 530594.7500 - val_loss: 1194688.6250\n","Epoch 54/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 526122.4375\n","Epoch 54: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 552096.1875 - val_loss: 2070399.3750\n","Epoch 55/1000\n","76/75 [==============================] - ETA: 0s - loss: 563965.0625\n","Epoch 55: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 563965.0625 - val_loss: 1908536.3750\n","Epoch 56/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 622378.1875\n","Epoch 56: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 642624.1250 - val_loss: 2959584.2500\n","Epoch 57/1000\n","76/75 [==============================] - ETA: 0s - loss: 766755.3125\n","Epoch 57: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 17ms/step - loss: 766755.3125 - val_loss: 3563697.2500\n","Epoch 58/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 763379.8125\n","Epoch 58: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 763097.0000 - val_loss: 2340210.7500\n","Epoch 59/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 570296.1875\n","Epoch 59: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 581836.8125 - val_loss: 1522652.3750\n","Epoch 60/1000\n","76/75 [==============================] - ETA: 0s - loss: 519769.9375\n","Epoch 60: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 519769.9375 - val_loss: 631159.1250\n","Epoch 61/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 516427.1562\n","Epoch 61: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 525539.0000 - val_loss: 556245.4375\n","Epoch 62/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 464379.2188\n","Epoch 62: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 474249.7500 - val_loss: 583303.9375\n","Epoch 63/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 413246.6562\n","Epoch 63: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 424562.0938 - val_loss: 723030.1250\n","Epoch 64/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 390223.0312\n","Epoch 64: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 406222.8438 - val_loss: 1196056.2500\n","Epoch 65/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 475782.0938\n","Epoch 65: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 489591.2188 - val_loss: 1083164.0000\n","Epoch 66/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 448553.8125\n","Epoch 66: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 463202.8438 - val_loss: 1167176.8750\n","Epoch 67/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 498742.8750\n","Epoch 67: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 525019.1250 - val_loss: 1286304.5000\n","Epoch 68/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 446390.5625\n","Epoch 68: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 463046.9375 - val_loss: 1301972.7500\n","Epoch 69/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 419473.4375\n","Epoch 69: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 427643.7188 - val_loss: 1281496.6250\n","Epoch 70/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 414079.0938\n","Epoch 70: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 426291.4688 - val_loss: 1572170.5000\n","Epoch 71/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 410777.4688\n","Epoch 71: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 431180.5312 - val_loss: 1007203.2500\n","Epoch 72/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 425412.0312\n","Epoch 72: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 443560.1562 - val_loss: 1221681.2500\n","Epoch 73/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 401543.0000\n","Epoch 73: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 420829.8438 - val_loss: 1370654.6250\n","Epoch 74/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 486379.7812\n","Epoch 74: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 499063.3750 - val_loss: 1772290.6250\n","Epoch 75/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 501396.4062\n","Epoch 75: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 510791.3750 - val_loss: 2377697.0000\n","Epoch 76/1000\n","76/75 [==============================] - ETA: 0s - loss: 587379.0625\n","Epoch 76: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 587379.0625 - val_loss: 1922971.5000\n","Epoch 77/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 461298.8125\n","Epoch 77: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 475266.7500 - val_loss: 1698930.3750\n","Epoch 78/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 440184.2188\n","Epoch 78: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 452961.4688 - val_loss: 1448660.7500\n","Epoch 79/1000\n","75/75 [============================>.] - ETA: 0s - loss: 397474.3750\n","Epoch 79: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 17ms/step - loss: 400781.9062 - val_loss: 705050.3125\n","Epoch 80/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 505845.0625\n","Epoch 80: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 517886.2500 - val_loss: 643449.8125\n","Epoch 81/1000\n","75/75 [============================>.] - ETA: 0s - loss: 531914.3750\n","Epoch 81: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 17ms/step - loss: 534953.9375 - val_loss: 564349.3125\n","Epoch 82/1000\n","76/75 [==============================] - ETA: 0s - loss: 518932.1875\n","Epoch 82: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 518932.1875 - val_loss: 590427.7500\n","Epoch 83/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 433229.1562\n","Epoch 83: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 446469.4375 - val_loss: 532850.0000\n","Epoch 84/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 457472.6875\n","Epoch 84: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 471955.0938 - val_loss: 854483.3750\n","Epoch 85/1000\n","76/75 [==============================] - ETA: 0s - loss: 500832.0938\n","Epoch 85: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 17ms/step - loss: 500832.0938 - val_loss: 786182.6875\n","Epoch 86/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 575248.5000\n","Epoch 86: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 589863.1875 - val_loss: 611665.2500\n","Epoch 87/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 448371.4062\n","Epoch 87: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 478077.5625 - val_loss: 938853.1250\n","Epoch 88/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 498529.7188\n","Epoch 88: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 518147.2188 - val_loss: 1221571.6250\n","Epoch 89/1000\n","76/75 [==============================] - ETA: 0s - loss: 724959.9375\n","Epoch 89: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 724959.9375 - val_loss: 1139781.6250\n","Epoch 90/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 692731.8125\n","Epoch 90: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 712704.8750 - val_loss: 810593.9375\n","Epoch 91/1000\n","76/75 [==============================] - ETA: 0s - loss: 603873.3750\n","Epoch 91: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 603873.3750 - val_loss: 1913892.0000\n","Epoch 92/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 674045.1875\n","Epoch 92: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 705066.0625 - val_loss: 2225288.7500\n","Epoch 93/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 580241.5625\n","Epoch 93: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 602119.7500 - val_loss: 2843826.0000\n","Epoch 94/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 658310.8125\n","Epoch 94: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 662870.5625 - val_loss: 1902290.7500\n","Epoch 95/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 483607.3125\n","Epoch 95: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 496372.0312 - val_loss: 2176371.7500\n","Epoch 96/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 528827.0625\n","Epoch 96: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 543337.6250 - val_loss: 1457434.0000\n","Epoch 97/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 400063.9688\n","Epoch 97: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 412111.1250 - val_loss: 860956.3750\n","Epoch 98/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 386802.7500\n","Epoch 98: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 399011.2500 - val_loss: 702524.8125\n","Epoch 99/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 422181.0625\n","Epoch 99: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 436887.3438 - val_loss: 719325.7500\n","Epoch 100/1000\n","76/75 [==============================] - ETA: 0s - loss: 427075.0000\n","Epoch 100: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 427075.0000 - val_loss: 628894.9375\n","Epoch 101/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 421825.0000\n","Epoch 101: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 435739.1875 - val_loss: 717535.6875\n","Epoch 102/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 442184.4688\n","Epoch 102: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 463632.9375 - val_loss: 1293372.1250\n","Epoch 103/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 446497.5938\n","Epoch 103: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 466616.4375 - val_loss: 2135109.7500\n","Epoch 104/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 481857.5000\n","Epoch 104: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 500090.1875 - val_loss: 2192400.5000\n","Epoch 105/1000\n","76/75 [==============================] - ETA: 0s - loss: 498281.2812\n","Epoch 105: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 498281.2812 - val_loss: 2508964.7500\n","Epoch 106/1000\n","76/75 [==============================] - ETA: 0s - loss: 557837.0000\n","Epoch 106: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 557837.0000 - val_loss: 1782160.5000\n","Epoch 107/1000\n","76/75 [==============================] - ETA: 0s - loss: 472878.7188\n","Epoch 107: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 472878.7188 - val_loss: 1572140.1250\n","Epoch 108/1000\n","76/75 [==============================] - ETA: 0s - loss: 443348.2500\n","Epoch 108: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 443348.2500 - val_loss: 655061.8750\n","Epoch 109/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 429782.5625\n","Epoch 109: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 446051.9062 - val_loss: 701581.7500\n","Epoch 110/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 389981.5312\n","Epoch 110: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 402389.9688 - val_loss: 624513.3750\n","Epoch 111/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 379826.0312\n","Epoch 111: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 398628.6875 - val_loss: 946052.7500\n","Epoch 112/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 364663.9062\n","Epoch 112: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 382952.3438 - val_loss: 886393.1875\n","Epoch 113/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 352331.5000\n","Epoch 113: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 369364.8750 - val_loss: 1414900.3750\n","Epoch 114/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 442513.9375\n","Epoch 114: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 453789.8750 - val_loss: 948962.2500\n","Epoch 115/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 449673.9062\n","Epoch 115: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 458037.5000 - val_loss: 1241762.7500\n","Epoch 116/1000\n","76/75 [==============================] - ETA: 0s - loss: 613331.5625\n","Epoch 116: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 613331.5625 - val_loss: 1099257.7500\n","Epoch 117/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 593710.3750\n","Epoch 117: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 606150.6250 - val_loss: 619664.0625\n","Epoch 118/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 430622.5625\n","Epoch 118: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 443305.9375 - val_loss: 1099910.0000\n","Epoch 119/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 455231.3125\n","Epoch 119: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 470388.0312 - val_loss: 1442735.5000\n","Epoch 120/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 428302.0938\n","Epoch 120: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 451886.0312 - val_loss: 2081309.5000\n","Epoch 121/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 595165.2500\n","Epoch 121: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 616237.8125 - val_loss: 2812928.5000\n","Epoch 122/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 575473.2500\n","Epoch 122: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 588194.4375 - val_loss: 3014364.2500\n","Epoch 123/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 633647.3125\n","Epoch 123: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 652899.5625 - val_loss: 2423977.7500\n","Epoch 124/1000\n","75/75 [============================>.] - ETA: 0s - loss: 510581.5938\n","Epoch 124: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 17ms/step - loss: 516522.6250 - val_loss: 1477685.5000\n","Epoch 125/1000\n","76/75 [==============================] - ETA: 0s - loss: 438371.1875\n","Epoch 125: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 438371.1875 - val_loss: 703074.8750\n","Epoch 126/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 441790.4062\n","Epoch 126: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 456535.7812 - val_loss: 651636.1875\n","Epoch 127/1000\n","76/75 [==============================] - ETA: 0s - loss: 434997.1875\n","Epoch 127: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 434997.1875 - val_loss: 922441.8750\n","Epoch 128/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 410056.4375\n","Epoch 128: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 418745.5625 - val_loss: 632802.5625\n","Epoch 129/1000\n","76/75 [==============================] - ETA: 0s - loss: 525205.6250\n","Epoch 129: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 525205.6250 - val_loss: 934029.3125\n","Epoch 130/1000\n","76/75 [==============================] - ETA: 0s - loss: 444089.0938\n","Epoch 130: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 444089.0938 - val_loss: 885835.1875\n","Epoch 131/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 382388.9062\n","Epoch 131: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 395185.3125 - val_loss: 1208852.1250\n","Epoch 132/1000\n","75/75 [============================>.] - ETA: 0s - loss: 403704.4688\n","Epoch 132: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 17ms/step - loss: 411193.5312 - val_loss: 1630545.7500\n","Epoch 133/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 425045.3438\n","Epoch 133: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 447715.0000 - val_loss: 2012652.0000\n","Epoch 134/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 411655.2188\n","Epoch 134: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 432071.9688 - val_loss: 1641348.3750\n","Epoch 135/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 403355.7500\n","Epoch 135: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 419454.4375 - val_loss: 2030650.8750\n","Epoch 136/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 457927.0000\n","Epoch 136: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 468515.3125 - val_loss: 1713366.2500\n","Epoch 137/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 384191.4688\n","Epoch 137: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 403141.6875 - val_loss: 1420537.7500\n","Epoch 138/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 365823.6250\n","Epoch 138: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 383883.5938 - val_loss: 811586.0000\n","Epoch 139/1000\n","75/75 [============================>.] - ETA: 0s - loss: 413956.3438\n","Epoch 139: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 18ms/step - loss: 418407.1562 - val_loss: 863112.8750\n","Epoch 140/1000\n","75/75 [============================>.] - ETA: 0s - loss: 383302.8438\n","Epoch 140: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 19ms/step - loss: 389027.2188 - val_loss: 729270.2500\n","Epoch 141/1000\n","76/75 [==============================] - ETA: 0s - loss: 377913.1250\n","Epoch 141: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 18ms/step - loss: 377913.1250 - val_loss: 944023.7500\n","Epoch 142/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 391408.9375\n","Epoch 142: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 408515.9375 - val_loss: 1143205.6250\n","Epoch 143/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 411924.8438\n","Epoch 143: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 423383.7188 - val_loss: 1301656.2500\n","Epoch 144/1000\n","76/75 [==============================] - ETA: 0s - loss: 471103.4688\n","Epoch 144: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 471103.4688 - val_loss: 976662.1875\n","Epoch 145/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 384389.1250\n","Epoch 145: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 397045.9688 - val_loss: 1257809.1250\n","Epoch 146/1000\n","75/75 [============================>.] - ETA: 0s - loss: 422493.5312\n","Epoch 146: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 426363.9688 - val_loss: 1244542.1250\n","Epoch 147/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 431810.1250\n","Epoch 147: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 444174.3438 - val_loss: 1455464.3750\n","Epoch 148/1000\n","76/75 [==============================] - ETA: 0s - loss: 475642.9688\n","Epoch 148: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 475642.9688 - val_loss: 1171404.6250\n","Epoch 149/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 480281.4688\n","Epoch 149: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 16ms/step - loss: 493772.8438 - val_loss: 1257366.7500\n","Epoch 150/1000\n","76/75 [==============================] - ETA: 0s - loss: 497982.4688\n","Epoch 150: val_loss did not improve from 513984.34375\n","75/75 [==============================] - 1s 17ms/step - loss: 497982.4688 - val_loss: 1524289.8750\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1447/1447 [00:00<00:00, 22997.07it/s]\n","100%|██████████| 1419/1419 [00:00<00:00, 22648.27it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/1000\n","76/75 [==============================] - ETA: 0s - loss: 35812016.0000\n","Epoch 1: val_loss improved from inf to 44646400.00000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-6-1000-15.h5\n","75/75 [==============================] - 6s 25ms/step - loss: 35812016.0000 - val_loss: 44646400.0000\n","Epoch 2/1000\n","76/75 [==============================] - ETA: 0s - loss: 22720552.0000\n","Epoch 2: val_loss improved from 44646400.00000 to 28581850.00000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-6-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 22720552.0000 - val_loss: 28581850.0000\n","Epoch 3/1000\n","76/75 [==============================] - ETA: 0s - loss: 19829944.0000\n","Epoch 3: val_loss improved from 28581850.00000 to 26431098.00000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-6-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 19829944.0000 - val_loss: 26431098.0000\n","Epoch 4/1000\n","76/75 [==============================] - ETA: 0s - loss: 18848366.0000\n","Epoch 4: val_loss improved from 26431098.00000 to 25364686.00000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-6-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 18848366.0000 - val_loss: 25364686.0000\n","Epoch 5/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 17542612.0000\n","Epoch 5: val_loss improved from 25364686.00000 to 23309806.00000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-6-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 16996668.0000 - val_loss: 23309806.0000\n","Epoch 6/1000\n","75/75 [============================>.] - ETA: 0s - loss: 15727839.0000\n","Epoch 6: val_loss improved from 23309806.00000 to 22394704.00000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-6-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 15601478.0000 - val_loss: 22394704.0000\n","Epoch 7/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 14526824.0000\n","Epoch 7: val_loss improved from 22394704.00000 to 22176522.00000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-6-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 14053214.0000 - val_loss: 22176522.0000\n","Epoch 8/1000\n","76/75 [==============================] - ETA: 0s - loss: 12665677.0000\n","Epoch 8: val_loss improved from 22176522.00000 to 22169142.00000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-6-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 12665677.0000 - val_loss: 22169142.0000\n","Epoch 9/1000\n","76/75 [==============================] - ETA: 0s - loss: 12177911.0000\n","Epoch 9: val_loss did not improve from 22169142.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 12177911.0000 - val_loss: 22273206.0000\n","Epoch 10/1000\n","75/75 [============================>.] - ETA: 0s - loss: 11553801.0000\n","Epoch 10: val_loss did not improve from 22169142.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 11467054.0000 - val_loss: 22957850.0000\n","Epoch 11/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 11016395.0000\n","Epoch 11: val_loss did not improve from 22169142.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 10706712.0000 - val_loss: 23816216.0000\n","Epoch 12/1000\n","75/75 [============================>.] - ETA: 0s - loss: 10631158.0000\n","Epoch 12: val_loss did not improve from 22169142.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 10551629.0000 - val_loss: 28328894.0000\n","Epoch 13/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 10933565.0000\n","Epoch 13: val_loss did not improve from 22169142.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 10599170.0000 - val_loss: 27978154.0000\n","Epoch 14/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 10853766.0000\n","Epoch 14: val_loss did not improve from 22169142.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 10525581.0000 - val_loss: 27324920.0000\n","Epoch 15/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 9974280.0000 \n","Epoch 15: val_loss did not improve from 22169142.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 9669318.0000 - val_loss: 26621970.0000\n","Epoch 16/1000\n","76/75 [==============================] - ETA: 0s - loss: 9415673.0000\n","Epoch 16: val_loss did not improve from 22169142.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 9415673.0000 - val_loss: 26282378.0000\n","Epoch 17/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 11169671.0000\n","Epoch 17: val_loss did not improve from 22169142.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 10834357.0000 - val_loss: 24708454.0000\n","Epoch 18/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 10698673.0000\n","Epoch 18: val_loss did not improve from 22169142.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 10357349.0000 - val_loss: 25245062.0000\n","Epoch 19/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 10127561.0000\n","Epoch 19: val_loss did not improve from 22169142.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 9813047.0000 - val_loss: 25276768.0000\n","Epoch 20/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 9390607.0000\n","Epoch 20: val_loss did not improve from 22169142.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 9105897.0000 - val_loss: 25726836.0000\n","Epoch 21/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 9471918.0000\n","Epoch 21: val_loss did not improve from 22169142.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 9175280.0000 - val_loss: 25563014.0000\n","Epoch 22/1000\n","76/75 [==============================] - ETA: 0s - loss: 9362465.0000\n","Epoch 22: val_loss did not improve from 22169142.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 9362465.0000 - val_loss: 24851336.0000\n","Epoch 23/1000\n","75/75 [============================>.] - ETA: 0s - loss: 8816664.0000\n","Epoch 23: val_loss did not improve from 22169142.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 8750470.0000 - val_loss: 25437318.0000\n","Epoch 24/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 9249264.0000\n","Epoch 24: val_loss did not improve from 22169142.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 8948935.0000 - val_loss: 26240370.0000\n","Epoch 25/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 9152423.0000\n","Epoch 25: val_loss did not improve from 22169142.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 8863303.0000 - val_loss: 25782906.0000\n","Epoch 26/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 8701243.0000\n","Epoch 26: val_loss did not improve from 22169142.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 8425085.0000 - val_loss: 27501578.0000\n","Epoch 27/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 8962676.0000\n","Epoch 27: val_loss did not improve from 22169142.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 8680076.0000 - val_loss: 26172650.0000\n","Epoch 28/1000\n","76/75 [==============================] - ETA: 0s - loss: 8420297.0000\n","Epoch 28: val_loss did not improve from 22169142.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 8420297.0000 - val_loss: 26281416.0000\n","Epoch 29/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 8262226.0000\n","Epoch 29: val_loss did not improve from 22169142.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 8010422.5000 - val_loss: 27699338.0000\n","Epoch 30/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 8683108.0000\n","Epoch 30: val_loss did not improve from 22169142.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 8404392.0000 - val_loss: 26277348.0000\n","Epoch 31/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 8337428.5000\n","Epoch 31: val_loss did not improve from 22169142.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 8072481.5000 - val_loss: 26049142.0000\n","Epoch 32/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 8084247.0000\n","Epoch 32: val_loss did not improve from 22169142.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 7829597.0000 - val_loss: 27368658.0000\n","Epoch 33/1000\n","75/75 [============================>.] - ETA: 0s - loss: 7855044.0000\n","Epoch 33: val_loss did not improve from 22169142.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 7795200.5000 - val_loss: 25711152.0000\n","Epoch 34/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 8341824.0000\n","Epoch 34: val_loss did not improve from 22169142.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 8077220.0000 - val_loss: 26765028.0000\n","Epoch 35/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 8065714.0000\n","Epoch 35: val_loss did not improve from 22169142.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 7810859.0000 - val_loss: 25935484.0000\n","Epoch 36/1000\n","76/75 [==============================] - ETA: 0s - loss: 7596260.5000\n","Epoch 36: val_loss did not improve from 22169142.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 7596260.5000 - val_loss: 27085198.0000\n","Epoch 37/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 8416641.0000\n","Epoch 37: val_loss did not improve from 22169142.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 8148653.5000 - val_loss: 25468600.0000\n","Epoch 38/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 8151519.5000\n","Epoch 38: val_loss did not improve from 22169142.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 7891345.5000 - val_loss: 25141312.0000\n","Epoch 39/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 7674105.5000\n","Epoch 39: val_loss did not improve from 22169142.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 7428321.5000 - val_loss: 23657082.0000\n","Epoch 40/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 7764536.0000\n","Epoch 40: val_loss did not improve from 22169142.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 7523351.0000 - val_loss: 23353734.0000\n","Epoch 41/1000\n","76/75 [==============================] - ETA: 0s - loss: 7363373.5000\n","Epoch 41: val_loss did not improve from 22169142.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 7363373.5000 - val_loss: 25113826.0000\n","Epoch 42/1000\n","76/75 [==============================] - ETA: 0s - loss: 7367662.5000\n","Epoch 42: val_loss did not improve from 22169142.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 7367662.5000 - val_loss: 23234144.0000\n","Epoch 43/1000\n","76/75 [==============================] - ETA: 0s - loss: 7070737.5000\n","Epoch 43: val_loss did not improve from 22169142.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 7070737.5000 - val_loss: 24859292.0000\n","Epoch 44/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 7117974.5000\n","Epoch 44: val_loss did not improve from 22169142.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6892791.0000 - val_loss: 24417368.0000\n","Epoch 45/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 7544398.0000\n","Epoch 45: val_loss did not improve from 22169142.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 7301339.0000 - val_loss: 26352078.0000\n","Epoch 46/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 7623037.5000\n","Epoch 46: val_loss improved from 22169142.00000 to 21683282.00000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-6-1000-15.h5\n","75/75 [==============================] - 2s 22ms/step - loss: 7377825.0000 - val_loss: 21683282.0000\n","Epoch 47/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 7009330.0000\n","Epoch 47: val_loss did not improve from 21683282.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 6787569.0000 - val_loss: 26701738.0000\n","Epoch 48/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 7089757.5000\n","Epoch 48: val_loss did not improve from 21683282.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6866618.5000 - val_loss: 25169166.0000\n","Epoch 49/1000\n","76/75 [==============================] - ETA: 0s - loss: 6918763.5000\n","Epoch 49: val_loss did not improve from 21683282.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 6918763.5000 - val_loss: 25655782.0000\n","Epoch 50/1000\n","76/75 [==============================] - ETA: 0s - loss: 6613843.0000\n","Epoch 50: val_loss did not improve from 21683282.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 6613843.0000 - val_loss: 25317750.0000\n","Epoch 51/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6711336.0000\n","Epoch 51: val_loss did not improve from 21683282.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6496034.5000 - val_loss: 24162650.0000\n","Epoch 52/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 7362632.0000\n","Epoch 52: val_loss did not improve from 21683282.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 7125285.0000 - val_loss: 23881206.0000\n","Epoch 53/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 7504092.5000\n","Epoch 53: val_loss did not improve from 21683282.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 7264468.5000 - val_loss: 25863360.0000\n","Epoch 54/1000\n","75/75 [============================>.] - ETA: 0s - loss: 7742081.5000\n","Epoch 54: val_loss did not improve from 21683282.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 7677106.5000 - val_loss: 22416376.0000\n","Epoch 55/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 7172254.0000\n","Epoch 55: val_loss did not improve from 21683282.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6945326.0000 - val_loss: 23967516.0000\n","Epoch 56/1000\n","76/75 [==============================] - ETA: 0s - loss: 6768760.5000\n","Epoch 56: val_loss did not improve from 21683282.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6768760.5000 - val_loss: 23479358.0000\n","Epoch 57/1000\n","76/75 [==============================] - ETA: 0s - loss: 6687496.5000\n","Epoch 57: val_loss did not improve from 21683282.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 6687496.5000 - val_loss: 26794672.0000\n","Epoch 58/1000\n","76/75 [==============================] - ETA: 0s - loss: 6547700.5000\n","Epoch 58: val_loss did not improve from 21683282.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6547700.5000 - val_loss: 24270400.0000\n","Epoch 59/1000\n","76/75 [==============================] - ETA: 0s - loss: 6372902.0000\n","Epoch 59: val_loss did not improve from 21683282.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6372902.0000 - val_loss: 22565198.0000\n","Epoch 60/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6780700.0000\n","Epoch 60: val_loss did not improve from 21683282.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6564234.0000 - val_loss: 24863660.0000\n","Epoch 61/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6782058.5000\n","Epoch 61: val_loss did not improve from 21683282.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6564040.5000 - val_loss: 23684436.0000\n","Epoch 62/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6955581.5000\n","Epoch 62: val_loss did not improve from 21683282.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6735682.0000 - val_loss: 23526226.0000\n","Epoch 63/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6842516.5000\n","Epoch 63: val_loss did not improve from 21683282.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6623922.0000 - val_loss: 26247740.0000\n","Epoch 64/1000\n","76/75 [==============================] - ETA: 0s - loss: 7435817.0000\n","Epoch 64: val_loss improved from 21683282.00000 to 21138204.00000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-6-1000-15.h5\n","75/75 [==============================] - 2s 24ms/step - loss: 7435817.0000 - val_loss: 21138204.0000\n","Epoch 65/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 7190950.0000\n","Epoch 65: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6962730.0000 - val_loss: 25753654.0000\n","Epoch 66/1000\n","76/75 [==============================] - ETA: 0s - loss: 6637732.0000\n","Epoch 66: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6637732.0000 - val_loss: 24928824.0000\n","Epoch 67/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6520495.5000\n","Epoch 67: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6314024.0000 - val_loss: 23304584.0000\n","Epoch 68/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6620276.5000\n","Epoch 68: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6409292.5000 - val_loss: 25473042.0000\n","Epoch 69/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 7108987.0000\n","Epoch 69: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6881442.5000 - val_loss: 22096578.0000\n","Epoch 70/1000\n","75/75 [============================>.] - ETA: 0s - loss: 6669239.5000\n","Epoch 70: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6613759.0000 - val_loss: 24044420.0000\n","Epoch 71/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6997535.0000\n","Epoch 71: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6773026.0000 - val_loss: 24111272.0000\n","Epoch 72/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 7615968.5000\n","Epoch 72: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 7371899.5000 - val_loss: 22721870.0000\n","Epoch 73/1000\n","76/75 [==============================] - ETA: 0s - loss: 7075441.0000\n","Epoch 73: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 7075441.0000 - val_loss: 22724276.0000\n","Epoch 74/1000\n","76/75 [==============================] - ETA: 0s - loss: 6696948.0000\n","Epoch 74: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6696948.0000 - val_loss: 25298966.0000\n","Epoch 75/1000\n","76/75 [==============================] - ETA: 0s - loss: 6408000.5000\n","Epoch 75: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6408000.5000 - val_loss: 25363020.0000\n","Epoch 76/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6629495.0000\n","Epoch 76: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6420605.0000 - val_loss: 23208888.0000\n","Epoch 77/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6539723.0000\n","Epoch 77: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6335024.5000 - val_loss: 25947198.0000\n","Epoch 78/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 7207497.0000\n","Epoch 78: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6973157.0000 - val_loss: 25307592.0000\n","Epoch 79/1000\n","76/75 [==============================] - ETA: 0s - loss: 6770207.5000\n","Epoch 79: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6770207.5000 - val_loss: 21436298.0000\n","Epoch 80/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6608047.0000\n","Epoch 80: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6397053.0000 - val_loss: 24482140.0000\n","Epoch 81/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6390975.0000\n","Epoch 81: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6189187.5000 - val_loss: 23815360.0000\n","Epoch 82/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6376563.5000\n","Epoch 82: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6172406.0000 - val_loss: 24062674.0000\n","Epoch 83/1000\n","76/75 [==============================] - ETA: 0s - loss: 6261853.5000\n","Epoch 83: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6261853.5000 - val_loss: 24217226.0000\n","Epoch 84/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 7329601.0000\n","Epoch 84: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 7092051.0000 - val_loss: 25209640.0000\n","Epoch 85/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 7589949.5000\n","Epoch 85: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 7355953.0000 - val_loss: 21776924.0000\n","Epoch 86/1000\n","75/75 [============================>.] - ETA: 0s - loss: 6499315.0000\n","Epoch 86: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6445563.0000 - val_loss: 24839994.0000\n","Epoch 87/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6557666.5000\n","Epoch 87: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6348385.0000 - val_loss: 22400590.0000\n","Epoch 88/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6467077.0000\n","Epoch 88: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6259884.0000 - val_loss: 24085374.0000\n","Epoch 89/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6303424.5000\n","Epoch 89: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6103453.0000 - val_loss: 22773734.0000\n","Epoch 90/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6550022.0000\n","Epoch 90: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6337785.5000 - val_loss: 24119814.0000\n","Epoch 91/1000\n","76/75 [==============================] - ETA: 0s - loss: 6457074.0000\n","Epoch 91: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6457074.0000 - val_loss: 23339642.0000\n","Epoch 92/1000\n","76/75 [==============================] - ETA: 0s - loss: 6598307.5000\n","Epoch 92: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6598307.5000 - val_loss: 23738324.0000\n","Epoch 93/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 7006707.0000\n","Epoch 93: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6788057.5000 - val_loss: 21659452.0000\n","Epoch 94/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6986403.5000\n","Epoch 94: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6769092.0000 - val_loss: 24741416.0000\n","Epoch 95/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6911976.0000\n","Epoch 95: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6694328.5000 - val_loss: 23439534.0000\n","Epoch 96/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6462444.0000\n","Epoch 96: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6258331.5000 - val_loss: 24103854.0000\n","Epoch 97/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6402023.5000\n","Epoch 97: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6201594.0000 - val_loss: 21851582.0000\n","Epoch 98/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6249107.0000\n","Epoch 98: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6049328.5000 - val_loss: 23482142.0000\n","Epoch 99/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6760345.0000\n","Epoch 99: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 6551517.0000 - val_loss: 23157926.0000\n","Epoch 100/1000\n","74/75 [============================>.] - ETA: 0s - loss: 6263009.5000\n","Epoch 100: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 19ms/step - loss: 6135750.0000 - val_loss: 22556536.0000\n","Epoch 101/1000\n","75/75 [============================>.] - ETA: 0s - loss: 6223661.5000\n","Epoch 101: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 19ms/step - loss: 6173204.5000 - val_loss: 23131252.0000\n","Epoch 102/1000\n","75/75 [============================>.] - ETA: 0s - loss: 6413160.5000\n","Epoch 102: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 6360033.0000 - val_loss: 23892900.0000\n","Epoch 103/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 7122580.5000\n","Epoch 103: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6904531.0000 - val_loss: 22493942.0000\n","Epoch 104/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 7257923.5000\n","Epoch 104: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 7024662.0000 - val_loss: 23452358.0000\n","Epoch 105/1000\n","76/75 [==============================] - ETA: 0s - loss: 6237585.5000\n","Epoch 105: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6237585.5000 - val_loss: 23290714.0000\n","Epoch 106/1000\n","75/75 [============================>.] - ETA: 0s - loss: 6404723.0000\n","Epoch 106: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6353202.0000 - val_loss: 23296522.0000\n","Epoch 107/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6767716.5000\n","Epoch 107: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6563819.5000 - val_loss: 22256756.0000\n","Epoch 108/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6640860.5000\n","Epoch 108: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6428317.0000 - val_loss: 24008262.0000\n","Epoch 109/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6376698.0000\n","Epoch 109: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6172061.0000 - val_loss: 22967582.0000\n","Epoch 110/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6188308.5000\n","Epoch 110: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 5993643.0000 - val_loss: 23887512.0000\n","Epoch 111/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6110977.0000\n","Epoch 111: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 5923243.5000 - val_loss: 21628042.0000\n","Epoch 112/1000\n","76/75 [==============================] - ETA: 0s - loss: 6082935.5000\n","Epoch 112: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6082935.5000 - val_loss: 24511316.0000\n","Epoch 113/1000\n","76/75 [==============================] - ETA: 0s - loss: 6046024.5000\n","Epoch 113: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6046024.5000 - val_loss: 22831154.0000\n","Epoch 114/1000\n","76/75 [==============================] - ETA: 0s - loss: 6241384.0000\n","Epoch 114: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6241384.0000 - val_loss: 21516370.0000\n","Epoch 115/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6969312.5000\n","Epoch 115: val_loss did not improve from 21138204.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6746826.5000 - val_loss: 25227078.0000\n","Epoch 116/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 8046383.5000\n","Epoch 116: val_loss improved from 21138204.00000 to 20158778.00000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-6-1000-15.h5\n","75/75 [==============================] - 2s 21ms/step - loss: 7798375.5000 - val_loss: 20158778.0000\n","Epoch 117/1000\n","76/75 [==============================] - ETA: 0s - loss: 6644417.0000\n","Epoch 117: val_loss did not improve from 20158778.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6644417.0000 - val_loss: 25109978.0000\n","Epoch 118/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6346767.0000\n","Epoch 118: val_loss did not improve from 20158778.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6146732.0000 - val_loss: 21241004.0000\n","Epoch 119/1000\n","76/75 [==============================] - ETA: 0s - loss: 5999381.5000\n","Epoch 119: val_loss did not improve from 20158778.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 5999381.5000 - val_loss: 22994730.0000\n","Epoch 120/1000\n","76/75 [==============================] - ETA: 0s - loss: 5952520.0000\n","Epoch 120: val_loss did not improve from 20158778.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 5952520.0000 - val_loss: 20694050.0000\n","Epoch 121/1000\n","76/75 [==============================] - ETA: 0s - loss: 5941209.0000\n","Epoch 121: val_loss did not improve from 20158778.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 5941209.0000 - val_loss: 22887606.0000\n","Epoch 122/1000\n","76/75 [==============================] - ETA: 0s - loss: 6496053.0000\n","Epoch 122: val_loss did not improve from 20158778.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 6496053.0000 - val_loss: 21275154.0000\n","Epoch 123/1000\n","76/75 [==============================] - ETA: 0s - loss: 6905181.5000\n","Epoch 123: val_loss did not improve from 20158778.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 6905181.5000 - val_loss: 23255626.0000\n","Epoch 124/1000\n","76/75 [==============================] - ETA: 0s - loss: 6921349.5000\n","Epoch 124: val_loss improved from 20158778.00000 to 20109940.00000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-6-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 6921349.5000 - val_loss: 20109940.0000\n","Epoch 125/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6968190.0000\n","Epoch 125: val_loss did not improve from 20109940.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 6743854.0000 - val_loss: 23917656.0000\n","Epoch 126/1000\n","76/75 [==============================] - ETA: 0s - loss: 6211639.5000\n","Epoch 126: val_loss did not improve from 20109940.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6211639.5000 - val_loss: 21661680.0000\n","Epoch 127/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6360246.5000\n","Epoch 127: val_loss did not improve from 20109940.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6158327.5000 - val_loss: 22175030.0000\n","Epoch 128/1000\n","75/75 [============================>.] - ETA: 0s - loss: 6065437.5000\n","Epoch 128: val_loss did not improve from 20109940.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 6015045.0000 - val_loss: 21480468.0000\n","Epoch 129/1000\n","76/75 [==============================] - ETA: 0s - loss: 6231343.5000\n","Epoch 129: val_loss did not improve from 20109940.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 6231343.5000 - val_loss: 21957838.0000\n","Epoch 130/1000\n","76/75 [==============================] - ETA: 0s - loss: 6643291.5000\n","Epoch 130: val_loss did not improve from 20109940.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6643291.5000 - val_loss: 20677256.0000\n","Epoch 131/1000\n","76/75 [==============================] - ETA: 0s - loss: 6386328.5000\n","Epoch 131: val_loss did not improve from 20109940.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6386328.5000 - val_loss: 23336030.0000\n","Epoch 132/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6480680.0000\n","Epoch 132: val_loss did not improve from 20109940.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6273718.5000 - val_loss: 21064366.0000\n","Epoch 133/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6252556.0000\n","Epoch 133: val_loss did not improve from 20109940.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6053233.0000 - val_loss: 23123596.0000\n","Epoch 134/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6245443.5000\n","Epoch 134: val_loss did not improve from 20109940.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6045708.0000 - val_loss: 21806588.0000\n","Epoch 135/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6173178.5000\n","Epoch 135: val_loss did not improve from 20109940.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 5979424.5000 - val_loss: 22848116.0000\n","Epoch 136/1000\n","76/75 [==============================] - ETA: 0s - loss: 6154595.5000\n","Epoch 136: val_loss did not improve from 20109940.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6154595.5000 - val_loss: 21758472.0000\n","Epoch 137/1000\n","76/75 [==============================] - ETA: 0s - loss: 6088804.5000\n","Epoch 137: val_loss did not improve from 20109940.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 6088804.5000 - val_loss: 23700850.0000\n","Epoch 138/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6534409.5000\n","Epoch 138: val_loss did not improve from 20109940.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6340804.5000 - val_loss: 20645066.0000\n","Epoch 139/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6383942.0000\n","Epoch 139: val_loss did not improve from 20109940.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6180633.5000 - val_loss: 22777418.0000\n","Epoch 140/1000\n","76/75 [==============================] - ETA: 0s - loss: 6486797.0000\n","Epoch 140: val_loss did not improve from 20109940.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6486797.0000 - val_loss: 21646414.0000\n","Epoch 141/1000\n","76/75 [==============================] - ETA: 0s - loss: 6145693.5000\n","Epoch 141: val_loss did not improve from 20109940.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6145693.5000 - val_loss: 24240378.0000\n","Epoch 142/1000\n","75/75 [============================>.] - ETA: 0s - loss: 6793621.0000\n","Epoch 142: val_loss did not improve from 20109940.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 6739913.0000 - val_loss: 20135124.0000\n","Epoch 143/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6701686.5000\n","Epoch 143: val_loss did not improve from 20109940.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6488205.0000 - val_loss: 23222446.0000\n","Epoch 144/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6295263.0000\n","Epoch 144: val_loss did not improve from 20109940.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6094283.0000 - val_loss: 21214024.0000\n","Epoch 145/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6507774.0000\n","Epoch 145: val_loss did not improve from 20109940.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6300122.5000 - val_loss: 22877260.0000\n","Epoch 146/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6212683.0000\n","Epoch 146: val_loss did not improve from 20109940.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6020419.0000 - val_loss: 21063824.0000\n","Epoch 147/1000\n","76/75 [==============================] - ETA: 0s - loss: 6182854.0000\n","Epoch 147: val_loss did not improve from 20109940.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6182854.0000 - val_loss: 23690356.0000\n","Epoch 148/1000\n","76/75 [==============================] - ETA: 0s - loss: 5906277.0000\n","Epoch 148: val_loss did not improve from 20109940.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 5906277.0000 - val_loss: 20676240.0000\n","Epoch 149/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6137529.0000\n","Epoch 149: val_loss did not improve from 20109940.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 5941141.5000 - val_loss: 22094334.0000\n","Epoch 150/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 5895622.0000\n","Epoch 150: val_loss did not improve from 20109940.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 5710994.0000 - val_loss: 21564306.0000\n","Epoch 151/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6232586.0000\n","Epoch 151: val_loss did not improve from 20109940.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6035307.5000 - val_loss: 23133592.0000\n","Epoch 152/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6414077.0000\n","Epoch 152: val_loss did not improve from 20109940.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 6213316.0000 - val_loss: 20980412.0000\n","Epoch 153/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6713488.0000\n","Epoch 153: val_loss did not improve from 20109940.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6501248.0000 - val_loss: 24550262.0000\n","Epoch 154/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 7135567.0000\n","Epoch 154: val_loss improved from 20109940.00000 to 19981202.00000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-6-1000-15.h5\n","75/75 [==============================] - 2s 21ms/step - loss: 6909311.0000 - val_loss: 19981202.0000\n","Epoch 155/1000\n","76/75 [==============================] - ETA: 0s - loss: 6330412.0000\n","Epoch 155: val_loss did not improve from 19981202.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 6330412.0000 - val_loss: 22153628.0000\n","Epoch 156/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6207202.0000\n","Epoch 156: val_loss did not improve from 19981202.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6010881.0000 - val_loss: 20273694.0000\n","Epoch 157/1000\n","75/75 [============================>.] - ETA: 0s - loss: 5849346.5000\n","Epoch 157: val_loss did not improve from 19981202.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 5800684.5000 - val_loss: 22779200.0000\n","Epoch 158/1000\n","76/75 [==============================] - ETA: 0s - loss: 5899847.5000\n","Epoch 158: val_loss did not improve from 19981202.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 5899847.5000 - val_loss: 21191616.0000\n","Epoch 159/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6120125.5000\n","Epoch 159: val_loss did not improve from 19981202.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 5924550.0000 - val_loss: 22983742.0000\n","Epoch 160/1000\n","75/75 [============================>.] - ETA: 0s - loss: 6041566.0000\n","Epoch 160: val_loss did not improve from 19981202.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 5994630.5000 - val_loss: 20368448.0000\n","Epoch 161/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6221741.0000\n","Epoch 161: val_loss did not improve from 19981202.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6023553.0000 - val_loss: 22590772.0000\n","Epoch 162/1000\n","76/75 [==============================] - ETA: 0s - loss: 6147833.5000\n","Epoch 162: val_loss did not improve from 19981202.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 6147833.5000 - val_loss: 20503906.0000\n","Epoch 163/1000\n","76/75 [==============================] - ETA: 0s - loss: 6129578.5000\n","Epoch 163: val_loss did not improve from 19981202.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 6129578.5000 - val_loss: 23940936.0000\n","Epoch 164/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6529297.5000\n","Epoch 164: val_loss did not improve from 19981202.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6324074.0000 - val_loss: 20883484.0000\n","Epoch 165/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6369846.0000\n","Epoch 165: val_loss did not improve from 19981202.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 6168102.5000 - val_loss: 23121668.0000\n","Epoch 166/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6387571.0000\n","Epoch 166: val_loss did not improve from 19981202.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6184398.0000 - val_loss: 21707894.0000\n","Epoch 167/1000\n","76/75 [==============================] - ETA: 0s - loss: 6085929.5000\n","Epoch 167: val_loss did not improve from 19981202.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 6085929.5000 - val_loss: 23567716.0000\n","Epoch 168/1000\n","76/75 [==============================] - ETA: 0s - loss: 6093056.0000\n","Epoch 168: val_loss did not improve from 19981202.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 6093056.0000 - val_loss: 22113856.0000\n","Epoch 169/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6563423.5000\n","Epoch 169: val_loss did not improve from 19981202.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 6355766.0000 - val_loss: 24623816.0000\n","Epoch 170/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6675917.0000\n","Epoch 170: val_loss did not improve from 19981202.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6470994.5000 - val_loss: 21675218.0000\n","Epoch 171/1000\n","76/75 [==============================] - ETA: 0s - loss: 6171639.0000\n","Epoch 171: val_loss did not improve from 19981202.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 6171639.0000 - val_loss: 23679398.0000\n","Epoch 172/1000\n","76/75 [==============================] - ETA: 0s - loss: 7029191.5000\n","Epoch 172: val_loss did not improve from 19981202.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 7029191.5000 - val_loss: 21020954.0000\n","Epoch 173/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6459560.5000\n","Epoch 173: val_loss did not improve from 19981202.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6255127.0000 - val_loss: 22918162.0000\n","Epoch 174/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6095041.5000\n","Epoch 174: val_loss did not improve from 19981202.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 5902108.5000 - val_loss: 21912428.0000\n","Epoch 175/1000\n","76/75 [==============================] - ETA: 0s - loss: 5758546.0000\n","Epoch 175: val_loss did not improve from 19981202.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 5758546.0000 - val_loss: 22514896.0000\n","Epoch 176/1000\n","76/75 [==============================] - ETA: 0s - loss: 5666009.5000\n","Epoch 176: val_loss did not improve from 19981202.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 5666009.5000 - val_loss: 20938780.0000\n","Epoch 177/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 5739907.5000\n","Epoch 177: val_loss did not improve from 19981202.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 5558814.5000 - val_loss: 22147210.0000\n","Epoch 178/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 5798176.0000\n","Epoch 178: val_loss did not improve from 19981202.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 5617782.0000 - val_loss: 21443338.0000\n","Epoch 179/1000\n","76/75 [==============================] - ETA: 0s - loss: 5721659.0000\n","Epoch 179: val_loss did not improve from 19981202.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 5721659.0000 - val_loss: 22920044.0000\n","Epoch 180/1000\n","76/75 [==============================] - ETA: 0s - loss: 6355344.0000\n","Epoch 180: val_loss did not improve from 19981202.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 6355344.0000 - val_loss: 20277902.0000\n","Epoch 181/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6287755.0000\n","Epoch 181: val_loss did not improve from 19981202.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6086675.5000 - val_loss: 24153914.0000\n","Epoch 182/1000\n","76/75 [==============================] - ETA: 0s - loss: 6198656.0000\n","Epoch 182: val_loss did not improve from 19981202.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6198656.0000 - val_loss: 21296234.0000\n","Epoch 183/1000\n","76/75 [==============================] - ETA: 0s - loss: 5964156.5000\n","Epoch 183: val_loss did not improve from 19981202.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 5964156.5000 - val_loss: 22515940.0000\n","Epoch 184/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6109958.0000\n","Epoch 184: val_loss did not improve from 19981202.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 5916286.5000 - val_loss: 20833876.0000\n","Epoch 185/1000\n","76/75 [==============================] - ETA: 0s - loss: 5576707.0000\n","Epoch 185: val_loss did not improve from 19981202.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 5576707.0000 - val_loss: 22009168.0000\n","Epoch 186/1000\n","76/75 [==============================] - ETA: 0s - loss: 5614207.5000\n","Epoch 186: val_loss did not improve from 19981202.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 5614207.5000 - val_loss: 21248318.0000\n","Epoch 187/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 5623558.5000\n","Epoch 187: val_loss did not improve from 19981202.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 5450292.5000 - val_loss: 22191364.0000\n","Epoch 188/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 5692098.5000\n","Epoch 188: val_loss improved from 19981202.00000 to 18790926.00000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-6-1000-15.h5\n","75/75 [==============================] - 2s 21ms/step - loss: 5516132.0000 - val_loss: 18790926.0000\n","Epoch 189/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 5826820.5000\n","Epoch 189: val_loss did not improve from 18790926.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 5642968.5000 - val_loss: 22734642.0000\n","Epoch 190/1000\n","76/75 [==============================] - ETA: 0s - loss: 5723904.5000\n","Epoch 190: val_loss did not improve from 18790926.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 5723904.5000 - val_loss: 20731844.0000\n","Epoch 191/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6242559.0000\n","Epoch 191: val_loss did not improve from 18790926.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6047815.0000 - val_loss: 23446026.0000\n","Epoch 192/1000\n","76/75 [==============================] - ETA: 0s - loss: 6803125.0000\n","Epoch 192: val_loss did not improve from 18790926.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 6803125.0000 - val_loss: 21228934.0000\n","Epoch 193/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6623592.5000\n","Epoch 193: val_loss did not improve from 18790926.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 6414045.0000 - val_loss: 23121166.0000\n","Epoch 194/1000\n","76/75 [==============================] - ETA: 0s - loss: 5978294.5000\n","Epoch 194: val_loss did not improve from 18790926.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 5978294.5000 - val_loss: 21322748.0000\n","Epoch 195/1000\n","76/75 [==============================] - ETA: 0s - loss: 5625997.0000\n","Epoch 195: val_loss did not improve from 18790926.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 5625997.0000 - val_loss: 23738612.0000\n","Epoch 196/1000\n","76/75 [==============================] - ETA: 0s - loss: 5727304.5000\n","Epoch 196: val_loss did not improve from 18790926.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 5727304.5000 - val_loss: 21565082.0000\n","Epoch 197/1000\n","76/75 [==============================] - ETA: 0s - loss: 5658960.5000\n","Epoch 197: val_loss did not improve from 18790926.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 5658960.5000 - val_loss: 22922792.0000\n","Epoch 198/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 5602981.0000\n","Epoch 198: val_loss did not improve from 18790926.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 5427668.0000 - val_loss: 22392250.0000\n","Epoch 199/1000\n","76/75 [==============================] - ETA: 0s - loss: 5356996.0000\n","Epoch 199: val_loss did not improve from 18790926.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 5356996.0000 - val_loss: 22242144.0000\n","Epoch 200/1000\n","75/75 [============================>.] - ETA: 0s - loss: 5698063.0000\n","Epoch 200: val_loss did not improve from 18790926.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 5650817.0000 - val_loss: 22015744.0000\n","Epoch 201/1000\n","76/75 [==============================] - ETA: 0s - loss: 5588994.5000\n","Epoch 201: val_loss did not improve from 18790926.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 5588994.5000 - val_loss: 22206226.0000\n","Epoch 202/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6778377.5000\n","Epoch 202: val_loss did not improve from 18790926.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6561831.0000 - val_loss: 21457522.0000\n","Epoch 203/1000\n","76/75 [==============================] - ETA: 0s - loss: 5591316.0000\n","Epoch 203: val_loss did not improve from 18790926.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 5591316.0000 - val_loss: 23657262.0000\n","Epoch 204/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6116231.5000\n","Epoch 204: val_loss did not improve from 18790926.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 5934362.5000 - val_loss: 20584194.0000\n","Epoch 205/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6091835.0000\n","Epoch 205: val_loss did not improve from 18790926.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 5899995.5000 - val_loss: 23851730.0000\n","Epoch 206/1000\n","76/75 [==============================] - ETA: 0s - loss: 5739801.0000\n","Epoch 206: val_loss did not improve from 18790926.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 5739801.0000 - val_loss: 20544692.0000\n","Epoch 207/1000\n","76/75 [==============================] - ETA: 0s - loss: 5513002.5000\n","Epoch 207: val_loss did not improve from 18790926.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 5513002.5000 - val_loss: 22537184.0000\n","Epoch 208/1000\n","76/75 [==============================] - ETA: 0s - loss: 5431045.5000\n","Epoch 208: val_loss did not improve from 18790926.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 5431045.5000 - val_loss: 20876560.0000\n","Epoch 209/1000\n","75/75 [============================>.] - ETA: 0s - loss: 5320408.5000\n","Epoch 209: val_loss did not improve from 18790926.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 5278195.0000 - val_loss: 20030654.0000\n","Epoch 210/1000\n","75/75 [============================>.] - ETA: 0s - loss: 5235498.0000\n","Epoch 210: val_loss did not improve from 18790926.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 5198821.5000 - val_loss: 22171856.0000\n","Epoch 211/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 5277652.0000\n","Epoch 211: val_loss did not improve from 18790926.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 5119372.5000 - val_loss: 19531620.0000\n","Epoch 212/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 5846280.0000\n","Epoch 212: val_loss did not improve from 18790926.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 5664261.5000 - val_loss: 23124812.0000\n","Epoch 213/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 5978545.5000\n","Epoch 213: val_loss did not improve from 18790926.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 5788372.5000 - val_loss: 20165876.0000\n","Epoch 214/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 5868869.5000\n","Epoch 214: val_loss did not improve from 18790926.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 5688385.0000 - val_loss: 22862142.0000\n","Epoch 215/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6499216.5000\n","Epoch 215: val_loss improved from 18790926.00000 to 17866660.00000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-6-1000-15.h5\n","75/75 [==============================] - 2s 21ms/step - loss: 6357527.0000 - val_loss: 17866660.0000\n","Epoch 216/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 5725374.0000\n","Epoch 216: val_loss did not improve from 17866660.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 5546181.5000 - val_loss: 22362086.0000\n","Epoch 217/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 5415063.0000\n","Epoch 217: val_loss did not improve from 17866660.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 5249234.5000 - val_loss: 22597754.0000\n","Epoch 218/1000\n","75/75 [============================>.] - ETA: 0s - loss: 4992984.0000\n","Epoch 218: val_loss did not improve from 17866660.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 4953024.5000 - val_loss: 21342414.0000\n","Epoch 219/1000\n","76/75 [==============================] - ETA: 0s - loss: 4890338.5000\n","Epoch 219: val_loss did not improve from 17866660.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 4890338.5000 - val_loss: 19169976.0000\n","Epoch 220/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 5210334.5000\n","Epoch 220: val_loss did not improve from 17866660.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 5046647.5000 - val_loss: 21693106.0000\n","Epoch 221/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 5497193.0000\n","Epoch 221: val_loss did not improve from 17866660.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 5326757.0000 - val_loss: 22065054.0000\n","Epoch 222/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 5528997.0000\n","Epoch 222: val_loss did not improve from 17866660.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 5362303.0000 - val_loss: 21790670.0000\n","Epoch 223/1000\n","75/75 [============================>.] - ETA: 0s - loss: 5112906.5000\n","Epoch 223: val_loss did not improve from 17866660.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 5075320.0000 - val_loss: 23170320.0000\n","Epoch 224/1000\n","76/75 [==============================] - ETA: 0s - loss: 5054822.0000\n","Epoch 224: val_loss did not improve from 17866660.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 5054822.0000 - val_loss: 19749578.0000\n","Epoch 225/1000\n","75/75 [============================>.] - ETA: 0s - loss: 5398807.5000\n","Epoch 225: val_loss did not improve from 17866660.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 5355863.5000 - val_loss: 22364450.0000\n","Epoch 226/1000\n","75/75 [============================>.] - ETA: 0s - loss: 7295202.5000\n","Epoch 226: val_loss improved from 17866660.00000 to 16982512.00000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-6-1000-15.h5\n","75/75 [==============================] - 2s 22ms/step - loss: 7246580.0000 - val_loss: 16982512.0000\n","Epoch 227/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 5777613.5000\n","Epoch 227: val_loss did not improve from 16982512.00000\n","75/75 [==============================] - 1s 18ms/step - loss: 5601771.0000 - val_loss: 24153284.0000\n","Epoch 228/1000\n","75/75 [============================>.] - ETA: 0s - loss: 5004713.5000\n","Epoch 228: val_loss did not improve from 16982512.00000\n","75/75 [==============================] - 1s 19ms/step - loss: 4964054.5000 - val_loss: 21878062.0000\n","Epoch 229/1000\n","75/75 [============================>.] - ETA: 0s - loss: 4852786.5000\n","Epoch 229: val_loss did not improve from 16982512.00000\n","75/75 [==============================] - 1s 18ms/step - loss: 4813668.0000 - val_loss: 22045102.0000\n","Epoch 230/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 5212762.0000\n","Epoch 230: val_loss did not improve from 16982512.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 5057909.0000 - val_loss: 23084972.0000\n","Epoch 231/1000\n","76/75 [==============================] - ETA: 0s - loss: 4972531.5000\n","Epoch 231: val_loss did not improve from 16982512.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 4972531.5000 - val_loss: 20464770.0000\n","Epoch 232/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 5068909.0000\n","Epoch 232: val_loss did not improve from 16982512.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 4914958.0000 - val_loss: 22749396.0000\n","Epoch 233/1000\n","76/75 [==============================] - ETA: 0s - loss: 5118794.0000\n","Epoch 233: val_loss did not improve from 16982512.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 5118794.0000 - val_loss: 21482802.0000\n","Epoch 234/1000\n","76/75 [==============================] - ETA: 0s - loss: 5306790.0000\n","Epoch 234: val_loss did not improve from 16982512.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 5306790.0000 - val_loss: 23669332.0000\n","Epoch 235/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 6465108.0000\n","Epoch 235: val_loss did not improve from 16982512.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 6266607.0000 - val_loss: 19922374.0000\n","Epoch 236/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 5532793.5000\n","Epoch 236: val_loss did not improve from 16982512.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 5360513.0000 - val_loss: 23439494.0000\n","Epoch 237/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 5049624.5000\n","Epoch 237: val_loss did not improve from 16982512.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 4895998.0000 - val_loss: 21990016.0000\n","Epoch 238/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 5113946.0000\n","Epoch 238: val_loss did not improve from 16982512.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 4956204.0000 - val_loss: 20389662.0000\n","Epoch 239/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 5367465.0000\n","Epoch 239: val_loss did not improve from 16982512.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 5200867.5000 - val_loss: 23324024.0000\n","Epoch 240/1000\n","76/75 [==============================] - ETA: 0s - loss: 5103438.5000\n","Epoch 240: val_loss did not improve from 16982512.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 5103438.5000 - val_loss: 20878678.0000\n","Epoch 241/1000\n","76/75 [==============================] - ETA: 0s - loss: 4541499.5000\n","Epoch 241: val_loss did not improve from 16982512.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4541499.5000 - val_loss: 21684530.0000\n","Epoch 242/1000\n","75/75 [============================>.] - ETA: 0s - loss: 4555286.5000\n","Epoch 242: val_loss did not improve from 16982512.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4519016.0000 - val_loss: 21218978.0000\n","Epoch 243/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 4662833.0000\n","Epoch 243: val_loss did not improve from 16982512.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4521351.5000 - val_loss: 23222502.0000\n","Epoch 244/1000\n","75/75 [============================>.] - ETA: 0s - loss: 4780007.5000\n","Epoch 244: val_loss did not improve from 16982512.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4742747.5000 - val_loss: 20426564.0000\n","Epoch 245/1000\n","74/75 [============================>.] - ETA: 0s - loss: 4895975.5000\n","Epoch 245: val_loss did not improve from 16982512.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4799284.5000 - val_loss: 22613026.0000\n","Epoch 246/1000\n","76/75 [==============================] - ETA: 0s - loss: 5024619.5000\n","Epoch 246: val_loss did not improve from 16982512.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 5024619.5000 - val_loss: 20216626.0000\n","Epoch 247/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 5255273.0000\n","Epoch 247: val_loss did not improve from 16982512.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 5094975.5000 - val_loss: 23028672.0000\n","Epoch 248/1000\n","76/75 [==============================] - ETA: 0s - loss: 5855330.5000\n","Epoch 248: val_loss did not improve from 16982512.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 5855330.5000 - val_loss: 20313406.0000\n","Epoch 249/1000\n","75/75 [============================>.] - ETA: 0s - loss: 5064676.5000\n","Epoch 249: val_loss did not improve from 16982512.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 5024557.5000 - val_loss: 21792794.0000\n","Epoch 250/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 5201311.0000\n","Epoch 250: val_loss did not improve from 16982512.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 5046922.5000 - val_loss: 20776356.0000\n","Epoch 251/1000\n","76/75 [==============================] - ETA: 0s - loss: 4869751.5000\n","Epoch 251: val_loss did not improve from 16982512.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 4869751.5000 - val_loss: 21441666.0000\n","Epoch 252/1000\n","76/75 [==============================] - ETA: 0s - loss: 4668711.5000\n","Epoch 252: val_loss did not improve from 16982512.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 4668711.5000 - val_loss: 20513302.0000\n","Epoch 253/1000\n","76/75 [==============================] - ETA: 0s - loss: 4618547.0000\n","Epoch 253: val_loss did not improve from 16982512.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4618547.0000 - val_loss: 20462806.0000\n","Epoch 254/1000\n","76/75 [==============================] - ETA: 0s - loss: 4666623.5000\n","Epoch 254: val_loss did not improve from 16982512.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4666623.5000 - val_loss: 20816228.0000\n","Epoch 255/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 4903751.5000\n","Epoch 255: val_loss did not improve from 16982512.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 4756156.5000 - val_loss: 24780778.0000\n","Epoch 256/1000\n","76/75 [==============================] - ETA: 0s - loss: 5027977.5000\n","Epoch 256: val_loss did not improve from 16982512.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 5027977.5000 - val_loss: 20552952.0000\n","Epoch 257/1000\n","76/75 [==============================] - ETA: 0s - loss: 4557794.0000\n","Epoch 257: val_loss did not improve from 16982512.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 4557794.0000 - val_loss: 21925696.0000\n","Epoch 258/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 5410432.0000\n","Epoch 258: val_loss did not improve from 16982512.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 5242435.0000 - val_loss: 21438084.0000\n","Epoch 259/1000\n","76/75 [==============================] - ETA: 0s - loss: 4871348.0000\n","Epoch 259: val_loss did not improve from 16982512.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 4871348.0000 - val_loss: 21791742.0000\n","Epoch 260/1000\n","76/75 [==============================] - ETA: 0s - loss: 5063171.5000\n","Epoch 260: val_loss did not improve from 16982512.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 5063171.5000 - val_loss: 21337026.0000\n","Epoch 261/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 4546500.5000\n","Epoch 261: val_loss did not improve from 16982512.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4412156.5000 - val_loss: 21371098.0000\n","Epoch 262/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 4878413.0000\n","Epoch 262: val_loss did not improve from 16982512.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 4745328.5000 - val_loss: 19120012.0000\n","Epoch 263/1000\n","76/75 [==============================] - ETA: 0s - loss: 5033535.0000\n","Epoch 263: val_loss did not improve from 16982512.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 5033535.0000 - val_loss: 23223584.0000\n","Epoch 264/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 4879125.5000\n","Epoch 264: val_loss did not improve from 16982512.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4734701.0000 - val_loss: 22989306.0000\n","Epoch 265/1000\n","76/75 [==============================] - ETA: 0s - loss: 4416308.0000\n","Epoch 265: val_loss did not improve from 16982512.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4416308.0000 - val_loss: 21452042.0000\n","Epoch 266/1000\n","76/75 [==============================] - ETA: 0s - loss: 5533533.0000\n","Epoch 266: val_loss improved from 16982512.00000 to 16576943.00000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-6-1000-15.h5\n","75/75 [==============================] - 2s 22ms/step - loss: 5533533.0000 - val_loss: 16576943.0000\n","Epoch 267/1000\n","76/75 [==============================] - ETA: 0s - loss: 5200269.0000\n","Epoch 267: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 5200269.0000 - val_loss: 22493834.0000\n","Epoch 268/1000\n","76/75 [==============================] - ETA: 0s - loss: 4567979.5000\n","Epoch 268: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4567979.5000 - val_loss: 21791664.0000\n","Epoch 269/1000\n","75/75 [============================>.] - ETA: 0s - loss: 4378791.5000\n","Epoch 269: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4347153.0000 - val_loss: 23408158.0000\n","Epoch 270/1000\n","76/75 [==============================] - ETA: 0s - loss: 4333155.0000\n","Epoch 270: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4333155.0000 - val_loss: 21964420.0000\n","Epoch 271/1000\n","76/75 [==============================] - ETA: 0s - loss: 4390838.0000\n","Epoch 271: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4390838.0000 - val_loss: 22543034.0000\n","Epoch 272/1000\n","75/75 [============================>.] - ETA: 0s - loss: 4643401.5000\n","Epoch 272: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4607672.5000 - val_loss: 20945198.0000\n","Epoch 273/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 4662389.5000\n","Epoch 273: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4524676.5000 - val_loss: 23254526.0000\n","Epoch 274/1000\n","76/75 [==============================] - ETA: 0s - loss: 4728804.0000\n","Epoch 274: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4728804.0000 - val_loss: 22035526.0000\n","Epoch 275/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 4773679.0000\n","Epoch 275: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4634483.5000 - val_loss: 23361000.0000\n","Epoch 276/1000\n","76/75 [==============================] - ETA: 0s - loss: 5468660.5000\n","Epoch 276: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 5468660.5000 - val_loss: 18031270.0000\n","Epoch 277/1000\n","76/75 [==============================] - ETA: 0s - loss: 5043002.5000\n","Epoch 277: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 5043002.5000 - val_loss: 22394214.0000\n","Epoch 278/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 4823918.0000\n","Epoch 278: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4677639.0000 - val_loss: 21792614.0000\n","Epoch 279/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 4722380.0000\n","Epoch 279: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 4583486.0000 - val_loss: 21893472.0000\n","Epoch 280/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 4480687.5000\n","Epoch 280: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4349177.0000 - val_loss: 20321966.0000\n","Epoch 281/1000\n","76/75 [==============================] - ETA: 0s - loss: 4097289.7500\n","Epoch 281: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4097289.7500 - val_loss: 21620676.0000\n","Epoch 282/1000\n","76/75 [==============================] - ETA: 0s - loss: 4109505.0000\n","Epoch 282: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4109505.0000 - val_loss: 20874674.0000\n","Epoch 283/1000\n","76/75 [==============================] - ETA: 0s - loss: 4266938.0000\n","Epoch 283: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4266938.0000 - val_loss: 21792266.0000\n","Epoch 284/1000\n","76/75 [==============================] - ETA: 0s - loss: 4322788.0000\n","Epoch 284: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4322788.0000 - val_loss: 20494806.0000\n","Epoch 285/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 4736385.5000\n","Epoch 285: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 4599949.0000 - val_loss: 21846562.0000\n","Epoch 286/1000\n","76/75 [==============================] - ETA: 0s - loss: 4573770.5000\n","Epoch 286: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4573770.5000 - val_loss: 21127190.0000\n","Epoch 287/1000\n","76/75 [==============================] - ETA: 0s - loss: 4274269.0000\n","Epoch 287: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 4274269.0000 - val_loss: 22609666.0000\n","Epoch 288/1000\n","76/75 [==============================] - ETA: 0s - loss: 4169458.7500\n","Epoch 288: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4169458.7500 - val_loss: 19352376.0000\n","Epoch 289/1000\n","75/75 [============================>.] - ETA: 0s - loss: 4106092.0000\n","Epoch 289: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4075257.0000 - val_loss: 23134378.0000\n","Epoch 290/1000\n","76/75 [==============================] - ETA: 0s - loss: 4579619.5000\n","Epoch 290: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4579619.5000 - val_loss: 19969500.0000\n","Epoch 291/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 4700693.0000\n","Epoch 291: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 4556813.0000 - val_loss: 23403138.0000\n","Epoch 292/1000\n","75/75 [============================>.] - ETA: 0s - loss: 5077150.0000\n","Epoch 292: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 5044225.5000 - val_loss: 20325206.0000\n","Epoch 293/1000\n","76/75 [==============================] - ETA: 0s - loss: 4695351.5000\n","Epoch 293: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4695351.5000 - val_loss: 23205130.0000\n","Epoch 294/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 4651720.5000\n","Epoch 294: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 4512163.0000 - val_loss: 22204972.0000\n","Epoch 295/1000\n","76/75 [==============================] - ETA: 0s - loss: 4089512.2500\n","Epoch 295: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4089512.2500 - val_loss: 21671800.0000\n","Epoch 296/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 4885927.0000\n","Epoch 296: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 4738663.5000 - val_loss: 18386166.0000\n","Epoch 297/1000\n","76/75 [==============================] - ETA: 0s - loss: 4474184.5000\n","Epoch 297: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4474184.5000 - val_loss: 23429480.0000\n","Epoch 298/1000\n","75/75 [============================>.] - ETA: 0s - loss: 4968615.5000\n","Epoch 298: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4929570.5000 - val_loss: 20936072.0000\n","Epoch 299/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 4862784.0000\n","Epoch 299: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 4720708.5000 - val_loss: 23915604.0000\n","Epoch 300/1000\n","76/75 [==============================] - ETA: 0s - loss: 5014562.0000\n","Epoch 300: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 5014562.0000 - val_loss: 17467580.0000\n","Epoch 301/1000\n","75/75 [============================>.] - ETA: 0s - loss: 4448269.5000\n","Epoch 301: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4415522.0000 - val_loss: 23697590.0000\n","Epoch 302/1000\n","75/75 [============================>.] - ETA: 0s - loss: 4550731.5000\n","Epoch 302: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4517375.0000 - val_loss: 18836038.0000\n","Epoch 303/1000\n","75/75 [============================>.] - ETA: 0s - loss: 4140480.7500\n","Epoch 303: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4108577.2500 - val_loss: 22987496.0000\n","Epoch 304/1000\n","75/75 [============================>.] - ETA: 0s - loss: 4267918.0000\n","Epoch 304: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4235744.0000 - val_loss: 21843900.0000\n","Epoch 305/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 4217958.0000\n","Epoch 305: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 4095491.2500 - val_loss: 20702786.0000\n","Epoch 306/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 5036758.5000\n","Epoch 306: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4889319.0000 - val_loss: 18876462.0000\n","Epoch 307/1000\n","76/75 [==============================] - ETA: 0s - loss: 4780144.0000\n","Epoch 307: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4780144.0000 - val_loss: 23783798.0000\n","Epoch 308/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 5012831.5000\n","Epoch 308: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 4861736.5000 - val_loss: 21499402.0000\n","Epoch 309/1000\n","75/75 [============================>.] - ETA: 0s - loss: 4193555.7500\n","Epoch 309: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4161835.0000 - val_loss: 23149608.0000\n","Epoch 310/1000\n","76/75 [==============================] - ETA: 0s - loss: 4172472.5000\n","Epoch 310: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4172472.5000 - val_loss: 19044450.0000\n","Epoch 311/1000\n","76/75 [==============================] - ETA: 0s - loss: 4195632.0000\n","Epoch 311: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4195632.0000 - val_loss: 21761176.0000\n","Epoch 312/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 4201869.0000\n","Epoch 312: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 4080913.2500 - val_loss: 20348882.0000\n","Epoch 313/1000\n","76/75 [==============================] - ETA: 0s - loss: 4063279.7500\n","Epoch 313: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4063279.7500 - val_loss: 21282358.0000\n","Epoch 314/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 4112461.2500\n","Epoch 314: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 3995410.7500 - val_loss: 20256814.0000\n","Epoch 315/1000\n","76/75 [==============================] - ETA: 0s - loss: 3946648.0000\n","Epoch 315: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 3946648.0000 - val_loss: 20519416.0000\n","Epoch 316/1000\n","74/75 [============================>.] - ETA: 0s - loss: 4470471.5000\n","Epoch 316: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4390479.5000 - val_loss: 19800044.0000\n","Epoch 317/1000\n","76/75 [==============================] - ETA: 0s - loss: 3909479.5000\n","Epoch 317: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 3909479.5000 - val_loss: 23664916.0000\n","Epoch 318/1000\n","76/75 [==============================] - ETA: 0s - loss: 4261539.0000\n","Epoch 318: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4261539.0000 - val_loss: 21820300.0000\n","Epoch 319/1000\n","75/75 [============================>.] - ETA: 0s - loss: 3871084.5000\n","Epoch 319: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 3841388.0000 - val_loss: 23215718.0000\n","Epoch 320/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 4147486.7500\n","Epoch 320: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 4037843.0000 - val_loss: 20217736.0000\n","Epoch 321/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 4202394.5000\n","Epoch 321: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 4076974.0000 - val_loss: 21635650.0000\n","Epoch 322/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 4267424.0000\n","Epoch 322: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 4140097.5000 - val_loss: 21803650.0000\n","Epoch 323/1000\n","75/75 [============================>.] - ETA: 0s - loss: 4245039.5000\n","Epoch 323: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4212322.0000 - val_loss: 22375234.0000\n","Epoch 324/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 5035920.5000\n","Epoch 324: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4888741.5000 - val_loss: 21215162.0000\n","Epoch 325/1000\n","76/75 [==============================] - ETA: 0s - loss: 4169608.5000\n","Epoch 325: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4169608.5000 - val_loss: 17493634.0000\n","Epoch 326/1000\n","76/75 [==============================] - ETA: 0s - loss: 4442421.5000\n","Epoch 326: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4442421.5000 - val_loss: 23021768.0000\n","Epoch 327/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 4447162.0000\n","Epoch 327: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4317024.0000 - val_loss: 24019482.0000\n","Epoch 328/1000\n","76/75 [==============================] - ETA: 0s - loss: 4451411.0000\n","Epoch 328: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4451411.0000 - val_loss: 18222338.0000\n","Epoch 329/1000\n","76/75 [==============================] - ETA: 0s - loss: 4562193.0000\n","Epoch 329: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4562193.0000 - val_loss: 24422756.0000\n","Epoch 330/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 4501133.5000\n","Epoch 330: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4365010.5000 - val_loss: 21847980.0000\n","Epoch 331/1000\n","74/75 [============================>.] - ETA: 0s - loss: 3838939.0000\n","Epoch 331: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 3764838.7500 - val_loss: 22934754.0000\n","Epoch 332/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 3960133.5000\n","Epoch 332: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 3842015.0000 - val_loss: 21236592.0000\n","Epoch 333/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 4050229.5000\n","Epoch 333: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 3932652.5000 - val_loss: 23029600.0000\n","Epoch 334/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 4114478.7500\n","Epoch 334: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 3995366.0000 - val_loss: 20600288.0000\n","Epoch 335/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 4435817.5000\n","Epoch 335: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4303681.5000 - val_loss: 22139790.0000\n","Epoch 336/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 4244566.5000\n","Epoch 336: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4122055.2500 - val_loss: 18374160.0000\n","Epoch 337/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 4448974.0000\n","Epoch 337: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 4317679.0000 - val_loss: 22774276.0000\n","Epoch 338/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 4292168.0000\n","Epoch 338: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4164890.7500 - val_loss: 20584740.0000\n","Epoch 339/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 4316748.5000\n","Epoch 339: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4189389.2500 - val_loss: 23804276.0000\n","Epoch 340/1000\n","76/75 [==============================] - ETA: 0s - loss: 4180608.5000\n","Epoch 340: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4180608.5000 - val_loss: 19959906.0000\n","Epoch 341/1000\n","74/75 [============================>.] - ETA: 0s - loss: 4170747.7500\n","Epoch 341: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4092867.0000 - val_loss: 23540206.0000\n","Epoch 342/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 4438133.5000\n","Epoch 342: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4313854.0000 - val_loss: 19053744.0000\n","Epoch 343/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 4570717.0000\n","Epoch 343: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4434187.5000 - val_loss: 24407992.0000\n","Epoch 344/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 4926057.5000\n","Epoch 344: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 4776513.5000 - val_loss: 20412068.0000\n","Epoch 345/1000\n","76/75 [==============================] - ETA: 0s - loss: 4044511.2500\n","Epoch 345: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4044511.2500 - val_loss: 22422764.0000\n","Epoch 346/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 4262348.5000\n","Epoch 346: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4134814.5000 - val_loss: 17630970.0000\n","Epoch 347/1000\n","76/75 [==============================] - ETA: 0s - loss: 4190378.0000\n","Epoch 347: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4190378.0000 - val_loss: 23975088.0000\n","Epoch 348/1000\n","75/75 [============================>.] - ETA: 0s - loss: 4120050.0000\n","Epoch 348: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4090979.2500 - val_loss: 16922230.0000\n","Epoch 349/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 4637743.5000\n","Epoch 349: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4498391.0000 - val_loss: 25666996.0000\n","Epoch 350/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 4332266.5000\n","Epoch 350: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 4203487.5000 - val_loss: 22292896.0000\n","Epoch 351/1000\n","75/75 [============================>.] - ETA: 0s - loss: 3960415.0000\n","Epoch 351: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 3930420.5000 - val_loss: 24315064.0000\n","Epoch 352/1000\n","76/75 [==============================] - ETA: 0s - loss: 4156684.7500\n","Epoch 352: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4156684.7500 - val_loss: 20160402.0000\n","Epoch 353/1000\n","74/75 [============================>.] - ETA: 0s - loss: 4077941.5000\n","Epoch 353: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 3999819.0000 - val_loss: 21378944.0000\n","Epoch 354/1000\n","76/75 [==============================] - ETA: 0s - loss: 4291362.0000\n","Epoch 354: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 19ms/step - loss: 4291362.0000 - val_loss: 25712878.0000\n","Epoch 355/1000\n","75/75 [============================>.] - ETA: 0s - loss: 4575246.5000\n","Epoch 355: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 19ms/step - loss: 4540807.5000 - val_loss: 19081372.0000\n","Epoch 356/1000\n","75/75 [============================>.] - ETA: 0s - loss: 4130862.0000\n","Epoch 356: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 18ms/step - loss: 4100267.5000 - val_loss: 21886108.0000\n","Epoch 357/1000\n","75/75 [============================>.] - ETA: 0s - loss: 3999335.7500\n","Epoch 357: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 3970748.0000 - val_loss: 19443772.0000\n","Epoch 358/1000\n","76/75 [==============================] - ETA: 0s - loss: 3846330.2500\n","Epoch 358: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 3846330.2500 - val_loss: 22679654.0000\n","Epoch 359/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 4520476.0000\n","Epoch 359: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 4385194.5000 - val_loss: 19695166.0000\n","Epoch 360/1000\n","74/75 [============================>.] - ETA: 0s - loss: 4638169.5000\n","Epoch 360: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4546036.5000 - val_loss: 22355546.0000\n","Epoch 361/1000\n","76/75 [==============================] - ETA: 0s - loss: 4031316.5000\n","Epoch 361: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4031316.5000 - val_loss: 18843930.0000\n","Epoch 362/1000\n","75/75 [============================>.] - ETA: 0s - loss: 4163218.0000\n","Epoch 362: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4131739.5000 - val_loss: 22725060.0000\n","Epoch 363/1000\n","75/75 [============================>.] - ETA: 0s - loss: 4321716.5000\n","Epoch 363: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4288645.0000 - val_loss: 17450434.0000\n","Epoch 364/1000\n","76/75 [==============================] - ETA: 0s - loss: 4401165.5000\n","Epoch 364: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4401165.5000 - val_loss: 24581440.0000\n","Epoch 365/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 4381244.0000\n","Epoch 365: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 4252007.0000 - val_loss: 21363236.0000\n","Epoch 366/1000\n","75/75 [============================>.] - ETA: 0s - loss: 4118059.7500\n","Epoch 366: val_loss did not improve from 16576943.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 4087729.0000 - val_loss: 21673372.0000\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1447/1447 [00:00<00:00, 23118.40it/s]\n","100%|██████████| 1419/1419 [00:00<00:00, 22128.30it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/1000\n","75/75 [============================>.] - ETA: 0s - loss: 3687531.7500\n","Epoch 1: val_loss improved from inf to 4728523.00000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 6s 25ms/step - loss: 3704727.7500 - val_loss: 4728523.0000\n","Epoch 2/1000\n","76/75 [==============================] - ETA: 0s - loss: 1371253.1250\n","Epoch 2: val_loss improved from 4728523.00000 to 2382806.50000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 1371253.1250 - val_loss: 2382806.5000\n","Epoch 3/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 1131899.6250\n","Epoch 3: val_loss improved from 2382806.50000 to 2326732.50000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 1099912.7500 - val_loss: 2326732.5000\n","Epoch 4/1000\n","75/75 [============================>.] - ETA: 0s - loss: 1111131.0000\n","Epoch 4: val_loss improved from 2326732.50000 to 2322176.75000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 1103025.8750 - val_loss: 2322176.7500\n","Epoch 5/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 1081617.5000\n","Epoch 5: val_loss improved from 2322176.75000 to 2292861.00000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 1055656.2500 - val_loss: 2292861.0000\n","Epoch 6/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 1095329.3750\n","Epoch 6: val_loss improved from 2292861.00000 to 2285276.00000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 1064510.6250 - val_loss: 2285276.0000\n","Epoch 7/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 1052841.1250\n","Epoch 7: val_loss improved from 2285276.00000 to 2281643.25000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 1023721.4375 - val_loss: 2281643.2500\n","Epoch 8/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 1051944.0000\n","Epoch 8: val_loss did not improve from 2281643.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 1024218.3125 - val_loss: 2287169.7500\n","Epoch 9/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 1072033.2500\n","Epoch 9: val_loss improved from 2281643.25000 to 2272324.25000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 1044765.1875 - val_loss: 2272324.2500\n","Epoch 10/1000\n","76/75 [==============================] - ETA: 0s - loss: 1034915.1875\n","Epoch 10: val_loss improved from 2272324.25000 to 2268017.25000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 1034915.1875 - val_loss: 2268017.2500\n","Epoch 11/1000\n","76/75 [==============================] - ETA: 0s - loss: 1037980.0625\n","Epoch 11: val_loss improved from 2268017.25000 to 2261813.25000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 1037980.0625 - val_loss: 2261813.2500\n","Epoch 12/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 1066514.5000\n","Epoch 12: val_loss improved from 2261813.25000 to 2236735.75000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 1036333.4375 - val_loss: 2236735.7500\n","Epoch 13/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 1054765.2500\n","Epoch 13: val_loss did not improve from 2236735.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 1026909.5000 - val_loss: 2251080.2500\n","Epoch 14/1000\n","76/75 [==============================] - ETA: 0s - loss: 982843.4375 \n","Epoch 14: val_loss improved from 2236735.75000 to 2221244.75000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 982843.4375 - val_loss: 2221244.7500\n","Epoch 15/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 1026226.8750\n","Epoch 15: val_loss improved from 2221244.75000 to 2171968.50000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 998606.6250 - val_loss: 2171968.5000\n","Epoch 16/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 984918.7500 \n","Epoch 16: val_loss improved from 2171968.50000 to 2089382.12500, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 959531.7500 - val_loss: 2089382.1250\n","Epoch 17/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 967541.6250 \n","Epoch 17: val_loss improved from 2089382.12500 to 2043465.50000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 939090.8750 - val_loss: 2043465.5000\n","Epoch 18/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 929317.3750\n","Epoch 18: val_loss improved from 2043465.50000 to 1772097.62500, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 902389.8125 - val_loss: 1772097.6250\n","Epoch 19/1000\n","76/75 [==============================] - ETA: 0s - loss: 920191.6875\n","Epoch 19: val_loss did not improve from 1772097.62500\n","75/75 [==============================] - 1s 16ms/step - loss: 920191.6875 - val_loss: 1785281.3750\n","Epoch 20/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 925369.8125\n","Epoch 20: val_loss did not improve from 1772097.62500\n","75/75 [==============================] - 1s 16ms/step - loss: 902066.3125 - val_loss: 1872277.1250\n","Epoch 21/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 882879.4375\n","Epoch 21: val_loss did not improve from 1772097.62500\n","75/75 [==============================] - 1s 16ms/step - loss: 860619.1875 - val_loss: 1825550.2500\n","Epoch 22/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 852068.8125\n","Epoch 22: val_loss improved from 1772097.62500 to 1699509.62500, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 827742.6875 - val_loss: 1699509.6250\n","Epoch 23/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 843315.3750\n","Epoch 23: val_loss improved from 1699509.62500 to 1656612.62500, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 1s 17ms/step - loss: 824576.7500 - val_loss: 1656612.6250\n","Epoch 24/1000\n","75/75 [============================>.] - ETA: 0s - loss: 814556.1875\n","Epoch 24: val_loss improved from 1656612.62500 to 1600890.00000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 808304.2500 - val_loss: 1600890.0000\n","Epoch 25/1000\n","76/75 [==============================] - ETA: 0s - loss: 769182.2500\n","Epoch 25: val_loss improved from 1600890.00000 to 1581607.37500, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 769182.2500 - val_loss: 1581607.3750\n","Epoch 26/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 807897.5625\n","Epoch 26: val_loss did not improve from 1581607.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 786794.5625 - val_loss: 1599687.3750\n","Epoch 27/1000\n","76/75 [==============================] - ETA: 0s - loss: 776835.7500\n","Epoch 27: val_loss improved from 1581607.37500 to 1578162.87500, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 776835.7500 - val_loss: 1578162.8750\n","Epoch 28/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 812805.7500\n","Epoch 28: val_loss improved from 1578162.87500 to 1510161.75000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 793736.3125 - val_loss: 1510161.7500\n","Epoch 29/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 818849.8125\n","Epoch 29: val_loss improved from 1510161.75000 to 1481608.00000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 798075.2500 - val_loss: 1481608.0000\n","Epoch 30/1000\n","76/75 [==============================] - ETA: 0s - loss: 785826.3125\n","Epoch 30: val_loss did not improve from 1481608.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 785826.3125 - val_loss: 1563623.3750\n","Epoch 31/1000\n","76/75 [==============================] - ETA: 0s - loss: 777322.3750\n","Epoch 31: val_loss did not improve from 1481608.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 777322.3750 - val_loss: 1545464.7500\n","Epoch 32/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 779655.0000\n","Epoch 32: val_loss did not improve from 1481608.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 760290.9375 - val_loss: 1520717.1250\n","Epoch 33/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 791706.5000\n","Epoch 33: val_loss did not improve from 1481608.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 772674.2500 - val_loss: 1536456.2500\n","Epoch 34/1000\n","76/75 [==============================] - ETA: 0s - loss: 771312.1875\n","Epoch 34: val_loss did not improve from 1481608.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 771312.1875 - val_loss: 1501713.0000\n","Epoch 35/1000\n","75/75 [============================>.] - ETA: 0s - loss: 765146.1875\n","Epoch 35: val_loss did not improve from 1481608.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 759514.6875 - val_loss: 1521053.3750\n","Epoch 36/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 804019.8125\n","Epoch 36: val_loss did not improve from 1481608.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 783835.0000 - val_loss: 1682863.0000\n","Epoch 37/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 925196.1875\n","Epoch 37: val_loss did not improve from 1481608.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 903946.4375 - val_loss: 1596529.0000\n","Epoch 38/1000\n","76/75 [==============================] - ETA: 0s - loss: 818912.6250\n","Epoch 38: val_loss did not improve from 1481608.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 818912.6250 - val_loss: 1581560.6250\n","Epoch 39/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 768946.3750\n","Epoch 39: val_loss did not improve from 1481608.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 750401.9375 - val_loss: 1546128.6250\n","Epoch 40/1000\n","76/75 [==============================] - ETA: 0s - loss: 761232.4375\n","Epoch 40: val_loss did not improve from 1481608.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 761232.4375 - val_loss: 1488464.1250\n","Epoch 41/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 792279.0625\n","Epoch 41: val_loss did not improve from 1481608.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 774071.3750 - val_loss: 1538865.7500\n","Epoch 42/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 764220.3125\n","Epoch 42: val_loss did not improve from 1481608.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 747475.6250 - val_loss: 1510075.0000\n","Epoch 43/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 752739.1250\n","Epoch 43: val_loss did not improve from 1481608.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 731835.0000 - val_loss: 1505888.6250\n","Epoch 44/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 765671.0000\n","Epoch 44: val_loss did not improve from 1481608.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 747564.3125 - val_loss: 1511004.0000\n","Epoch 45/1000\n","76/75 [==============================] - ETA: 0s - loss: 741187.1875\n","Epoch 45: val_loss did not improve from 1481608.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 741187.1875 - val_loss: 1538766.5000\n","Epoch 46/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 772853.0625\n","Epoch 46: val_loss did not improve from 1481608.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 754677.9375 - val_loss: 1537127.3750\n","Epoch 47/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 773739.4375\n","Epoch 47: val_loss improved from 1481608.00000 to 1468156.50000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 2s 21ms/step - loss: 754565.7500 - val_loss: 1468156.5000\n","Epoch 48/1000\n","76/75 [==============================] - ETA: 0s - loss: 746684.6875\n","Epoch 48: val_loss did not improve from 1468156.50000\n","75/75 [==============================] - 1s 16ms/step - loss: 746684.6875 - val_loss: 1527623.0000\n","Epoch 49/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 737474.2500\n","Epoch 49: val_loss did not improve from 1468156.50000\n","75/75 [==============================] - 1s 16ms/step - loss: 716753.4375 - val_loss: 1482022.2500\n","Epoch 50/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 726769.5000\n","Epoch 50: val_loss did not improve from 1468156.50000\n","75/75 [==============================] - 1s 16ms/step - loss: 707478.2500 - val_loss: 1497763.2500\n","Epoch 51/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 723549.5625\n","Epoch 51: val_loss did not improve from 1468156.50000\n","75/75 [==============================] - 1s 16ms/step - loss: 705021.8750 - val_loss: 1515748.1250\n","Epoch 52/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 721887.2500\n","Epoch 52: val_loss improved from 1468156.50000 to 1460921.87500, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 703832.8125 - val_loss: 1460921.8750\n","Epoch 53/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 722288.8125\n","Epoch 53: val_loss improved from 1460921.87500 to 1457350.37500, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 703650.3750 - val_loss: 1457350.3750\n","Epoch 54/1000\n","76/75 [==============================] - ETA: 0s - loss: 679404.6875\n","Epoch 54: val_loss did not improve from 1457350.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 679404.6875 - val_loss: 1457711.0000\n","Epoch 55/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 695255.8125\n","Epoch 55: val_loss improved from 1457350.37500 to 1456447.87500, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 677426.3750 - val_loss: 1456447.8750\n","Epoch 56/1000\n","76/75 [==============================] - ETA: 0s - loss: 699581.1250\n","Epoch 56: val_loss improved from 1456447.87500 to 1438619.25000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 699581.1250 - val_loss: 1438619.2500\n","Epoch 57/1000\n","75/75 [============================>.] - ETA: 0s - loss: 699926.5625\n","Epoch 57: val_loss did not improve from 1438619.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 695405.5625 - val_loss: 1444508.5000\n","Epoch 58/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 717567.1875\n","Epoch 58: val_loss did not improve from 1438619.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 701801.6250 - val_loss: 1444678.7500\n","Epoch 59/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 715735.4375\n","Epoch 59: val_loss did not improve from 1438619.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 697308.9375 - val_loss: 1448502.1250\n","Epoch 60/1000\n","75/75 [============================>.] - ETA: 0s - loss: 703125.9375\n","Epoch 60: val_loss improved from 1438619.25000 to 1415634.00000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 698678.4375 - val_loss: 1415634.0000\n","Epoch 61/1000\n","76/75 [==============================] - ETA: 0s - loss: 691539.1250\n","Epoch 61: val_loss did not improve from 1415634.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 691539.1250 - val_loss: 1431655.8750\n","Epoch 62/1000\n","75/75 [============================>.] - ETA: 0s - loss: 704570.6250\n","Epoch 62: val_loss did not improve from 1415634.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 700642.3125 - val_loss: 1438504.3750\n","Epoch 63/1000\n","76/75 [==============================] - ETA: 0s - loss: 724729.4375\n","Epoch 63: val_loss improved from 1415634.00000 to 1410641.75000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 724729.4375 - val_loss: 1410641.7500\n","Epoch 64/1000\n","76/75 [==============================] - ETA: 0s - loss: 667024.4375\n","Epoch 64: val_loss improved from 1410641.75000 to 1387149.50000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 667024.4375 - val_loss: 1387149.5000\n","Epoch 65/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 695004.5000\n","Epoch 65: val_loss did not improve from 1387149.50000\n","75/75 [==============================] - 1s 16ms/step - loss: 680859.3750 - val_loss: 1437520.6250\n","Epoch 66/1000\n","76/75 [==============================] - ETA: 0s - loss: 679679.6875\n","Epoch 66: val_loss did not improve from 1387149.50000\n","75/75 [==============================] - 1s 16ms/step - loss: 679679.6875 - val_loss: 1401396.3750\n","Epoch 67/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 686348.6250\n","Epoch 67: val_loss improved from 1387149.50000 to 1385092.00000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 672659.0000 - val_loss: 1385092.0000\n","Epoch 68/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 696054.7500\n","Epoch 68: val_loss did not improve from 1385092.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 682055.1250 - val_loss: 1422280.8750\n","Epoch 69/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 685521.7500\n","Epoch 69: val_loss did not improve from 1385092.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 667774.6875 - val_loss: 1386208.1250\n","Epoch 70/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 680095.8750\n","Epoch 70: val_loss improved from 1385092.00000 to 1357231.00000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 665659.6250 - val_loss: 1357231.0000\n","Epoch 71/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 660001.5625\n","Epoch 71: val_loss did not improve from 1357231.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 644676.3125 - val_loss: 1357818.6250\n","Epoch 72/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 679242.6875\n","Epoch 72: val_loss improved from 1357231.00000 to 1344161.37500, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 663107.4375 - val_loss: 1344161.3750\n","Epoch 73/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 676861.7500\n","Epoch 73: val_loss did not improve from 1344161.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 665791.1875 - val_loss: 1378127.6250\n","Epoch 74/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 666749.4375\n","Epoch 74: val_loss did not improve from 1344161.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 654558.8750 - val_loss: 1348059.0000\n","Epoch 75/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 692074.9375\n","Epoch 75: val_loss improved from 1344161.37500 to 1335543.25000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 676833.5000 - val_loss: 1335543.2500\n","Epoch 76/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 678202.5625\n","Epoch 76: val_loss did not improve from 1335543.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 663687.5625 - val_loss: 1347421.6250\n","Epoch 77/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 649587.0625\n","Epoch 77: val_loss did not improve from 1335543.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 636082.0000 - val_loss: 1356494.5000\n","Epoch 78/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 660284.1250\n","Epoch 78: val_loss did not improve from 1335543.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 648480.0000 - val_loss: 1340380.1250\n","Epoch 79/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 662461.1250\n","Epoch 79: val_loss improved from 1335543.25000 to 1333095.75000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 649240.3750 - val_loss: 1333095.7500\n","Epoch 80/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 653905.1875\n","Epoch 80: val_loss improved from 1333095.75000 to 1325385.25000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 637807.4375 - val_loss: 1325385.2500\n","Epoch 81/1000\n","76/75 [==============================] - ETA: 0s - loss: 642543.3750\n","Epoch 81: val_loss did not improve from 1325385.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 642543.3750 - val_loss: 1330175.1250\n","Epoch 82/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 654973.8750\n","Epoch 82: val_loss did not improve from 1325385.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 641734.8125 - val_loss: 1335666.3750\n","Epoch 83/1000\n","76/75 [==============================] - ETA: 0s - loss: 623457.9375\n","Epoch 83: val_loss did not improve from 1325385.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 623457.9375 - val_loss: 1327186.8750\n","Epoch 84/1000\n","76/75 [==============================] - ETA: 0s - loss: 617965.8750\n","Epoch 84: val_loss improved from 1325385.25000 to 1317382.25000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 617965.8750 - val_loss: 1317382.2500\n","Epoch 85/1000\n","76/75 [==============================] - ETA: 0s - loss: 627021.3125\n","Epoch 85: val_loss did not improve from 1317382.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 627021.3125 - val_loss: 1325321.8750\n","Epoch 86/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 614597.3750\n","Epoch 86: val_loss did not improve from 1317382.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 600180.3750 - val_loss: 1320447.2500\n","Epoch 87/1000\n","76/75 [==============================] - ETA: 0s - loss: 617749.6250\n","Epoch 87: val_loss did not improve from 1317382.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 617749.6250 - val_loss: 1317834.1250\n","Epoch 88/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 620928.3125\n","Epoch 88: val_loss improved from 1317382.25000 to 1306939.87500, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 607046.6250 - val_loss: 1306939.8750\n","Epoch 89/1000\n","75/75 [============================>.] - ETA: 0s - loss: 624497.3750\n","Epoch 89: val_loss did not improve from 1306939.87500\n","75/75 [==============================] - 1s 16ms/step - loss: 619940.8125 - val_loss: 1307257.0000\n","Epoch 90/1000\n","76/75 [==============================] - ETA: 0s - loss: 612081.0625\n","Epoch 90: val_loss did not improve from 1306939.87500\n","75/75 [==============================] - 1s 16ms/step - loss: 612081.0625 - val_loss: 1317767.7500\n","Epoch 91/1000\n","75/75 [============================>.] - ETA: 0s - loss: 613294.2500\n","Epoch 91: val_loss did not improve from 1306939.87500\n","75/75 [==============================] - 1s 20ms/step - loss: 609478.1875 - val_loss: 1313349.3750\n","Epoch 92/1000\n","76/75 [==============================] - ETA: 0s - loss: 615768.1875\n","Epoch 92: val_loss did not improve from 1306939.87500\n","75/75 [==============================] - 1s 19ms/step - loss: 615768.1875 - val_loss: 1355082.3750\n","Epoch 93/1000\n","75/75 [============================>.] - ETA: 0s - loss: 603918.3750\n","Epoch 93: val_loss improved from 1306939.87500 to 1278275.50000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 599616.9375 - val_loss: 1278275.5000\n","Epoch 94/1000\n","75/75 [============================>.] - ETA: 0s - loss: 611105.7500\n","Epoch 94: val_loss improved from 1278275.50000 to 1267489.75000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 607051.4375 - val_loss: 1267489.7500\n","Epoch 95/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 629116.5000\n","Epoch 95: val_loss did not improve from 1267489.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 616119.9375 - val_loss: 1313593.7500\n","Epoch 96/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 619950.0000\n","Epoch 96: val_loss did not improve from 1267489.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 605248.8750 - val_loss: 1298710.6250\n","Epoch 97/1000\n","76/75 [==============================] - ETA: 0s - loss: 609438.3750\n","Epoch 97: val_loss did not improve from 1267489.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 609438.3750 - val_loss: 1296960.6250\n","Epoch 98/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 621519.0625\n","Epoch 98: val_loss did not improve from 1267489.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 606280.8125 - val_loss: 1283021.2500\n","Epoch 99/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 597806.6250\n","Epoch 99: val_loss did not improve from 1267489.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 585258.3750 - val_loss: 1283148.7500\n","Epoch 100/1000\n","76/75 [==============================] - ETA: 0s - loss: 583287.0625\n","Epoch 100: val_loss did not improve from 1267489.75000\n","75/75 [==============================] - 1s 17ms/step - loss: 583287.0625 - val_loss: 1286284.1250\n","Epoch 101/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 596994.3125\n","Epoch 101: val_loss did not improve from 1267489.75000\n","75/75 [==============================] - 1s 17ms/step - loss: 581876.8125 - val_loss: 1287513.3750\n","Epoch 102/1000\n","74/75 [============================>.] - ETA: 0s - loss: 582650.1250\n","Epoch 102: val_loss did not improve from 1267489.75000\n","75/75 [==============================] - 1s 17ms/step - loss: 573612.1875 - val_loss: 1284165.3750\n","Epoch 103/1000\n","76/75 [==============================] - ETA: 0s - loss: 584147.6875\n","Epoch 103: val_loss did not improve from 1267489.75000\n","75/75 [==============================] - 1s 17ms/step - loss: 584147.6875 - val_loss: 1272138.6250\n","Epoch 104/1000\n","76/75 [==============================] - ETA: 0s - loss: 560539.0625\n","Epoch 104: val_loss did not improve from 1267489.75000\n","75/75 [==============================] - 1s 19ms/step - loss: 560539.0625 - val_loss: 1272957.7500\n","Epoch 105/1000\n","75/75 [============================>.] - ETA: 0s - loss: 567153.5000\n","Epoch 105: val_loss did not improve from 1267489.75000\n","75/75 [==============================] - 1s 19ms/step - loss: 563682.8125 - val_loss: 1304364.6250\n","Epoch 106/1000\n","75/75 [============================>.] - ETA: 0s - loss: 562330.8750\n","Epoch 106: val_loss did not improve from 1267489.75000\n","75/75 [==============================] - 1s 17ms/step - loss: 559287.4375 - val_loss: 1304715.3750\n","Epoch 107/1000\n","76/75 [==============================] - ETA: 0s - loss: 577283.1875\n","Epoch 107: val_loss did not improve from 1267489.75000\n","75/75 [==============================] - 1s 17ms/step - loss: 577283.1875 - val_loss: 1304638.1250\n","Epoch 108/1000\n","76/75 [==============================] - ETA: 0s - loss: 560569.2500\n","Epoch 108: val_loss did not improve from 1267489.75000\n","75/75 [==============================] - 1s 17ms/step - loss: 560569.2500 - val_loss: 1278975.1250\n","Epoch 109/1000\n","76/75 [==============================] - ETA: 0s - loss: 561512.1875\n","Epoch 109: val_loss did not improve from 1267489.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 561512.1875 - val_loss: 1293960.3750\n","Epoch 110/1000\n","76/75 [==============================] - ETA: 0s - loss: 563111.5000\n","Epoch 110: val_loss did not improve from 1267489.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 563111.5000 - val_loss: 1282046.6250\n","Epoch 111/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 592202.3750\n","Epoch 111: val_loss did not improve from 1267489.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 579896.3750 - val_loss: 1310894.2500\n","Epoch 112/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 570417.1875\n","Epoch 112: val_loss did not improve from 1267489.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 556733.5625 - val_loss: 1325459.2500\n","Epoch 113/1000\n","76/75 [==============================] - ETA: 0s - loss: 581469.6250\n","Epoch 113: val_loss did not improve from 1267489.75000\n","75/75 [==============================] - 1s 17ms/step - loss: 581469.6250 - val_loss: 1317657.3750\n","Epoch 114/1000\n","76/75 [==============================] - ETA: 0s - loss: 556139.5000\n","Epoch 114: val_loss did not improve from 1267489.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 556139.5000 - val_loss: 1289536.2500\n","Epoch 115/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 550802.8750\n","Epoch 115: val_loss did not improve from 1267489.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 539865.2500 - val_loss: 1299311.7500\n","Epoch 116/1000\n","76/75 [==============================] - ETA: 0s - loss: 555353.0625\n","Epoch 116: val_loss did not improve from 1267489.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 555353.0625 - val_loss: 1328058.3750\n","Epoch 117/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 581645.4375\n","Epoch 117: val_loss did not improve from 1267489.75000\n","75/75 [==============================] - 1s 17ms/step - loss: 567193.1875 - val_loss: 1283711.1250\n","Epoch 118/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 578334.2500\n","Epoch 118: val_loss did not improve from 1267489.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 566689.7500 - val_loss: 1269798.7500\n","Epoch 119/1000\n","76/75 [==============================] - ETA: 0s - loss: 533537.7500\n","Epoch 119: val_loss did not improve from 1267489.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 533537.7500 - val_loss: 1355600.6250\n","Epoch 120/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 561763.6250\n","Epoch 120: val_loss did not improve from 1267489.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 549381.3125 - val_loss: 1309623.2500\n","Epoch 121/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 551020.2500\n","Epoch 121: val_loss did not improve from 1267489.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 539212.3750 - val_loss: 1297088.7500\n","Epoch 122/1000\n","76/75 [==============================] - ETA: 0s - loss: 555340.4375\n","Epoch 122: val_loss did not improve from 1267489.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 555340.4375 - val_loss: 1338992.2500\n","Epoch 123/1000\n","76/75 [==============================] - ETA: 0s - loss: 581671.0625\n","Epoch 123: val_loss did not improve from 1267489.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 581671.0625 - val_loss: 1271424.6250\n","Epoch 124/1000\n","76/75 [==============================] - ETA: 0s - loss: 582020.3125\n","Epoch 124: val_loss did not improve from 1267489.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 582020.3125 - val_loss: 1287618.5000\n","Epoch 125/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 529096.5000\n","Epoch 125: val_loss did not improve from 1267489.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 520962.4688 - val_loss: 1368181.5000\n","Epoch 126/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 559802.7500\n","Epoch 126: val_loss did not improve from 1267489.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 546682.9375 - val_loss: 1275445.2500\n","Epoch 127/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 543072.2500\n","Epoch 127: val_loss did not improve from 1267489.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 533061.8750 - val_loss: 1343735.2500\n","Epoch 128/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 565290.5625\n","Epoch 128: val_loss improved from 1267489.75000 to 1264392.12500, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 2s 21ms/step - loss: 552181.5000 - val_loss: 1264392.1250\n","Epoch 129/1000\n","76/75 [==============================] - ETA: 0s - loss: 574243.8750\n","Epoch 129: val_loss improved from 1264392.12500 to 1242800.50000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 574243.8750 - val_loss: 1242800.5000\n","Epoch 130/1000\n","76/75 [==============================] - ETA: 0s - loss: 550990.2500\n","Epoch 130: val_loss did not improve from 1242800.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 550990.2500 - val_loss: 1379170.5000\n","Epoch 131/1000\n","76/75 [==============================] - ETA: 0s - loss: 550487.1250\n","Epoch 131: val_loss did not improve from 1242800.50000\n","75/75 [==============================] - 1s 16ms/step - loss: 550487.1250 - val_loss: 1359070.5000\n","Epoch 132/1000\n","76/75 [==============================] - ETA: 0s - loss: 568985.0625\n","Epoch 132: val_loss did not improve from 1242800.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 568985.0625 - val_loss: 1276490.6250\n","Epoch 133/1000\n","76/75 [==============================] - ETA: 0s - loss: 544592.3125\n","Epoch 133: val_loss did not improve from 1242800.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 544592.3125 - val_loss: 1419205.8750\n","Epoch 134/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 574953.1875\n","Epoch 134: val_loss improved from 1242800.50000 to 1233586.25000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 564683.1250 - val_loss: 1233586.2500\n","Epoch 135/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 532597.1250\n","Epoch 135: val_loss did not improve from 1233586.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 525471.5000 - val_loss: 1288825.2500\n","Epoch 136/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 515916.0938\n","Epoch 136: val_loss did not improve from 1233586.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 507486.5000 - val_loss: 1339096.1250\n","Epoch 137/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 610550.5000\n","Epoch 137: val_loss did not improve from 1233586.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 599252.0000 - val_loss: 1268291.5000\n","Epoch 138/1000\n","76/75 [==============================] - ETA: 0s - loss: 509645.6562\n","Epoch 138: val_loss did not improve from 1233586.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 509645.6562 - val_loss: 1406844.5000\n","Epoch 139/1000\n","75/75 [============================>.] - ETA: 0s - loss: 535745.3750\n","Epoch 139: val_loss did not improve from 1233586.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 532520.6875 - val_loss: 1284130.8750\n","Epoch 140/1000\n","75/75 [============================>.] - ETA: 0s - loss: 504301.9688\n","Epoch 140: val_loss did not improve from 1233586.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 501180.2188 - val_loss: 1321613.2500\n","Epoch 141/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 553690.0625\n","Epoch 141: val_loss did not improve from 1233586.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 543033.7500 - val_loss: 1242540.3750\n","Epoch 142/1000\n","75/75 [============================>.] - ETA: 0s - loss: 513825.2812\n","Epoch 142: val_loss did not improve from 1233586.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 510442.0000 - val_loss: 1398404.0000\n","Epoch 143/1000\n","76/75 [==============================] - ETA: 0s - loss: 552861.8125\n","Epoch 143: val_loss did not improve from 1233586.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 552861.8125 - val_loss: 1256499.8750\n","Epoch 144/1000\n","75/75 [============================>.] - ETA: 0s - loss: 509106.8438\n","Epoch 144: val_loss did not improve from 1233586.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 505267.5938 - val_loss: 1340015.5000\n","Epoch 145/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 567031.1875\n","Epoch 145: val_loss did not improve from 1233586.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 553987.6875 - val_loss: 1253693.8750\n","Epoch 146/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 524772.4375\n","Epoch 146: val_loss did not improve from 1233586.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 513698.0625 - val_loss: 1325585.7500\n","Epoch 147/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 515908.7188\n","Epoch 147: val_loss did not improve from 1233586.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 504645.9062 - val_loss: 1306327.2500\n","Epoch 148/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 516612.9062\n","Epoch 148: val_loss did not improve from 1233586.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 510311.8125 - val_loss: 1278881.7500\n","Epoch 149/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 514749.1875\n","Epoch 149: val_loss did not improve from 1233586.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 503720.0312 - val_loss: 1277415.2500\n","Epoch 150/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 498130.5312\n","Epoch 150: val_loss did not improve from 1233586.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 489946.3438 - val_loss: 1313945.2500\n","Epoch 151/1000\n","76/75 [==============================] - ETA: 0s - loss: 517161.6562\n","Epoch 151: val_loss did not improve from 1233586.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 517161.6562 - val_loss: 1375562.8750\n","Epoch 152/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 615575.5000\n","Epoch 152: val_loss did not improve from 1233586.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 603898.6250 - val_loss: 1287476.6250\n","Epoch 153/1000\n","76/75 [==============================] - ETA: 0s - loss: 512551.6250\n","Epoch 153: val_loss did not improve from 1233586.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 512551.6250 - val_loss: 1291744.3750\n","Epoch 154/1000\n","75/75 [============================>.] - ETA: 0s - loss: 536181.2500\n","Epoch 154: val_loss did not improve from 1233586.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 533400.0625 - val_loss: 1255236.3750\n","Epoch 155/1000\n","76/75 [==============================] - ETA: 0s - loss: 489474.0312\n","Epoch 155: val_loss did not improve from 1233586.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 489474.0312 - val_loss: 1293990.8750\n","Epoch 156/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 507244.5938\n","Epoch 156: val_loss did not improve from 1233586.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 495939.7812 - val_loss: 1289078.3750\n","Epoch 157/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 504520.8750\n","Epoch 157: val_loss did not improve from 1233586.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 494495.9062 - val_loss: 1298810.6250\n","Epoch 158/1000\n","75/75 [============================>.] - ETA: 0s - loss: 494017.1875\n","Epoch 158: val_loss did not improve from 1233586.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 493431.4375 - val_loss: 1264950.8750\n","Epoch 159/1000\n","76/75 [==============================] - ETA: 0s - loss: 514012.9375\n","Epoch 159: val_loss did not improve from 1233586.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 514012.9375 - val_loss: 1284611.7500\n","Epoch 160/1000\n","76/75 [==============================] - ETA: 0s - loss: 493455.6250\n","Epoch 160: val_loss did not improve from 1233586.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 493455.6250 - val_loss: 1360975.6250\n","Epoch 161/1000\n","76/75 [==============================] - ETA: 0s - loss: 526052.0625\n","Epoch 161: val_loss did not improve from 1233586.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 526052.0625 - val_loss: 1302002.8750\n","Epoch 162/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 518051.8438\n","Epoch 162: val_loss did not improve from 1233586.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 507210.7188 - val_loss: 1262298.8750\n","Epoch 163/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 495998.4688\n","Epoch 163: val_loss did not improve from 1233586.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 485097.5625 - val_loss: 1267839.3750\n","Epoch 164/1000\n","76/75 [==============================] - ETA: 0s - loss: 486636.4375\n","Epoch 164: val_loss improved from 1233586.25000 to 1228670.75000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 2s 23ms/step - loss: 486636.4375 - val_loss: 1228670.7500\n","Epoch 165/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 479130.2500\n","Epoch 165: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 17ms/step - loss: 471232.3125 - val_loss: 1287794.7500\n","Epoch 166/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 522916.4688\n","Epoch 166: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 513651.4688 - val_loss: 1303544.3750\n","Epoch 167/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 488068.0625\n","Epoch 167: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 480071.6562 - val_loss: 1361550.7500\n","Epoch 168/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 598130.3125\n","Epoch 168: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 587558.0000 - val_loss: 1329451.1250\n","Epoch 169/1000\n","76/75 [==============================] - ETA: 0s - loss: 515529.8125\n","Epoch 169: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 515529.8125 - val_loss: 1321651.7500\n","Epoch 170/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 527054.6875\n","Epoch 170: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 516781.9688 - val_loss: 1356016.5000\n","Epoch 171/1000\n","76/75 [==============================] - ETA: 0s - loss: 501633.7500\n","Epoch 171: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 17ms/step - loss: 501633.7500 - val_loss: 1397823.2500\n","Epoch 172/1000\n","76/75 [==============================] - ETA: 0s - loss: 508875.4375\n","Epoch 172: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 17ms/step - loss: 508875.4375 - val_loss: 1294588.2500\n","Epoch 173/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 486424.7500\n","Epoch 173: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 479500.8438 - val_loss: 1373423.7500\n","Epoch 174/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 504088.4375\n","Epoch 174: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 493836.9062 - val_loss: 1265035.8750\n","Epoch 175/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 478072.1250\n","Epoch 175: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 468466.9375 - val_loss: 1313547.3750\n","Epoch 176/1000\n","76/75 [==============================] - ETA: 0s - loss: 480369.2812\n","Epoch 176: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 480369.2812 - val_loss: 1347301.0000\n","Epoch 177/1000\n","75/75 [============================>.] - ETA: 0s - loss: 481118.5000\n","Epoch 177: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 17ms/step - loss: 477942.5312 - val_loss: 1365404.0000\n","Epoch 178/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 530595.1875\n","Epoch 178: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 17ms/step - loss: 522899.7812 - val_loss: 1293377.8750\n","Epoch 179/1000\n","76/75 [==============================] - ETA: 0s - loss: 538766.6875\n","Epoch 179: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 17ms/step - loss: 538766.6875 - val_loss: 1383622.2500\n","Epoch 180/1000\n","76/75 [==============================] - ETA: 0s - loss: 594945.2500\n","Epoch 180: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 17ms/step - loss: 594945.2500 - val_loss: 1321518.6250\n","Epoch 181/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 550500.5000\n","Epoch 181: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 17ms/step - loss: 542571.8750 - val_loss: 1367041.1250\n","Epoch 182/1000\n","76/75 [==============================] - ETA: 0s - loss: 485362.3438\n","Epoch 182: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 17ms/step - loss: 485362.3438 - val_loss: 1291836.1250\n","Epoch 183/1000\n","76/75 [==============================] - ETA: 0s - loss: 471681.7812\n","Epoch 183: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 471681.7812 - val_loss: 1369976.7500\n","Epoch 184/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 487221.1562\n","Epoch 184: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 478681.6562 - val_loss: 1242760.0000\n","Epoch 185/1000\n","76/75 [==============================] - ETA: 0s - loss: 478343.0625\n","Epoch 185: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 478343.0625 - val_loss: 1270463.3750\n","Epoch 186/1000\n","75/75 [============================>.] - ETA: 0s - loss: 474575.0625\n","Epoch 186: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 17ms/step - loss: 471378.1562 - val_loss: 1243977.5000\n","Epoch 187/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 476067.9688\n","Epoch 187: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 17ms/step - loss: 466949.9062 - val_loss: 1340774.2500\n","Epoch 188/1000\n","75/75 [============================>.] - ETA: 0s - loss: 488157.6875\n","Epoch 188: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 17ms/step - loss: 486496.7500 - val_loss: 1237971.2500\n","Epoch 189/1000\n","75/75 [============================>.] - ETA: 0s - loss: 489207.3438\n","Epoch 189: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 17ms/step - loss: 486121.8438 - val_loss: 1495251.1250\n","Epoch 190/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 608186.5000\n","Epoch 190: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 600580.3125 - val_loss: 1361695.1250\n","Epoch 191/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 512770.4688\n","Epoch 191: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 505208.0000 - val_loss: 1331939.2500\n","Epoch 192/1000\n","76/75 [==============================] - ETA: 0s - loss: 480513.7500\n","Epoch 192: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 17ms/step - loss: 480513.7500 - val_loss: 1334322.2500\n","Epoch 193/1000\n","76/75 [==============================] - ETA: 0s - loss: 470420.7188\n","Epoch 193: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 17ms/step - loss: 470420.7188 - val_loss: 1261371.8750\n","Epoch 194/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 482009.7188\n","Epoch 194: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 477033.3438 - val_loss: 1265740.6250\n","Epoch 195/1000\n","75/75 [============================>.] - ETA: 0s - loss: 482730.9375\n","Epoch 195: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 17ms/step - loss: 480127.5000 - val_loss: 1273595.8750\n","Epoch 196/1000\n","75/75 [============================>.] - ETA: 0s - loss: 472743.5000\n","Epoch 196: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 17ms/step - loss: 470238.4375 - val_loss: 1345447.3750\n","Epoch 197/1000\n","75/75 [============================>.] - ETA: 0s - loss: 472358.0625\n","Epoch 197: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 17ms/step - loss: 470608.0625 - val_loss: 1264995.5000\n","Epoch 198/1000\n","76/75 [==============================] - ETA: 0s - loss: 453765.5000\n","Epoch 198: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 17ms/step - loss: 453765.5000 - val_loss: 1429592.7500\n","Epoch 199/1000\n","74/75 [============================>.] - ETA: 0s - loss: 502060.4688\n","Epoch 199: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 17ms/step - loss: 496354.9062 - val_loss: 1237231.3750\n","Epoch 200/1000\n","76/75 [==============================] - ETA: 0s - loss: 458826.8438\n","Epoch 200: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 17ms/step - loss: 458826.8438 - val_loss: 1294726.5000\n","Epoch 201/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 477004.1562\n","Epoch 201: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 467005.7812 - val_loss: 1296047.5000\n","Epoch 202/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 447124.7188\n","Epoch 202: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 439484.1250 - val_loss: 1319669.1250\n","Epoch 203/1000\n","76/75 [==============================] - ETA: 0s - loss: 463206.3750\n","Epoch 203: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 17ms/step - loss: 463206.3750 - val_loss: 1307237.2500\n","Epoch 204/1000\n","75/75 [============================>.] - ETA: 0s - loss: 511109.9062\n","Epoch 204: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 17ms/step - loss: 507413.8125 - val_loss: 1444551.5000\n","Epoch 205/1000\n","76/75 [==============================] - ETA: 0s - loss: 482493.0625\n","Epoch 205: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 17ms/step - loss: 482493.0625 - val_loss: 1276369.2500\n","Epoch 206/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 511631.7188\n","Epoch 206: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 501739.8125 - val_loss: 1377373.8750\n","Epoch 207/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 489504.6562\n","Epoch 207: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 482071.1250 - val_loss: 1298258.5000\n","Epoch 208/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 477225.9062\n","Epoch 208: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 470829.8125 - val_loss: 1361548.3750\n","Epoch 209/1000\n","76/75 [==============================] - ETA: 0s - loss: 499509.9062\n","Epoch 209: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 17ms/step - loss: 499509.9062 - val_loss: 1259934.6250\n","Epoch 210/1000\n","75/75 [============================>.] - ETA: 0s - loss: 459524.5625\n","Epoch 210: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 457290.7188 - val_loss: 1383153.6250\n","Epoch 211/1000\n","76/75 [==============================] - ETA: 0s - loss: 464401.7500\n","Epoch 211: val_loss did not improve from 1228670.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 464401.7500 - val_loss: 1316379.0000\n","Epoch 212/1000\n","76/75 [==============================] - ETA: 0s - loss: 476528.8750\n","Epoch 212: val_loss improved from 1228670.75000 to 1198850.00000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 2s 22ms/step - loss: 476528.8750 - val_loss: 1198850.0000\n","Epoch 213/1000\n","76/75 [==============================] - ETA: 0s - loss: 438263.9062\n","Epoch 213: val_loss did not improve from 1198850.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 438263.9062 - val_loss: 1298999.5000\n","Epoch 214/1000\n","76/75 [==============================] - ETA: 0s - loss: 461183.9062\n","Epoch 214: val_loss improved from 1198850.00000 to 1191304.75000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 461183.9062 - val_loss: 1191304.7500\n","Epoch 215/1000\n","76/75 [==============================] - ETA: 0s - loss: 469062.0938\n","Epoch 215: val_loss did not improve from 1191304.75000\n","75/75 [==============================] - 1s 17ms/step - loss: 469062.0938 - val_loss: 1371047.7500\n","Epoch 216/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 475077.7812\n","Epoch 216: val_loss did not improve from 1191304.75000\n","75/75 [==============================] - 1s 16ms/step - loss: 468706.0312 - val_loss: 1310059.5000\n","Epoch 217/1000\n","75/75 [============================>.] - ETA: 0s - loss: 465459.6250\n","Epoch 217: val_loss improved from 1191304.75000 to 1175662.62500, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 1s 18ms/step - loss: 462277.7812 - val_loss: 1175662.6250\n","Epoch 218/1000\n","76/75 [==============================] - ETA: 0s - loss: 446682.2500\n","Epoch 218: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 17ms/step - loss: 446682.2500 - val_loss: 1199557.0000\n","Epoch 219/1000\n","75/75 [============================>.] - ETA: 0s - loss: 449850.7500\n","Epoch 219: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 16ms/step - loss: 447051.2188 - val_loss: 1370760.8750\n","Epoch 220/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 525518.0625\n","Epoch 220: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 16ms/step - loss: 516153.9688 - val_loss: 1220320.1250\n","Epoch 221/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 513412.5625\n","Epoch 221: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 17ms/step - loss: 506106.1875 - val_loss: 1287117.5000\n","Epoch 222/1000\n","75/75 [============================>.] - ETA: 0s - loss: 459462.5000\n","Epoch 222: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 16ms/step - loss: 457974.8438 - val_loss: 1255226.8750\n","Epoch 223/1000\n","76/75 [==============================] - ETA: 0s - loss: 455427.5312\n","Epoch 223: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 16ms/step - loss: 455427.5312 - val_loss: 1241685.6250\n","Epoch 224/1000\n","76/75 [==============================] - ETA: 0s - loss: 448725.4688\n","Epoch 224: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 17ms/step - loss: 448725.4688 - val_loss: 1283329.8750\n","Epoch 225/1000\n","76/75 [==============================] - ETA: 0s - loss: 503798.7500\n","Epoch 225: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 17ms/step - loss: 503798.7500 - val_loss: 1241195.7500\n","Epoch 226/1000\n","74/75 [============================>.] - ETA: 0s - loss: 493816.2812\n","Epoch 226: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 17ms/step - loss: 487555.4375 - val_loss: 1289247.7500\n","Epoch 227/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 483680.3438\n","Epoch 227: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 17ms/step - loss: 479847.9688 - val_loss: 1203374.8750\n","Epoch 228/1000\n","76/75 [==============================] - ETA: 0s - loss: 464543.2188\n","Epoch 228: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 19ms/step - loss: 464543.2188 - val_loss: 1367933.8750\n","Epoch 229/1000\n","74/75 [============================>.] - ETA: 0s - loss: 487126.8438\n","Epoch 229: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 19ms/step - loss: 480045.4375 - val_loss: 1225190.2500\n","Epoch 230/1000\n","76/75 [==============================] - ETA: 0s - loss: 450225.2188\n","Epoch 230: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 18ms/step - loss: 450225.2188 - val_loss: 1256121.8750\n","Epoch 231/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 463569.2812\n","Epoch 231: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 16ms/step - loss: 457572.8125 - val_loss: 1212259.5000\n","Epoch 232/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 449633.3750\n","Epoch 232: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 17ms/step - loss: 440244.4688 - val_loss: 1277629.7500\n","Epoch 233/1000\n","76/75 [==============================] - ETA: 0s - loss: 443042.4062\n","Epoch 233: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 16ms/step - loss: 443042.4062 - val_loss: 1281381.6250\n","Epoch 234/1000\n","76/75 [==============================] - ETA: 0s - loss: 453550.5938\n","Epoch 234: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 17ms/step - loss: 453550.5938 - val_loss: 1275731.3750\n","Epoch 235/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 498153.7188\n","Epoch 235: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 16ms/step - loss: 495959.4688 - val_loss: 1345942.8750\n","Epoch 236/1000\n","76/75 [==============================] - ETA: 0s - loss: 474943.0312\n","Epoch 236: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 17ms/step - loss: 474943.0312 - val_loss: 1227698.7500\n","Epoch 237/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 432239.2812\n","Epoch 237: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 16ms/step - loss: 422647.2188 - val_loss: 1271029.5000\n","Epoch 238/1000\n","75/75 [============================>.] - ETA: 0s - loss: 435923.4062\n","Epoch 238: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 17ms/step - loss: 434948.8125 - val_loss: 1278011.0000\n","Epoch 239/1000\n","76/75 [==============================] - ETA: 0s - loss: 427012.8750\n","Epoch 239: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 17ms/step - loss: 427012.8750 - val_loss: 1258546.6250\n","Epoch 240/1000\n","76/75 [==============================] - ETA: 0s - loss: 444419.9688\n","Epoch 240: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 17ms/step - loss: 444419.9688 - val_loss: 1186594.8750\n","Epoch 241/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 452584.5938\n","Epoch 241: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 16ms/step - loss: 446761.3750 - val_loss: 1406579.7500\n","Epoch 242/1000\n","76/75 [==============================] - ETA: 0s - loss: 487899.9688\n","Epoch 242: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 17ms/step - loss: 487899.9688 - val_loss: 1284619.7500\n","Epoch 243/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 464667.5000\n","Epoch 243: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 17ms/step - loss: 455517.6875 - val_loss: 1311574.0000\n","Epoch 244/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 451191.0625\n","Epoch 244: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 16ms/step - loss: 447687.5000 - val_loss: 1306644.3750\n","Epoch 245/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 448215.0625\n","Epoch 245: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 16ms/step - loss: 440180.8750 - val_loss: 1329214.7500\n","Epoch 246/1000\n","76/75 [==============================] - ETA: 0s - loss: 430097.1875\n","Epoch 246: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 17ms/step - loss: 430097.1875 - val_loss: 1181236.8750\n","Epoch 247/1000\n","76/75 [==============================] - ETA: 0s - loss: 446829.1250\n","Epoch 247: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 17ms/step - loss: 446829.1250 - val_loss: 1280968.6250\n","Epoch 248/1000\n","75/75 [============================>.] - ETA: 0s - loss: 456551.4062\n","Epoch 248: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 17ms/step - loss: 455021.6875 - val_loss: 1221608.5000\n","Epoch 249/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 454813.1562\n","Epoch 249: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 16ms/step - loss: 445173.7188 - val_loss: 1305364.8750\n","Epoch 250/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 444193.9688\n","Epoch 250: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 16ms/step - loss: 436530.7188 - val_loss: 1250358.0000\n","Epoch 251/1000\n","76/75 [==============================] - ETA: 0s - loss: 432438.9688\n","Epoch 251: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 17ms/step - loss: 432438.9688 - val_loss: 1237900.5000\n","Epoch 252/1000\n","76/75 [==============================] - ETA: 0s - loss: 451413.1250\n","Epoch 252: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 17ms/step - loss: 451413.1250 - val_loss: 1219665.6250\n","Epoch 253/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 449153.1562\n","Epoch 253: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 16ms/step - loss: 441227.3750 - val_loss: 1224323.8750\n","Epoch 254/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 442169.5000\n","Epoch 254: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 16ms/step - loss: 435493.7188 - val_loss: 1346125.6250\n","Epoch 255/1000\n","76/75 [==============================] - ETA: 0s - loss: 465075.1875\n","Epoch 255: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 17ms/step - loss: 465075.1875 - val_loss: 1202627.0000\n","Epoch 256/1000\n","76/75 [==============================] - ETA: 0s - loss: 478587.7812\n","Epoch 256: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 17ms/step - loss: 478587.7812 - val_loss: 1294991.2500\n","Epoch 257/1000\n","76/75 [==============================] - ETA: 0s - loss: 451122.4375\n","Epoch 257: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 16ms/step - loss: 451122.4375 - val_loss: 1386717.7500\n","Epoch 258/1000\n","76/75 [==============================] - ETA: 0s - loss: 468564.6875\n","Epoch 258: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 16ms/step - loss: 468564.6875 - val_loss: 1196734.8750\n","Epoch 259/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 459461.6562\n","Epoch 259: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 16ms/step - loss: 450729.0000 - val_loss: 1330712.0000\n","Epoch 260/1000\n","75/75 [============================>.] - ETA: 0s - loss: 450959.8125\n","Epoch 260: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 17ms/step - loss: 448653.5938 - val_loss: 1207040.6250\n","Epoch 261/1000\n","76/75 [==============================] - ETA: 0s - loss: 458457.8750\n","Epoch 261: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 16ms/step - loss: 458457.8750 - val_loss: 1446193.3750\n","Epoch 262/1000\n","76/75 [==============================] - ETA: 0s - loss: 473848.8438\n","Epoch 262: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 16ms/step - loss: 473848.8438 - val_loss: 1198833.7500\n","Epoch 263/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 454144.5312\n","Epoch 263: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 16ms/step - loss: 445099.3750 - val_loss: 1409500.0000\n","Epoch 264/1000\n","76/75 [==============================] - ETA: 0s - loss: 469638.8750\n","Epoch 264: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 17ms/step - loss: 469638.8750 - val_loss: 1193546.1250\n","Epoch 265/1000\n","76/75 [==============================] - ETA: 0s - loss: 448926.9375\n","Epoch 265: val_loss did not improve from 1175662.62500\n","75/75 [==============================] - 1s 16ms/step - loss: 448926.9375 - val_loss: 1359336.6250\n","Epoch 266/1000\n","76/75 [==============================] - ETA: 0s - loss: 466779.8438\n","Epoch 266: val_loss improved from 1175662.62500 to 1167895.62500, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 2s 22ms/step - loss: 466779.8438 - val_loss: 1167895.6250\n","Epoch 267/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 458385.8125\n","Epoch 267: val_loss did not improve from 1167895.62500\n","75/75 [==============================] - 1s 17ms/step - loss: 450678.0312 - val_loss: 1385421.6250\n","Epoch 268/1000\n","76/75 [==============================] - ETA: 0s - loss: 458191.6250\n","Epoch 268: val_loss did not improve from 1167895.62500\n","75/75 [==============================] - 1s 17ms/step - loss: 458191.6250 - val_loss: 1213041.2500\n","Epoch 269/1000\n","76/75 [==============================] - ETA: 0s - loss: 433872.6875\n","Epoch 269: val_loss did not improve from 1167895.62500\n","75/75 [==============================] - 1s 17ms/step - loss: 433872.6875 - val_loss: 1301177.6250\n","Epoch 270/1000\n","76/75 [==============================] - ETA: 0s - loss: 437924.5312\n","Epoch 270: val_loss did not improve from 1167895.62500\n","75/75 [==============================] - 1s 17ms/step - loss: 437924.5312 - val_loss: 1254150.1250\n","Epoch 271/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 431263.4375\n","Epoch 271: val_loss did not improve from 1167895.62500\n","75/75 [==============================] - 1s 17ms/step - loss: 424825.0625 - val_loss: 1227726.7500\n","Epoch 272/1000\n","74/75 [============================>.] - ETA: 0s - loss: 427876.5625\n","Epoch 272: val_loss did not improve from 1167895.62500\n","75/75 [==============================] - 1s 17ms/step - loss: 421909.1562 - val_loss: 1194770.8750\n","Epoch 273/1000\n","76/75 [==============================] - ETA: 0s - loss: 418453.8438\n","Epoch 273: val_loss did not improve from 1167895.62500\n","75/75 [==============================] - 1s 17ms/step - loss: 418453.8438 - val_loss: 1305492.8750\n","Epoch 274/1000\n","75/75 [============================>.] - ETA: 0s - loss: 445686.1562\n","Epoch 274: val_loss did not improve from 1167895.62500\n","75/75 [==============================] - 1s 17ms/step - loss: 443025.2500 - val_loss: 1184752.2500\n","Epoch 275/1000\n","75/75 [============================>.] - ETA: 0s - loss: 432411.1250\n","Epoch 275: val_loss did not improve from 1167895.62500\n","75/75 [==============================] - 1s 17ms/step - loss: 429275.4688 - val_loss: 1279544.0000\n","Epoch 276/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 457279.0000\n","Epoch 276: val_loss did not improve from 1167895.62500\n","75/75 [==============================] - 1s 17ms/step - loss: 451601.7812 - val_loss: 1185212.7500\n","Epoch 277/1000\n","75/75 [============================>.] - ETA: 0s - loss: 429038.4688\n","Epoch 277: val_loss did not improve from 1167895.62500\n","75/75 [==============================] - 1s 17ms/step - loss: 427019.8750 - val_loss: 1309644.6250\n","Epoch 278/1000\n","74/75 [============================>.] - ETA: 0s - loss: 466246.9062\n","Epoch 278: val_loss did not improve from 1167895.62500\n","75/75 [==============================] - 1s 17ms/step - loss: 458475.8750 - val_loss: 1181046.5000\n","Epoch 279/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 450936.9062\n","Epoch 279: val_loss did not improve from 1167895.62500\n","75/75 [==============================] - 2s 20ms/step - loss: 442891.6875 - val_loss: 1351378.2500\n","Epoch 280/1000\n","76/75 [==============================] - ETA: 0s - loss: 452791.6562\n","Epoch 280: val_loss improved from 1167895.62500 to 1158901.50000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 2s 21ms/step - loss: 452791.6562 - val_loss: 1158901.5000\n","Epoch 281/1000\n","75/75 [============================>.] - ETA: 0s - loss: 464498.1562\n","Epoch 281: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 462539.3125 - val_loss: 1541622.5000\n","Epoch 282/1000\n","76/75 [==============================] - ETA: 0s - loss: 539112.6250\n","Epoch 282: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 539112.6250 - val_loss: 1220413.8750\n","Epoch 283/1000\n","74/75 [============================>.] - ETA: 0s - loss: 469877.1562\n","Epoch 283: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 463477.7188 - val_loss: 1330189.1250\n","Epoch 284/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 443725.1875\n","Epoch 284: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 438520.3750 - val_loss: 1345905.1250\n","Epoch 285/1000\n","76/75 [==============================] - ETA: 0s - loss: 442327.9062\n","Epoch 285: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 442327.9062 - val_loss: 1283055.1250\n","Epoch 286/1000\n","76/75 [==============================] - ETA: 0s - loss: 443272.6875\n","Epoch 286: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 443272.6875 - val_loss: 1279691.2500\n","Epoch 287/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 447742.0000\n","Epoch 287: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 440011.2500 - val_loss: 1232389.7500\n","Epoch 288/1000\n","75/75 [============================>.] - ETA: 0s - loss: 433290.6250\n","Epoch 288: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 432328.8125 - val_loss: 1299780.8750\n","Epoch 289/1000\n","76/75 [==============================] - ETA: 0s - loss: 434590.9062\n","Epoch 289: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 434590.9062 - val_loss: 1237169.6250\n","Epoch 290/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 433940.4688\n","Epoch 290: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 424939.0000 - val_loss: 1204821.3750\n","Epoch 291/1000\n","76/75 [==============================] - ETA: 0s - loss: 418495.7500\n","Epoch 291: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 418495.7500 - val_loss: 1292776.7500\n","Epoch 292/1000\n","75/75 [============================>.] - ETA: 0s - loss: 469096.7500\n","Epoch 292: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 466078.5625 - val_loss: 1209562.1250\n","Epoch 293/1000\n","76/75 [==============================] - ETA: 0s - loss: 498195.6875\n","Epoch 293: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 498195.6875 - val_loss: 1376140.8750\n","Epoch 294/1000\n","76/75 [==============================] - ETA: 0s - loss: 477079.1250\n","Epoch 294: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 16ms/step - loss: 477079.1250 - val_loss: 1223810.3750\n","Epoch 295/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 428985.2812\n","Epoch 295: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 421693.0938 - val_loss: 1291940.0000\n","Epoch 296/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 444919.9375\n","Epoch 296: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 438105.8125 - val_loss: 1198994.7500\n","Epoch 297/1000\n","76/75 [==============================] - ETA: 0s - loss: 415343.2812\n","Epoch 297: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 415343.2812 - val_loss: 1272678.2500\n","Epoch 298/1000\n","75/75 [============================>.] - ETA: 0s - loss: 443375.5000\n","Epoch 298: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 441490.7500 - val_loss: 1230822.3750\n","Epoch 299/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 442509.4688\n","Epoch 299: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 436430.9062 - val_loss: 1258481.7500\n","Epoch 300/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 490662.5312\n","Epoch 300: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 16ms/step - loss: 483855.0625 - val_loss: 1198141.2500\n","Epoch 301/1000\n","76/75 [==============================] - ETA: 0s - loss: 477562.8750\n","Epoch 301: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 16ms/step - loss: 477562.8750 - val_loss: 1343148.1250\n","Epoch 302/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 436828.0625\n","Epoch 302: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 16ms/step - loss: 432264.5312 - val_loss: 1254028.3750\n","Epoch 303/1000\n","76/75 [==============================] - ETA: 0s - loss: 428883.3438\n","Epoch 303: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 16ms/step - loss: 428883.3438 - val_loss: 1392760.0000\n","Epoch 304/1000\n","75/75 [============================>.] - ETA: 0s - loss: 473077.8750\n","Epoch 304: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 471623.0625 - val_loss: 1211115.8750\n","Epoch 305/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 447338.3750\n","Epoch 305: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 16ms/step - loss: 439913.7812 - val_loss: 1403943.3750\n","Epoch 306/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 444535.6562\n","Epoch 306: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 437650.6875 - val_loss: 1249928.6250\n","Epoch 307/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 433238.0938\n","Epoch 307: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 16ms/step - loss: 424657.8438 - val_loss: 1323365.5000\n","Epoch 308/1000\n","76/75 [==============================] - ETA: 0s - loss: 457259.5625\n","Epoch 308: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 457259.5625 - val_loss: 1179222.2500\n","Epoch 309/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 463301.9062\n","Epoch 309: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 16ms/step - loss: 452629.5312 - val_loss: 1431017.1250\n","Epoch 310/1000\n","76/75 [==============================] - ETA: 0s - loss: 477428.0000\n","Epoch 310: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 477428.0000 - val_loss: 1235946.1250\n","Epoch 311/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 473600.3125\n","Epoch 311: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 16ms/step - loss: 465185.4062 - val_loss: 1475689.7500\n","Epoch 312/1000\n","75/75 [============================>.] - ETA: 0s - loss: 460553.8125\n","Epoch 312: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 16ms/step - loss: 458244.9375 - val_loss: 1181156.3750\n","Epoch 313/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 467294.7500\n","Epoch 313: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 458383.1562 - val_loss: 1369167.1250\n","Epoch 314/1000\n","75/75 [============================>.] - ETA: 0s - loss: 421340.8750\n","Epoch 314: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 419903.5938 - val_loss: 1268859.1250\n","Epoch 315/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 433274.6250\n","Epoch 315: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 16ms/step - loss: 424902.1875 - val_loss: 1391491.0000\n","Epoch 316/1000\n","75/75 [============================>.] - ETA: 0s - loss: 475707.6562\n","Epoch 316: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 473430.6562 - val_loss: 1167243.0000\n","Epoch 317/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 473026.5312\n","Epoch 317: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 16ms/step - loss: 461361.1562 - val_loss: 1485068.3750\n","Epoch 318/1000\n","75/75 [============================>.] - ETA: 0s - loss: 452618.9062\n","Epoch 318: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 449571.8438 - val_loss: 1261165.5000\n","Epoch 319/1000\n","76/75 [==============================] - ETA: 0s - loss: 415168.4062\n","Epoch 319: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 16ms/step - loss: 415168.4062 - val_loss: 1304272.3750\n","Epoch 320/1000\n","75/75 [============================>.] - ETA: 0s - loss: 457760.8438\n","Epoch 320: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 455266.9062 - val_loss: 1225469.8750\n","Epoch 321/1000\n","76/75 [==============================] - ETA: 0s - loss: 412995.8125\n","Epoch 321: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 412995.8125 - val_loss: 1213458.5000\n","Epoch 322/1000\n","74/75 [============================>.] - ETA: 0s - loss: 417759.5938\n","Epoch 322: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 413444.5938 - val_loss: 1259740.6250\n","Epoch 323/1000\n","75/75 [============================>.] - ETA: 0s - loss: 433924.6875\n","Epoch 323: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 431857.2500 - val_loss: 1241409.1250\n","Epoch 324/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 426320.9375\n","Epoch 324: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 16ms/step - loss: 418257.5938 - val_loss: 1236701.6250\n","Epoch 325/1000\n","74/75 [============================>.] - ETA: 0s - loss: 420063.9688\n","Epoch 325: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 414330.4688 - val_loss: 1255946.5000\n","Epoch 326/1000\n","75/75 [============================>.] - ETA: 0s - loss: 435445.6875\n","Epoch 326: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 433396.5938 - val_loss: 1305422.6250\n","Epoch 327/1000\n","75/75 [============================>.] - ETA: 0s - loss: 443903.1875\n","Epoch 327: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 442078.0000 - val_loss: 1201579.0000\n","Epoch 328/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 475546.3438\n","Epoch 328: val_loss did not improve from 1158901.50000\n","75/75 [==============================] - 1s 17ms/step - loss: 463597.8750 - val_loss: 1508964.8750\n","Epoch 329/1000\n","75/75 [============================>.] - ETA: 0s - loss: 466199.8750\n","Epoch 329: val_loss improved from 1158901.50000 to 1142414.25000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 2s 22ms/step - loss: 464369.0312 - val_loss: 1142414.2500\n","Epoch 330/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 440560.9062\n","Epoch 330: val_loss did not improve from 1142414.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 429458.9375 - val_loss: 1367134.6250\n","Epoch 331/1000\n","76/75 [==============================] - ETA: 0s - loss: 439770.1875\n","Epoch 331: val_loss did not improve from 1142414.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 439770.1875 - val_loss: 1201396.0000\n","Epoch 332/1000\n","76/75 [==============================] - ETA: 0s - loss: 439887.4375\n","Epoch 332: val_loss did not improve from 1142414.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 439887.4375 - val_loss: 1475152.8750\n","Epoch 333/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 476557.1562\n","Epoch 333: val_loss did not improve from 1142414.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 465484.2812 - val_loss: 1204216.6250\n","Epoch 334/1000\n","75/75 [============================>.] - ETA: 0s - loss: 429946.6875\n","Epoch 334: val_loss did not improve from 1142414.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 427284.4062 - val_loss: 1355713.7500\n","Epoch 335/1000\n","76/75 [==============================] - ETA: 0s - loss: 415462.6562\n","Epoch 335: val_loss did not improve from 1142414.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 415462.6562 - val_loss: 1264521.2500\n","Epoch 336/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 419659.8125\n","Epoch 336: val_loss did not improve from 1142414.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 411687.4688 - val_loss: 1205330.7500\n","Epoch 337/1000\n","75/75 [============================>.] - ETA: 0s - loss: 414867.2500\n","Epoch 337: val_loss did not improve from 1142414.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 413035.5625 - val_loss: 1354454.5000\n","Epoch 338/1000\n","76/75 [==============================] - ETA: 0s - loss: 439974.5000\n","Epoch 338: val_loss did not improve from 1142414.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 439974.5000 - val_loss: 1159145.3750\n","Epoch 339/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 437612.0312\n","Epoch 339: val_loss did not improve from 1142414.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 429725.7812 - val_loss: 1443199.3750\n","Epoch 340/1000\n","74/75 [============================>.] - ETA: 0s - loss: 470496.1562\n","Epoch 340: val_loss did not improve from 1142414.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 465782.4688 - val_loss: 1203513.3750\n","Epoch 341/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 437487.3438\n","Epoch 341: val_loss did not improve from 1142414.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 428715.0938 - val_loss: 1291540.2500\n","Epoch 342/1000\n","76/75 [==============================] - ETA: 0s - loss: 414905.5000\n","Epoch 342: val_loss did not improve from 1142414.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 414905.5000 - val_loss: 1234421.6250\n","Epoch 343/1000\n","76/75 [==============================] - ETA: 0s - loss: 405218.0938\n","Epoch 343: val_loss did not improve from 1142414.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 405218.0938 - val_loss: 1322265.5000\n","Epoch 344/1000\n","76/75 [==============================] - ETA: 0s - loss: 413215.8438\n","Epoch 344: val_loss did not improve from 1142414.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 413215.8438 - val_loss: 1175021.1250\n","Epoch 345/1000\n","75/75 [============================>.] - ETA: 0s - loss: 428039.1250\n","Epoch 345: val_loss did not improve from 1142414.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 425967.5000 - val_loss: 1527671.8750\n","Epoch 346/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 502605.2812\n","Epoch 346: val_loss improved from 1142414.25000 to 1139727.37500, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 2s 21ms/step - loss: 493702.1875 - val_loss: 1139727.3750\n","Epoch 347/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 433967.5312\n","Epoch 347: val_loss did not improve from 1139727.37500\n","75/75 [==============================] - 1s 16ms/step - loss: 421863.2500 - val_loss: 1344243.8750\n","Epoch 348/1000\n","76/75 [==============================] - ETA: 0s - loss: 422574.1875\n","Epoch 348: val_loss did not improve from 1139727.37500\n","75/75 [==============================] - 1s 17ms/step - loss: 422574.1875 - val_loss: 1248252.0000\n","Epoch 349/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 421502.4062\n","Epoch 349: val_loss did not improve from 1139727.37500\n","75/75 [==============================] - 1s 17ms/step - loss: 412063.9375 - val_loss: 1251830.3750\n","Epoch 350/1000\n","75/75 [============================>.] - ETA: 0s - loss: 428147.2812\n","Epoch 350: val_loss did not improve from 1139727.37500\n","75/75 [==============================] - 1s 18ms/step - loss: 425488.1562 - val_loss: 1187896.3750\n","Epoch 351/1000\n","74/75 [============================>.] - ETA: 0s - loss: 434832.7500\n","Epoch 351: val_loss did not improve from 1139727.37500\n","75/75 [==============================] - 1s 19ms/step - loss: 428552.7500 - val_loss: 1439223.1250\n","Epoch 352/1000\n","74/75 [============================>.] - ETA: 0s - loss: 486602.2500\n","Epoch 352: val_loss improved from 1139727.37500 to 1133206.00000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 2s 21ms/step - loss: 481293.1250 - val_loss: 1133206.0000\n","Epoch 353/1000\n","76/75 [==============================] - ETA: 0s - loss: 464037.0625\n","Epoch 353: val_loss did not improve from 1133206.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 464037.0625 - val_loss: 1528981.1250\n","Epoch 354/1000\n","76/75 [==============================] - ETA: 0s - loss: 446095.2812\n","Epoch 354: val_loss did not improve from 1133206.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 446095.2812 - val_loss: 1318511.0000\n","Epoch 355/1000\n","76/75 [==============================] - ETA: 0s - loss: 413715.2812\n","Epoch 355: val_loss did not improve from 1133206.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 413715.2812 - val_loss: 1334539.7500\n","Epoch 356/1000\n","75/75 [============================>.] - ETA: 0s - loss: 433903.9062\n","Epoch 356: val_loss did not improve from 1133206.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 431324.9688 - val_loss: 1298362.7500\n","Epoch 357/1000\n","75/75 [============================>.] - ETA: 0s - loss: 401789.4062\n","Epoch 357: val_loss did not improve from 1133206.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 399431.5000 - val_loss: 1315909.1250\n","Epoch 358/1000\n","75/75 [============================>.] - ETA: 0s - loss: 427191.7812\n","Epoch 358: val_loss did not improve from 1133206.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 423987.8438 - val_loss: 1322934.2500\n","Epoch 359/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 412435.4688\n","Epoch 359: val_loss did not improve from 1133206.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 404636.7812 - val_loss: 1264565.5000\n","Epoch 360/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 431324.9062\n","Epoch 360: val_loss did not improve from 1133206.00000\n","75/75 [==============================] - 1s 16ms/step - loss: 423167.0625 - val_loss: 1309144.2500\n","Epoch 361/1000\n","76/75 [==============================] - ETA: 0s - loss: 402187.1250\n","Epoch 361: val_loss did not improve from 1133206.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 402187.1250 - val_loss: 1456152.8750\n","Epoch 362/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 424842.9375\n","Epoch 362: val_loss did not improve from 1133206.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 414625.7812 - val_loss: 1293709.1250\n","Epoch 363/1000\n","76/75 [==============================] - ETA: 0s - loss: 401756.3750\n","Epoch 363: val_loss did not improve from 1133206.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 401756.3750 - val_loss: 1265447.2500\n","Epoch 364/1000\n","75/75 [============================>.] - ETA: 0s - loss: 395260.1875\n","Epoch 364: val_loss did not improve from 1133206.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 393111.7812 - val_loss: 1377492.8750\n","Epoch 365/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 443640.8125\n","Epoch 365: val_loss did not improve from 1133206.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 432259.7812 - val_loss: 1186154.8750\n","Epoch 366/1000\n","75/75 [============================>.] - ETA: 0s - loss: 433449.7500\n","Epoch 366: val_loss did not improve from 1133206.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 431101.1875 - val_loss: 1455650.3750\n","Epoch 367/1000\n","75/75 [============================>.] - ETA: 0s - loss: 434222.8438\n","Epoch 367: val_loss did not improve from 1133206.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 432530.2188 - val_loss: 1265793.0000\n","Epoch 368/1000\n","74/75 [============================>.] - ETA: 0s - loss: 433896.6875\n","Epoch 368: val_loss did not improve from 1133206.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 426786.5312 - val_loss: 1282015.1250\n","Epoch 369/1000\n","75/75 [============================>.] - ETA: 0s - loss: 403478.5625\n","Epoch 369: val_loss did not improve from 1133206.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 402000.0000 - val_loss: 1245825.0000\n","Epoch 370/1000\n","74/75 [============================>.] - ETA: 0s - loss: 402748.6250\n","Epoch 370: val_loss did not improve from 1133206.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 397941.0938 - val_loss: 1312837.0000\n","Epoch 371/1000\n","75/75 [============================>.] - ETA: 0s - loss: 426493.2188\n","Epoch 371: val_loss did not improve from 1133206.00000\n","75/75 [==============================] - 1s 17ms/step - loss: 424389.4688 - val_loss: 1400070.2500\n","Epoch 372/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 518679.5625\n","Epoch 372: val_loss improved from 1133206.00000 to 1091710.25000, saving model to /content/drive/MyDrive/농산물예측/aT_data/check17/transformer-7-1000-15.h5\n","75/75 [==============================] - 2s 22ms/step - loss: 509041.0625 - val_loss: 1091710.2500\n","Epoch 373/1000\n","75/75 [============================>.] - ETA: 0s - loss: 458088.7188\n","Epoch 373: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 455959.2500 - val_loss: 1374269.5000\n","Epoch 374/1000\n","75/75 [============================>.] - ETA: 0s - loss: 441016.8438\n","Epoch 374: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 438582.2500 - val_loss: 1429990.7500\n","Epoch 375/1000\n","76/75 [==============================] - ETA: 0s - loss: 429350.5000\n","Epoch 375: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 429350.5000 - val_loss: 1278257.3750\n","Epoch 376/1000\n","76/75 [==============================] - ETA: 0s - loss: 411812.9062\n","Epoch 376: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 411812.9062 - val_loss: 1266159.3750\n","Epoch 377/1000\n","76/75 [==============================] - ETA: 0s - loss: 407176.0000\n","Epoch 377: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 407176.0000 - val_loss: 1243562.7500\n","Epoch 378/1000\n","75/75 [============================>.] - ETA: 0s - loss: 416444.0312\n","Epoch 378: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 414361.1875 - val_loss: 1374376.6250\n","Epoch 379/1000\n","75/75 [============================>.] - ETA: 0s - loss: 423404.5000\n","Epoch 379: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 421596.4062 - val_loss: 1229965.6250\n","Epoch 380/1000\n","76/75 [==============================] - ETA: 0s - loss: 410347.3750\n","Epoch 380: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 410347.3750 - val_loss: 1312243.6250\n","Epoch 381/1000\n","76/75 [==============================] - ETA: 0s - loss: 407231.5625\n","Epoch 381: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 407231.5625 - val_loss: 1381189.3750\n","Epoch 382/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 415707.5312\n","Epoch 382: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 406742.8125 - val_loss: 1366999.6250\n","Epoch 383/1000\n","76/75 [==============================] - ETA: 0s - loss: 403560.3750\n","Epoch 383: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 403560.3750 - val_loss: 1267737.7500\n","Epoch 384/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 412113.2812\n","Epoch 384: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 404377.7188 - val_loss: 1397660.6250\n","Epoch 385/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 434537.9375\n","Epoch 385: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 424151.9062 - val_loss: 1296537.7500\n","Epoch 386/1000\n","75/75 [============================>.] - ETA: 0s - loss: 403130.2500\n","Epoch 386: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 401381.8750 - val_loss: 1307744.3750\n","Epoch 387/1000\n","75/75 [============================>.] - ETA: 0s - loss: 423731.5312\n","Epoch 387: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 421352.3438 - val_loss: 1468573.2500\n","Epoch 388/1000\n","74/75 [============================>.] - ETA: 0s - loss: 441439.4375\n","Epoch 388: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 435571.9062 - val_loss: 1248364.5000\n","Epoch 389/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 426221.2812\n","Epoch 389: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 416430.8125 - val_loss: 1591674.2500\n","Epoch 390/1000\n","75/75 [============================>.] - ETA: 0s - loss: 453430.7500\n","Epoch 390: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 450552.6875 - val_loss: 1328990.6250\n","Epoch 391/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 420851.7500\n","Epoch 391: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 412685.4375 - val_loss: 1367526.2500\n","Epoch 392/1000\n","76/75 [==============================] - ETA: 0s - loss: 421681.6562\n","Epoch 392: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 421681.6562 - val_loss: 1232655.7500\n","Epoch 393/1000\n","76/75 [==============================] - ETA: 0s - loss: 378768.2500\n","Epoch 393: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 378768.2500 - val_loss: 1253096.3750\n","Epoch 394/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 414978.1875\n","Epoch 394: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 405026.6250 - val_loss: 1492284.5000\n","Epoch 395/1000\n","76/75 [==============================] - ETA: 0s - loss: 466573.7500\n","Epoch 395: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 466573.7500 - val_loss: 1110815.6250\n","Epoch 396/1000\n","75/75 [============================>.] - ETA: 0s - loss: 438130.2188\n","Epoch 396: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 436406.4688 - val_loss: 1354508.0000\n","Epoch 397/1000\n","75/75 [============================>.] - ETA: 0s - loss: 409000.1562\n","Epoch 397: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 407170.9375 - val_loss: 1312174.3750\n","Epoch 398/1000\n","76/75 [==============================] - ETA: 0s - loss: 387597.5625\n","Epoch 398: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 387597.5625 - val_loss: 1319886.3750\n","Epoch 399/1000\n","75/75 [============================>.] - ETA: 0s - loss: 399265.3750\n","Epoch 399: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 397035.6250 - val_loss: 1230119.3750\n","Epoch 400/1000\n","75/75 [============================>.] - ETA: 0s - loss: 402660.4062\n","Epoch 400: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 400293.3125 - val_loss: 1356849.2500\n","Epoch 401/1000\n","74/75 [============================>.] - ETA: 0s - loss: 423199.8438\n","Epoch 401: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 416976.7500 - val_loss: 1263529.8750\n","Epoch 402/1000\n","76/75 [==============================] - ETA: 0s - loss: 387719.0000\n","Epoch 402: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 387719.0000 - val_loss: 1271124.3750\n","Epoch 403/1000\n","74/75 [============================>.] - ETA: 0s - loss: 424898.3438\n","Epoch 403: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 417989.5938 - val_loss: 1409056.8750\n","Epoch 404/1000\n","76/75 [==============================] - ETA: 0s - loss: 443562.3438\n","Epoch 404: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 443562.3438 - val_loss: 1235386.0000\n","Epoch 405/1000\n","76/75 [==============================] - ETA: 0s - loss: 386998.5938\n","Epoch 405: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 386998.5938 - val_loss: 1279468.5000\n","Epoch 406/1000\n","76/75 [==============================] - ETA: 0s - loss: 402500.5312\n","Epoch 406: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 402500.5312 - val_loss: 1368394.1250\n","Epoch 407/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 444307.0625\n","Epoch 407: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 432731.4688 - val_loss: 1171550.1250\n","Epoch 408/1000\n","76/75 [==============================] - ETA: 0s - loss: 400036.5938\n","Epoch 408: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 400036.5938 - val_loss: 1362632.6250\n","Epoch 409/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 419309.6562\n","Epoch 409: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 408947.7188 - val_loss: 1288791.3750\n","Epoch 410/1000\n","75/75 [============================>.] - ETA: 0s - loss: 395139.9688\n","Epoch 410: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 392776.8125 - val_loss: 1305156.2500\n","Epoch 411/1000\n","75/75 [============================>.] - ETA: 0s - loss: 416459.5938\n","Epoch 411: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 413508.2188 - val_loss: 1320413.0000\n","Epoch 412/1000\n","74/75 [============================>.] - ETA: 0s - loss: 392490.2188\n","Epoch 412: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 387330.0000 - val_loss: 1363531.1250\n","Epoch 413/1000\n","75/75 [============================>.] - ETA: 0s - loss: 402263.2812\n","Epoch 413: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 399738.0625 - val_loss: 1285949.7500\n","Epoch 414/1000\n","76/75 [==============================] - ETA: 0s - loss: 399342.3438\n","Epoch 414: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 399342.3438 - val_loss: 1444588.8750\n","Epoch 415/1000\n","75/75 [============================>.] - ETA: 0s - loss: 422024.8750\n","Epoch 415: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 420415.5625 - val_loss: 1193364.0000\n","Epoch 416/1000\n","75/75 [============================>.] - ETA: 0s - loss: 431248.9062\n","Epoch 416: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 428432.3750 - val_loss: 1443710.2500\n","Epoch 417/1000\n","76/75 [==============================] - ETA: 0s - loss: 417878.6250\n","Epoch 417: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 417878.6250 - val_loss: 1248707.8750\n","Epoch 418/1000\n","76/75 [==============================] - ETA: 0s - loss: 393668.7188\n","Epoch 418: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 393668.7188 - val_loss: 1414861.1250\n","Epoch 419/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 421071.0938\n","Epoch 419: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 410423.0312 - val_loss: 1476449.1250\n","Epoch 420/1000\n","75/75 [============================>.] - ETA: 0s - loss: 401918.4688\n","Epoch 420: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 400081.9688 - val_loss: 1302640.7500\n","Epoch 421/1000\n","76/75 [==============================] - ETA: 0s - loss: 383663.5938\n","Epoch 421: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 383663.5938 - val_loss: 1272234.3750\n","Epoch 422/1000\n","76/75 [==============================] - ETA: 0s - loss: 399514.0625\n","Epoch 422: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 399514.0625 - val_loss: 1444471.7500\n","Epoch 423/1000\n","75/75 [============================>.] - ETA: 0s - loss: 412346.8125\n","Epoch 423: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 410089.9062 - val_loss: 1256536.1250\n","Epoch 424/1000\n","75/75 [============================>.] - ETA: 0s - loss: 401866.3125\n","Epoch 424: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 399995.5312 - val_loss: 1395844.7500\n","Epoch 425/1000\n","76/75 [==============================] - ETA: 0s - loss: 401568.4375\n","Epoch 425: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 401568.4375 - val_loss: 1313852.0000\n","Epoch 426/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 396294.2188\n","Epoch 426: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 387453.9688 - val_loss: 1295666.7500\n","Epoch 427/1000\n","75/75 [============================>.] - ETA: 0s - loss: 390140.1250\n","Epoch 427: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 390152.8438 - val_loss: 1428179.6250\n","Epoch 428/1000\n","76/75 [==============================] - ETA: 0s - loss: 407090.5312\n","Epoch 428: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 407090.5312 - val_loss: 1266091.8750\n","Epoch 429/1000\n","75/75 [============================>.] - ETA: 0s - loss: 392777.1562\n","Epoch 429: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 390634.3438 - val_loss: 1400733.7500\n","Epoch 430/1000\n","75/75 [============================>.] - ETA: 0s - loss: 452609.9688\n","Epoch 430: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 449581.3750 - val_loss: 1155672.0000\n","Epoch 431/1000\n","74/75 [============================>.] - ETA: 0s - loss: 413464.8750\n","Epoch 431: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 408193.6562 - val_loss: 1460344.7500\n","Epoch 432/1000\n","76/75 [==============================] - ETA: 0s - loss: 436187.0000\n","Epoch 432: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 436187.0000 - val_loss: 1288978.0000\n","Epoch 433/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 406103.0312\n","Epoch 433: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 397987.4375 - val_loss: 1462802.8750\n","Epoch 434/1000\n","76/75 [==============================] - ETA: 0s - loss: 388688.3125\n","Epoch 434: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 388688.3125 - val_loss: 1318030.6250\n","Epoch 435/1000\n","75/75 [============================>.] - ETA: 0s - loss: 394642.2812\n","Epoch 435: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 392776.5938 - val_loss: 1483054.3750\n","Epoch 436/1000\n","76/75 [==============================] - ETA: 0s - loss: 431186.1250\n","Epoch 436: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 431186.1250 - val_loss: 1187397.5000\n","Epoch 437/1000\n","75/75 [============================>.] - ETA: 0s - loss: 398952.7812\n","Epoch 437: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 398466.5938 - val_loss: 1367237.8750\n","Epoch 438/1000\n","76/75 [==============================] - ETA: 0s - loss: 392918.0625\n","Epoch 438: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 392918.0625 - val_loss: 1386468.7500\n","Epoch 439/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 398179.7500\n","Epoch 439: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 390779.7812 - val_loss: 1391388.0000\n","Epoch 440/1000\n","75/75 [============================>.] - ETA: 0s - loss: 406929.7500\n","Epoch 440: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 405632.6250 - val_loss: 1242160.1250\n","Epoch 441/1000\n","74/75 [============================>.] - ETA: 0s - loss: 417269.9375\n","Epoch 441: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 411490.2500 - val_loss: 1552410.8750\n","Epoch 442/1000\n","76/75 [==============================] - ETA: 0s - loss: 454112.3125\n","Epoch 442: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 454112.3125 - val_loss: 1271797.6250\n","Epoch 443/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 413055.8125\n","Epoch 443: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 403981.1875 - val_loss: 1408057.1250\n","Epoch 444/1000\n","74/75 [============================>.] - ETA: 0s - loss: 407713.3438\n","Epoch 444: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 402709.0312 - val_loss: 1285100.8750\n","Epoch 445/1000\n","76/75 [==============================] - ETA: 0s - loss: 386460.9688\n","Epoch 445: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 386460.9688 - val_loss: 1376345.6250\n","Epoch 446/1000\n","75/75 [============================>.] - ETA: 0s - loss: 403661.2500\n","Epoch 446: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 401328.1875 - val_loss: 1323008.5000\n","Epoch 447/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 389655.2812\n","Epoch 447: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 381718.8125 - val_loss: 1271992.2500\n","Epoch 448/1000\n","74/75 [============================>.] - ETA: 0s - loss: 393617.1250\n","Epoch 448: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 387989.7812 - val_loss: 1290017.1250\n","Epoch 449/1000\n","75/75 [============================>.] - ETA: 0s - loss: 389184.6250\n","Epoch 449: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 390380.9062 - val_loss: 1267431.7500\n","Epoch 450/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 408329.4688\n","Epoch 450: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 399137.8438 - val_loss: 1387639.1250\n","Epoch 451/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 407370.2500\n","Epoch 451: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 397920.4062 - val_loss: 1353038.8750\n","Epoch 452/1000\n","75/75 [============================>.] - ETA: 0s - loss: 399803.4688\n","Epoch 452: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 397369.4375 - val_loss: 1409423.5000\n","Epoch 453/1000\n","76/75 [==============================] - ETA: 0s - loss: 422205.4375\n","Epoch 453: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 422205.4375 - val_loss: 1249605.8750\n","Epoch 454/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 403683.8438\n","Epoch 454: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 16ms/step - loss: 394297.1562 - val_loss: 1402043.2500\n","Epoch 455/1000\n","76/75 [==============================] - ETA: 0s - loss: 389428.5312\n","Epoch 455: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 389428.5312 - val_loss: 1346621.1250\n","Epoch 456/1000\n","75/75 [============================>.] - ETA: 0s - loss: 397212.0312\n","Epoch 456: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 394827.9375 - val_loss: 1436464.8750\n","Epoch 457/1000\n","75/75 [============================>.] - ETA: 0s - loss: 407450.0000\n","Epoch 457: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 404863.7188 - val_loss: 1417028.8750\n","Epoch 458/1000\n","75/75 [============================>.] - ETA: 0s - loss: 405855.0312\n","Epoch 458: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 403337.7500 - val_loss: 1213504.3750\n","Epoch 459/1000\n","75/75 [============================>.] - ETA: 0s - loss: 444318.7500\n","Epoch 459: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 441787.1875 - val_loss: 1456405.1250\n","Epoch 460/1000\n","75/75 [============================>.] - ETA: 0s - loss: 412587.4062\n","Epoch 460: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 410280.3125 - val_loss: 1268415.0000\n","Epoch 461/1000\n","76/75 [==============================] - ETA: 0s - loss: 391223.6250\n","Epoch 461: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 391223.6250 - val_loss: 1364796.0000\n","Epoch 462/1000\n","76/75 [==============================] - ETA: 0s - loss: 397135.8438\n","Epoch 462: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 397135.8438 - val_loss: 1339440.5000\n","Epoch 463/1000\n","74/75 [============================>.] - ETA: 0s - loss: 392924.4375\n","Epoch 463: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 386928.9375 - val_loss: 1297842.2500\n","Epoch 464/1000\n","76/75 [==============================] - ETA: 0s - loss: 402110.9375\n","Epoch 464: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 402110.9375 - val_loss: 1447858.0000\n","Epoch 465/1000\n","75/75 [============================>.] - ETA: 0s - loss: 411419.5000\n","Epoch 465: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 410103.7500 - val_loss: 1303646.5000\n","Epoch 466/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 395567.9062\n","Epoch 466: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 385996.7188 - val_loss: 1363905.6250\n","Epoch 467/1000\n","76/75 [==============================] - ETA: 0s - loss: 417045.2812\n","Epoch 467: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 417045.2812 - val_loss: 1213492.1250\n","Epoch 468/1000\n","75/75 [============================>.] - ETA: 0s - loss: 399673.4375\n","Epoch 468: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 397056.7188 - val_loss: 1399560.3750\n","Epoch 469/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 425090.0312\n","Epoch 469: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 415637.1875 - val_loss: 1330824.3750\n","Epoch 470/1000\n","74/75 [============================>.] - ETA: 0s - loss: 387301.5625\n","Epoch 470: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 384031.8125 - val_loss: 1322659.8750\n","Epoch 471/1000\n","73/75 [===========================>..] - ETA: 0s - loss: 425267.6875\n","Epoch 471: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 416135.7500 - val_loss: 1566181.5000\n","Epoch 472/1000\n","75/75 [============================>.] - ETA: 0s - loss: 436750.7500\n","Epoch 472: val_loss did not improve from 1091710.25000\n","75/75 [==============================] - 1s 17ms/step - loss: 433652.0000 - val_loss: 1171912.0000\n"]}]},{"cell_type":"code","source":["for i in tqdm(range(10)):\n","  \n","  make_csv(f'/content/drive/MyDrive/농산물예측/aT_data/aT_test_raw/sep_{i}/','test') \n","  \n","  for j in range(37):\n","    # print(f'sep:{i}, 품목:{j}')\n","    # print(len(globals()[f'test_total_sep_{i}_{j}'])) # df row수 - 0이 있나 확인하기 위해서\n","\n","    \n","\n","    # 정규 표현식 적용\n","    globals()[f'test_total_sep_{i}_{j}'].columns = globals()[f'test_total_sep_{i}_{j}'].columns.str.replace(r'\\([^)]*\\)','',regex= True)\n","\n","    # 스케일링 & 타겟값 변환\n","    #globals()[f'train_scaled_{i}'] = scaling_df(globals()[f'total_{i}'])\n","    globals()[f'test_total_sep_{i}_{j}'] = scaling_df(globals()[f'test_total_sep_{i}_{j}'])\n","\n","    # train 데이터랑 똑같이 selection\n","    globals()[f'test_total_sep_{i}_{j}'] = globals()[f'test_total_sep_{i}_{j}'][list(globals()[f'train_scaled_{j}'].columns)]\n","\n","    \n","\n","    # 해당일자평균가격 테스트 데이터에서는 타겟값 없애고 돌리네\n","    globals()[f'test_total_sep_{i}_{j}'].drop('해당일자_전체평균가격',axis=1, inplace=True)\n","\n","\n","    file_number = j\n","\n","    # nan 처리\n","    globals()[f'test_total_sep_{i}_{j}'].fillna(0, inplace = True)\n","    # 형상 맞추기 코드는 아직 넣지 않았다. (안넣어도 되지 않을까?)\n","\n","    # x_test 생성\n","    df_test = astype_data(globals()[f'test_total_sep_{i}_{j}'].values.reshape(1, globals()[f'test_total_sep_{i}_{j}'].values.shape[0], globals()[f'test_total_sep_{i}_{j}'].values.shape[1]))\n","\n","\n","\n","    # model test\n","    if os.path.exists(f'/content/drive/MyDrive/농산물예측/aT_data/model_output{try_cnt}') == False:\n","            os.mkdir(f'/content/drive/MyDrive/농산물예측/aT_data/model_output{try_cnt}')\n","\n","    if os.path.exists(f'/content/drive/MyDrive/농산물예측/aT_data/model_output{try_cnt}/set_{i}') == False:\n","            os.mkdir(f'/content/drive/MyDrive/농산물예측/aT_data/model_output{try_cnt}/set_{i}')\n","\n","    # 해당하는 모델 불러오기\n","    model_test = tf.keras.models.load_model(f'/content/drive/MyDrive/농산물예측/aT_data/model{try_cnt}/transformer-{file_number}-{epoch}-{batch}.h5')\n","    pred = model_test.predict(df_test)\n","\n","\n","    # 여기서 다시 형변환 해주자!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!np.expm1\n","    # pred = np.expm1(pred)\n","\n","    \n","\n","    # 결과 저장\n","    save_df = pd.DataFrame(pred).T\n","    save_df.to_csv(f'/content/drive/MyDrive/농산물예측/aT_data/model_output{try_cnt}/set_{i}/predict_{file_number}.csv', index=False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_vZmOsDAMLGi","executionInfo":{"status":"ok","timestamp":1664517423321,"user_tz":-540,"elapsed":796473,"user":{"displayName":"이태영","userId":"15551978220746215296"}},"outputId":"903cb1e5-835a-4358-aaed-cf6ba00ae50f"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 0/10 [00:00<?, ?it/s]\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","100%|██████████| 37/37 [00:00<00:00, 189.47it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n"," 24%|██▍       | 9/37 [00:00<00:00, 86.22it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:00<00:00, 86.05it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:00<00:00, 79.72it/s]\u001b[A\n","100%|██████████| 37/37 [00:00<00:00, 79.86it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:02, 16.10it/s]\u001b[A\n"," 11%|█         | 4/37 [00:00<00:02, 11.48it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:00<00:02, 13.32it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:00<00:02, 14.39it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:00<00:01, 15.19it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:00<00:01, 15.68it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:00<00:01, 15.96it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:01<00:01, 16.01it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:01<00:01, 15.92it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:01<00:01, 15.91it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:01<00:00, 15.97it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:01<00:00, 16.04it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:01<00:00, 15.79it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:01<00:00, 16.04it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:01<00:00, 16.05it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:02<00:00, 15.98it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:02<00:00, 16.15it/s]\u001b[A\n","100%|██████████| 37/37 [00:02<00:00, 15.58it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:02, 16.70it/s]\u001b[A\n"," 11%|█         | 4/37 [00:00<00:02, 15.43it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:00<00:01, 15.52it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:00<00:01, 15.84it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:00<00:01, 15.73it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:00<00:01, 15.25it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:00<00:01, 15.02it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:01<00:01, 15.14it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:01<00:01, 15.24it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:01<00:01, 15.16it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:01<00:00, 15.01it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:01<00:00, 15.40it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:01<00:00, 15.04it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:01<00:00, 15.21it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:01<00:00, 15.39it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:02<00:00, 15.32it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:02<00:00, 15.17it/s]\u001b[A\n","100%|██████████| 37/37 [00:02<00:00, 15.33it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  8%|▊         | 3/37 [00:00<00:01, 22.50it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:00<00:01, 21.95it/s]\u001b[A\n"," 24%|██▍       | 9/37 [00:00<00:01, 22.80it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:00<00:01, 22.56it/s]\u001b[A\n"," 41%|████      | 15/37 [00:00<00:00, 22.23it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:00<00:00, 22.25it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:00<00:00, 22.39it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:01<00:00, 22.60it/s]\u001b[A\n"," 73%|███████▎  | 27/37 [00:01<00:00, 22.34it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:01<00:00, 22.42it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:01<00:00, 22.82it/s]\u001b[A\n","100%|██████████| 37/37 [00:01<00:00, 22.47it/s]\n","WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f9ca365e830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f9ca6cf6950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"," 10%|█         | 1/10 [01:13<10:58, 73.17s/it]\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:03<02:07,  3.53s/it]\u001b[A\n","  5%|▌         | 2/37 [00:03<00:58,  1.66s/it]\u001b[A\n","  8%|▊         | 3/37 [00:04<00:34,  1.02s/it]\u001b[A\n"," 11%|█         | 4/37 [00:04<00:24,  1.33it/s]\u001b[A\n"," 14%|█▎        | 5/37 [00:04<00:19,  1.68it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:05<00:14,  2.13it/s]\u001b[A\n"," 19%|█▉        | 7/37 [00:05<00:11,  2.55it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:05<00:06,  4.36it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:05<00:06,  4.25it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:06<00:06,  4.15it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:06<00:05,  4.22it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:06<00:06,  3.83it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:06<00:04,  4.75it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:07<00:04,  4.36it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:07<00:04,  4.16it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:07<00:04,  4.14it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:08<00:04,  4.05it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:08<00:04,  3.45it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:08<00:04,  3.39it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:08<00:04,  3.50it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:09<00:01,  5.65it/s]\u001b[A\n"," 78%|███████▊  | 29/37 [00:09<00:01,  7.02it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:09<00:01,  5.92it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:10<00:01,  5.24it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:10<00:00,  6.05it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:10<00:00,  4.61it/s]\u001b[A\n"," 95%|█████████▍| 35/37 [00:11<00:00,  3.97it/s]\u001b[A\n"," 97%|█████████▋| 36/37 [00:11<00:00,  3.97it/s]\u001b[A\n","100%|██████████| 37/37 [00:11<00:00,  3.15it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:00<00:09,  3.71it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:09,  3.65it/s]\u001b[A\n","  8%|▊         | 3/37 [00:00<00:09,  3.60it/s]\u001b[A\n"," 11%|█         | 4/37 [00:01<00:09,  3.42it/s]\u001b[A\n"," 14%|█▎        | 5/37 [00:01<00:08,  3.65it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:01<00:08,  3.77it/s]\u001b[A\n"," 19%|█▉        | 7/37 [00:01<00:07,  3.75it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:02<00:07,  3.79it/s]\u001b[A\n"," 24%|██▍       | 9/37 [00:02<00:07,  3.60it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:02<00:07,  3.43it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:03<00:07,  3.42it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:03<00:08,  3.05it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:03<00:07,  3.39it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:03<00:06,  3.58it/s]\u001b[A\n"," 41%|████      | 15/37 [00:04<00:06,  3.46it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:04<00:05,  3.56it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:04<00:05,  3.59it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:05<00:05,  3.56it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:05<00:05,  3.48it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:05<00:04,  3.47it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:05<00:04,  3.52it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:06<00:04,  3.70it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:06<00:03,  3.80it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:06<00:03,  3.69it/s]\u001b[A\n"," 68%|██████▊   | 25/37 [00:06<00:03,  3.84it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:07<00:02,  3.74it/s]\u001b[A\n"," 73%|███████▎  | 27/37 [00:07<00:02,  3.61it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:07<00:02,  3.62it/s]\u001b[A\n"," 78%|███████▊  | 29/37 [00:08<00:02,  3.56it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:08<00:01,  3.60it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:08<00:01,  3.65it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:09<00:01,  3.13it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:09<00:01,  3.34it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:09<00:00,  3.61it/s]\u001b[A\n"," 95%|█████████▍| 35/37 [00:09<00:00,  3.24it/s]\u001b[A\n"," 97%|█████████▋| 36/37 [00:10<00:00,  3.43it/s]\u001b[A\n","100%|██████████| 37/37 [00:10<00:00,  3.54it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:00<00:12,  2.96it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:13,  2.54it/s]\u001b[A\n","  8%|▊         | 3/37 [00:01<00:15,  2.24it/s]\u001b[A\n"," 11%|█         | 4/37 [00:01<00:10,  3.22it/s]\u001b[A\n"," 14%|█▎        | 5/37 [00:01<00:07,  4.14it/s]\u001b[A\n"," 19%|█▉        | 7/37 [00:01<00:04,  6.60it/s]\u001b[A\n"," 24%|██▍       | 9/37 [00:01<00:03,  8.77it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:01<00:02, 10.57it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:02<00:02, 11.89it/s]\u001b[A\n"," 41%|████      | 15/37 [00:02<00:01, 13.07it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:02<00:01, 13.99it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:02<00:01, 14.72it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:02<00:01, 15.20it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:02<00:00, 15.56it/s]\u001b[A\n"," 68%|██████▊   | 25/37 [00:02<00:00, 15.86it/s]\u001b[A\n"," 73%|███████▎  | 27/37 [00:02<00:00, 15.91it/s]\u001b[A\n"," 78%|███████▊  | 29/37 [00:02<00:00, 16.09it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:03<00:00, 16.28it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:03<00:00, 16.39it/s]\u001b[A\n"," 95%|█████████▍| 35/37 [00:03<00:00, 16.36it/s]\u001b[A\n","100%|██████████| 37/37 [00:03<00:00, 10.67it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:02, 15.90it/s]\u001b[A\n"," 11%|█         | 4/37 [00:00<00:02, 15.50it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:00<00:02, 14.71it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:00<00:01, 14.81it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:00<00:01, 15.19it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:00<00:01, 15.05it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:01<00:01, 11.61it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:01<00:01, 12.49it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:01<00:01, 13.16it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:01<00:01, 13.28it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:01<00:01, 13.85it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:01<00:00, 14.29it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:01<00:00, 14.63it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:01<00:00, 14.80it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:02<00:00, 15.05it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:02<00:00, 15.18it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:02<00:00, 15.10it/s]\u001b[A\n","100%|██████████| 37/37 [00:02<00:00, 14.35it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  8%|▊         | 3/37 [00:00<00:01, 23.19it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:00<00:01, 22.83it/s]\u001b[A\n"," 24%|██▍       | 9/37 [00:00<00:01, 23.58it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:00<00:01, 23.18it/s]\u001b[A\n"," 41%|████      | 15/37 [00:00<00:00, 22.82it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:00<00:00, 22.48it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:00<00:00, 22.20it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:01<00:00, 22.45it/s]\u001b[A\n"," 73%|███████▎  | 27/37 [00:01<00:00, 22.38it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:01<00:00, 22.33it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:01<00:00, 22.32it/s]\u001b[A\n","100%|██████████| 37/37 [00:01<00:00, 22.52it/s]\n"," 20%|██        | 2/10 [02:33<10:17, 77.14s/it]\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:02<01:46,  2.96s/it]\u001b[A\n","  5%|▌         | 2/37 [00:03<00:49,  1.41s/it]\u001b[A\n","  8%|▊         | 3/37 [00:03<00:30,  1.12it/s]\u001b[A\n"," 11%|█         | 4/37 [00:03<00:22,  1.45it/s]\u001b[A\n"," 14%|█▎        | 5/37 [00:04<00:19,  1.68it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:04<00:15,  2.06it/s]\u001b[A\n"," 19%|█▉        | 7/37 [00:04<00:12,  2.45it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:05<00:05,  4.50it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:05<00:06,  4.33it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:05<00:06,  4.07it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:05<00:06,  3.98it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:06<00:05,  3.89it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:06<00:04,  5.02it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:06<00:05,  3.98it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:07<00:05,  3.62it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:07<00:05,  3.35it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:07<00:04,  3.52it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:08<00:04,  3.43it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:08<00:04,  3.57it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:08<00:03,  3.66it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:08<00:01,  5.72it/s]\u001b[A\n"," 78%|███████▊  | 29/37 [00:09<00:01,  6.78it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:09<00:01,  5.97it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:09<00:01,  4.69it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:10<00:00,  5.42it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:10<00:00,  4.96it/s]\u001b[A\n"," 95%|█████████▍| 35/37 [00:10<00:00,  4.47it/s]\u001b[A\n"," 97%|█████████▋| 36/37 [00:11<00:00,  4.35it/s]\u001b[A\n","100%|██████████| 37/37 [00:11<00:00,  3.27it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:00<00:11,  3.21it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:09,  3.52it/s]\u001b[A\n","  8%|▊         | 3/37 [00:00<00:10,  3.28it/s]\u001b[A\n"," 11%|█         | 4/37 [00:01<00:09,  3.47it/s]\u001b[A\n"," 14%|█▎        | 5/37 [00:01<00:09,  3.47it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:01<00:08,  3.71it/s]\u001b[A\n"," 19%|█▉        | 7/37 [00:02<00:08,  3.43it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:02<00:08,  3.44it/s]\u001b[A\n"," 24%|██▍       | 9/37 [00:02<00:07,  3.51it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:02<00:07,  3.47it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:03<00:07,  3.42it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:03<00:07,  3.39it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:03<00:06,  3.46it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:04<00:06,  3.45it/s]\u001b[A\n"," 41%|████      | 15/37 [00:04<00:06,  3.38it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:04<00:06,  3.47it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:04<00:05,  3.57it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:05<00:07,  2.56it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:05<00:06,  2.89it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:06<00:06,  2.48it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:06<00:06,  2.50it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:07<00:05,  2.71it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:07<00:05,  2.78it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:07<00:04,  2.87it/s]\u001b[A\n"," 68%|██████▊   | 25/37 [00:07<00:03,  3.01it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:08<00:03,  2.82it/s]\u001b[A\n"," 73%|███████▎  | 27/37 [00:08<00:03,  2.66it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:09<00:03,  2.71it/s]\u001b[A\n"," 78%|███████▊  | 29/37 [00:09<00:02,  2.98it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:09<00:02,  3.13it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:09<00:01,  3.18it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:10<00:01,  3.14it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:10<00:01,  3.29it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:10<00:00,  3.25it/s]\u001b[A\n"," 95%|█████████▍| 35/37 [00:11<00:00,  2.99it/s]\u001b[A\n"," 97%|█████████▋| 36/37 [00:11<00:00,  2.87it/s]\u001b[A\n","100%|██████████| 37/37 [00:11<00:00,  3.09it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:00<00:12,  2.96it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:11,  2.95it/s]\u001b[A\n","  8%|▊         | 3/37 [00:01<00:13,  2.48it/s]\u001b[A\n"," 11%|█         | 4/37 [00:01<00:10,  3.02it/s]\u001b[A\n"," 14%|█▎        | 5/37 [00:01<00:08,  3.71it/s]\u001b[A\n"," 19%|█▉        | 7/37 [00:01<00:04,  6.08it/s]\u001b[A\n"," 24%|██▍       | 9/37 [00:01<00:03,  8.28it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:01<00:02, 10.06it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:02<00:02, 11.45it/s]\u001b[A\n"," 41%|████      | 15/37 [00:02<00:01, 12.79it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:02<00:01, 13.70it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:02<00:01, 14.51it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:02<00:01, 15.18it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:02<00:00, 15.55it/s]\u001b[A\n"," 68%|██████▊   | 25/37 [00:02<00:00, 15.63it/s]\u001b[A\n"," 73%|███████▎  | 27/37 [00:02<00:00, 15.94it/s]\u001b[A\n"," 78%|███████▊  | 29/37 [00:03<00:00, 16.12it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:03<00:00, 16.01it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:03<00:00, 16.00it/s]\u001b[A\n"," 95%|█████████▍| 35/37 [00:03<00:00, 16.05it/s]\u001b[A\n","100%|██████████| 37/37 [00:03<00:00, 10.52it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:02, 15.36it/s]\u001b[A\n"," 11%|█         | 4/37 [00:00<00:02, 14.83it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:00<00:02, 14.77it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:00<00:01, 14.95it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:00<00:01, 14.95it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:00<00:01, 15.19it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:00<00:01, 15.23it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:01<00:01, 15.31it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:01<00:01, 15.11it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:01<00:01, 15.09it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:01<00:01, 14.83it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:01<00:00, 15.11it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:01<00:00, 15.39it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:01<00:00, 15.40it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:01<00:00, 15.43it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:02<00:00, 15.40it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:02<00:00, 15.48it/s]\u001b[A\n","100%|██████████| 37/37 [00:02<00:00, 15.21it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  8%|▊         | 3/37 [00:00<00:01, 23.88it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:00<00:01, 22.60it/s]\u001b[A\n"," 24%|██▍       | 9/37 [00:00<00:01, 23.66it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:00<00:01, 23.01it/s]\u001b[A\n"," 41%|████      | 15/37 [00:00<00:01, 17.34it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:00<00:01, 18.85it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:01<00:00, 19.91it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:01<00:00, 20.66it/s]\u001b[A\n"," 73%|███████▎  | 27/37 [00:01<00:00, 21.07it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:01<00:00, 21.50it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:01<00:00, 21.79it/s]\u001b[A\n","100%|██████████| 37/37 [00:01<00:00, 21.03it/s]\n"," 30%|███       | 3/10 [03:53<09:11, 78.85s/it]\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:02<01:47,  2.98s/it]\u001b[A\n","  5%|▌         | 2/37 [00:03<00:48,  1.39s/it]\u001b[A\n","  8%|▊         | 3/37 [00:03<00:30,  1.11it/s]\u001b[A\n"," 11%|█         | 4/37 [00:04<00:23,  1.38it/s]\u001b[A\n"," 14%|█▎        | 5/37 [00:04<00:19,  1.65it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:04<00:15,  2.03it/s]\u001b[A\n"," 19%|█▉        | 7/37 [00:04<00:12,  2.32it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:05<00:06,  4.06it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:05<00:06,  4.01it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:05<00:06,  3.99it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:06<00:06,  3.69it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:06<00:06,  3.83it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:06<00:04,  4.50it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:07<00:05,  3.87it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:07<00:05,  3.71it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:07<00:05,  3.41it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:08<00:05,  3.39it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:08<00:04,  3.41it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:08<00:04,  3.52it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:08<00:03,  3.60it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:09<00:02,  5.23it/s]\u001b[A\n"," 78%|███████▊  | 29/37 [00:09<00:01,  6.53it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:09<00:01,  5.16it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:10<00:01,  4.95it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:10<00:00,  5.00it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:10<00:00,  4.77it/s]\u001b[A\n"," 95%|█████████▍| 35/37 [00:11<00:00,  4.52it/s]\u001b[A\n"," 97%|█████████▋| 36/37 [00:11<00:00,  4.23it/s]\u001b[A\n","100%|██████████| 37/37 [00:11<00:00,  3.17it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:00<00:10,  3.55it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:17,  2.04it/s]\u001b[A\n","  8%|▊         | 3/37 [00:01<00:13,  2.48it/s]\u001b[A\n"," 11%|█         | 4/37 [00:01<00:11,  2.77it/s]\u001b[A\n"," 14%|█▎        | 5/37 [00:01<00:10,  3.04it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:02<00:09,  3.37it/s]\u001b[A\n"," 19%|█▉        | 7/37 [00:02<00:09,  3.05it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:02<00:09,  3.11it/s]\u001b[A\n"," 24%|██▍       | 9/37 [00:03<00:08,  3.14it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:03<00:08,  3.07it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:03<00:09,  2.84it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:04<00:08,  3.11it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:04<00:07,  3.01it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:04<00:07,  3.20it/s]\u001b[A\n"," 41%|████      | 15/37 [00:04<00:06,  3.31it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:05<00:06,  3.31it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:05<00:06,  2.97it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:05<00:06,  3.11it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:06<00:05,  3.34it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:06<00:05,  3.19it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:06<00:04,  3.25it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:07<00:04,  3.19it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:07<00:04,  3.30it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:07<00:03,  3.44it/s]\u001b[A\n"," 68%|██████▊   | 25/37 [00:07<00:03,  3.45it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:08<00:03,  3.45it/s]\u001b[A\n"," 73%|███████▎  | 27/37 [00:08<00:02,  3.44it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:08<00:02,  3.49it/s]\u001b[A\n"," 78%|███████▊  | 29/37 [00:09<00:02,  3.59it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:09<00:01,  3.65it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:09<00:01,  3.57it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:09<00:01,  3.55it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:10<00:01,  3.62it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:10<00:00,  3.68it/s]\u001b[A\n"," 95%|█████████▍| 35/37 [00:10<00:00,  3.42it/s]\u001b[A\n"," 97%|█████████▋| 36/37 [00:11<00:00,  3.08it/s]\u001b[A\n","100%|██████████| 37/37 [00:11<00:00,  3.21it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:00<00:10,  3.33it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:11,  2.98it/s]\u001b[A\n","  8%|▊         | 3/37 [00:01<00:13,  2.52it/s]\u001b[A\n"," 11%|█         | 4/37 [00:01<00:11,  2.97it/s]\u001b[A\n"," 14%|█▎        | 5/37 [00:01<00:09,  3.25it/s]\u001b[A\n"," 19%|█▉        | 7/37 [00:01<00:05,  5.47it/s]\u001b[A\n"," 24%|██▍       | 9/37 [00:01<00:03,  7.57it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:02<00:02,  9.44it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:02<00:02, 11.00it/s]\u001b[A\n"," 41%|████      | 15/37 [00:02<00:01, 12.44it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:02<00:01, 13.50it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:02<00:01, 14.35it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:02<00:01, 14.89it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:02<00:00, 15.30it/s]\u001b[A\n"," 68%|██████▊   | 25/37 [00:02<00:00, 15.35it/s]\u001b[A\n"," 73%|███████▎  | 27/37 [00:02<00:00, 15.42it/s]\u001b[A\n"," 78%|███████▊  | 29/37 [00:03<00:00, 15.38it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:03<00:00, 15.57it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:03<00:00, 15.82it/s]\u001b[A\n"," 95%|█████████▍| 35/37 [00:03<00:00, 15.96it/s]\u001b[A\n","100%|██████████| 37/37 [00:03<00:00, 10.21it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:02, 16.25it/s]\u001b[A\n"," 11%|█         | 4/37 [00:00<00:02, 15.92it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:00<00:02, 15.22it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:00<00:01, 15.56it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:00<00:01, 15.50it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:00<00:01, 15.23it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:00<00:01, 15.29it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:01<00:01, 15.14it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:01<00:01, 14.90it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:01<00:01, 14.77it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:01<00:01, 14.44it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:01<00:00, 14.51it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:01<00:00, 15.01it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:01<00:00, 14.68it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:02<00:00, 14.71it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:02<00:00, 14.92it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:02<00:00, 15.16it/s]\u001b[A\n","100%|██████████| 37/37 [00:02<00:00, 15.07it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  8%|▊         | 3/37 [00:00<00:01, 22.83it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:00<00:01, 22.90it/s]\u001b[A\n"," 24%|██▍       | 9/37 [00:00<00:01, 23.19it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:00<00:01, 22.99it/s]\u001b[A\n"," 41%|████      | 15/37 [00:00<00:00, 22.86it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:00<00:00, 22.74it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:00<00:00, 22.30it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:01<00:00, 22.32it/s]\u001b[A\n"," 73%|███████▎  | 27/37 [00:01<00:00, 22.36it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:01<00:00, 22.08it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:01<00:00, 22.09it/s]\u001b[A\n","100%|██████████| 37/37 [00:01<00:00, 22.34it/s]\n"," 40%|████      | 4/10 [05:14<07:56, 79.41s/it]\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:02<01:24,  2.35s/it]\u001b[A\n","  5%|▌         | 2/37 [00:02<00:41,  1.18s/it]\u001b[A\n","  8%|▊         | 3/37 [00:02<00:26,  1.31it/s]\u001b[A\n"," 11%|█         | 4/37 [00:03<00:20,  1.65it/s]\u001b[A\n"," 14%|█▎        | 5/37 [00:03<00:17,  1.87it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:04<00:15,  2.05it/s]\u001b[A\n"," 19%|█▉        | 7/37 [00:04<00:12,  2.39it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:04<00:06,  4.24it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:04<00:06,  4.21it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:05<00:06,  4.02it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:05<00:05,  4.03it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:05<00:06,  3.82it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:06<00:04,  4.50it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:06<00:04,  4.38it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:06<00:04,  4.11it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:06<00:04,  4.13it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:07<00:04,  4.04it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:07<00:04,  3.92it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:07<00:03,  3.95it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:07<00:03,  3.86it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:08<00:01,  6.13it/s]\u001b[A\n"," 78%|███████▊  | 29/37 [00:08<00:01,  7.62it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:08<00:01,  6.68it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:09<00:01,  5.64it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:09<00:00,  6.05it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:09<00:00,  5.26it/s]\u001b[A\n"," 95%|█████████▍| 35/37 [00:09<00:00,  4.52it/s]\u001b[A\n"," 97%|█████████▋| 36/37 [00:10<00:00,  3.95it/s]\u001b[A\n","100%|██████████| 37/37 [00:10<00:00,  3.51it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:00<00:09,  3.75it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:09,  3.74it/s]\u001b[A\n","  8%|▊         | 3/37 [00:00<00:10,  3.09it/s]\u001b[A\n"," 11%|█         | 4/37 [00:01<00:11,  2.76it/s]\u001b[A\n"," 14%|█▎        | 5/37 [00:01<00:10,  3.02it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:02<00:11,  2.65it/s]\u001b[A\n"," 19%|█▉        | 7/37 [00:02<00:11,  2.60it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:02<00:11,  2.59it/s]\u001b[A\n"," 24%|██▍       | 9/37 [00:03<00:10,  2.72it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:03<00:09,  2.95it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:03<00:08,  3.06it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:04<00:07,  3.21it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:04<00:07,  3.40it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:04<00:06,  3.61it/s]\u001b[A\n"," 41%|████      | 15/37 [00:04<00:05,  3.70it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:05<00:05,  3.62it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:05<00:05,  3.72it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:05<00:05,  3.22it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:06<00:05,  3.00it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:06<00:05,  2.90it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:06<00:05,  2.71it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:07<00:05,  2.69it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:07<00:04,  2.87it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:07<00:04,  3.03it/s]\u001b[A\n"," 68%|██████▊   | 25/37 [00:08<00:03,  3.23it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:08<00:03,  3.37it/s]\u001b[A\n"," 73%|███████▎  | 27/37 [00:08<00:03,  3.33it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:09<00:02,  3.21it/s]\u001b[A\n"," 78%|███████▊  | 29/37 [00:09<00:02,  2.87it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:09<00:02,  2.78it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:10<00:01,  3.01it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:10<00:01,  3.11it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:10<00:01,  3.26it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:11<00:00,  3.33it/s]\u001b[A\n"," 95%|█████████▍| 35/37 [00:11<00:00,  3.44it/s]\u001b[A\n"," 97%|█████████▋| 36/37 [00:11<00:00,  3.17it/s]\u001b[A\n","100%|██████████| 37/37 [00:11<00:00,  3.10it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:00<00:10,  3.27it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:10,  3.27it/s]\u001b[A\n","  8%|▊         | 3/37 [00:01<00:12,  2.64it/s]\u001b[A\n"," 11%|█         | 4/37 [00:01<00:12,  2.64it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:01<00:06,  4.65it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:01<00:04,  6.73it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:01<00:03,  8.66it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:01<00:02, 10.32it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:02<00:01, 11.68it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:02<00:01, 12.92it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:02<00:01, 13.82it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:02<00:01, 14.46it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:02<00:01, 14.77it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:02<00:00, 15.17it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:02<00:00, 15.34it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:02<00:00, 15.51it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:03<00:00, 15.61it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:03<00:00, 15.61it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:03<00:00, 15.56it/s]\u001b[A\n","100%|██████████| 37/37 [00:03<00:00, 10.45it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:02, 15.29it/s]\u001b[A\n"," 11%|█         | 4/37 [00:00<00:02, 15.78it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:00<00:02, 15.46it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:00<00:01, 15.16it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:00<00:01, 15.17it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:00<00:01, 14.98it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:00<00:01, 14.89it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:01<00:01, 15.34it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:01<00:01, 15.58it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:01<00:01, 15.27it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:01<00:00, 15.31it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:01<00:00, 14.89it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:01<00:00, 14.92it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:01<00:00, 14.60it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:02<00:00, 14.50it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:02<00:00, 14.63it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:02<00:00, 14.98it/s]\u001b[A\n","100%|██████████| 37/37 [00:02<00:00, 14.96it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  8%|▊         | 3/37 [00:00<00:01, 23.08it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:00<00:01, 22.08it/s]\u001b[A\n"," 24%|██▍       | 9/37 [00:00<00:01, 22.91it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:00<00:01, 22.56it/s]\u001b[A\n"," 41%|████      | 15/37 [00:00<00:00, 22.39it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:00<00:00, 22.48it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:00<00:00, 22.41it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:01<00:00, 22.30it/s]\u001b[A\n"," 73%|███████▎  | 27/37 [00:01<00:00, 21.83it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:01<00:00, 21.96it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:01<00:00, 21.63it/s]\u001b[A\n","100%|██████████| 37/37 [00:01<00:00, 22.07it/s]\n"," 50%|█████     | 5/10 [06:34<06:38, 79.77s/it]\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:02<01:24,  2.35s/it]\u001b[A\n","  5%|▌         | 2/37 [00:02<00:39,  1.13s/it]\u001b[A\n","  8%|▊         | 3/37 [00:02<00:25,  1.35it/s]\u001b[A\n"," 11%|█         | 4/37 [00:03<00:18,  1.80it/s]\u001b[A\n"," 14%|█▎        | 5/37 [00:03<00:14,  2.20it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:03<00:12,  2.44it/s]\u001b[A\n"," 19%|█▉        | 7/37 [00:04<00:11,  2.72it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:04<00:05,  4.56it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:04<00:06,  4.09it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:05<00:06,  3.62it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:05<00:07,  3.12it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:05<00:07,  2.98it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:06<00:05,  3.98it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:06<00:05,  3.88it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:06<00:05,  3.79it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:07<00:04,  3.91it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:07<00:04,  3.81it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:07<00:04,  3.46it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:07<00:04,  3.52it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:08<00:04,  3.19it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:08<00:02,  4.75it/s]\u001b[A\n"," 78%|███████▊  | 29/37 [00:09<00:01,  5.73it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:09<00:01,  4.72it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:09<00:01,  4.42it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:10<00:00,  5.12it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:10<00:00,  4.49it/s]\u001b[A\n"," 95%|█████████▍| 35/37 [00:10<00:00,  4.01it/s]\u001b[A\n"," 97%|█████████▋| 36/37 [00:11<00:00,  3.71it/s]\u001b[A\n","100%|██████████| 37/37 [00:11<00:00,  3.28it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:00<00:10,  3.46it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:09,  3.62it/s]\u001b[A\n","  8%|▊         | 3/37 [00:01<00:14,  2.41it/s]\u001b[A\n"," 11%|█         | 4/37 [00:01<00:13,  2.48it/s]\u001b[A\n"," 14%|█▎        | 5/37 [00:01<00:11,  2.76it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:02<00:10,  2.91it/s]\u001b[A\n"," 19%|█▉        | 7/37 [00:02<00:10,  2.95it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:02<00:09,  3.19it/s]\u001b[A\n"," 24%|██▍       | 9/37 [00:02<00:08,  3.36it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:03<00:08,  3.02it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:03<00:08,  3.07it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:03<00:07,  3.17it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:04<00:07,  3.42it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:04<00:06,  3.54it/s]\u001b[A\n"," 41%|████      | 15/37 [00:04<00:07,  3.08it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:05<00:06,  3.25it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:05<00:06,  3.08it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:05<00:06,  2.81it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:06<00:06,  3.00it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:06<00:05,  3.13it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:06<00:05,  3.18it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:07<00:07,  2.00it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:08<00:06,  2.15it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:08<00:05,  2.20it/s]\u001b[A\n"," 68%|██████▊   | 25/37 [00:08<00:05,  2.36it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:09<00:04,  2.67it/s]\u001b[A\n"," 73%|███████▎  | 27/37 [00:09<00:03,  2.88it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:09<00:03,  2.84it/s]\u001b[A\n"," 78%|███████▊  | 29/37 [00:10<00:02,  2.88it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:10<00:02,  3.06it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:10<00:02,  2.86it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:11<00:01,  2.78it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:11<00:01,  2.98it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:11<00:00,  3.18it/s]\u001b[A\n"," 95%|█████████▍| 35/37 [00:12<00:00,  3.31it/s]\u001b[A\n"," 97%|█████████▋| 36/37 [00:12<00:00,  3.43it/s]\u001b[A\n","100%|██████████| 37/37 [00:12<00:00,  2.93it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:00<00:11,  3.01it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:11,  3.09it/s]\u001b[A\n","  8%|▊         | 3/37 [00:01<00:14,  2.41it/s]\u001b[A\n"," 11%|█         | 4/37 [00:01<00:10,  3.15it/s]\u001b[A\n"," 14%|█▎        | 5/37 [00:01<00:08,  3.66it/s]\u001b[A\n"," 19%|█▉        | 7/37 [00:01<00:04,  6.01it/s]\u001b[A\n"," 24%|██▍       | 9/37 [00:01<00:03,  8.06it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:01<00:02,  9.80it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:02<00:02, 11.33it/s]\u001b[A\n"," 41%|████      | 15/37 [00:02<00:01, 12.57it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:02<00:01, 13.48it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:02<00:01, 14.05it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:02<00:01, 14.43it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:02<00:00, 14.89it/s]\u001b[A\n"," 68%|██████▊   | 25/37 [00:02<00:00, 15.04it/s]\u001b[A\n"," 73%|███████▎  | 27/37 [00:02<00:00, 15.27it/s]\u001b[A\n"," 78%|███████▊  | 29/37 [00:03<00:00, 15.46it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:03<00:00, 15.66it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:03<00:00, 15.64it/s]\u001b[A\n"," 95%|█████████▍| 35/37 [00:03<00:00, 15.70it/s]\u001b[A\n","100%|██████████| 37/37 [00:03<00:00, 10.36it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:02, 15.50it/s]\u001b[A\n"," 11%|█         | 4/37 [00:00<00:02, 14.79it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:00<00:02, 14.98it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:00<00:01, 14.69it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:00<00:01, 14.89it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:00<00:01, 15.00it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:00<00:01, 15.06it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:01<00:01, 15.22it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:01<00:01, 15.46it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:01<00:01, 15.43it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:01<00:00, 15.47it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:01<00:00, 15.37it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:01<00:00, 15.43it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:01<00:00, 15.50it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:01<00:00, 15.35it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:02<00:00, 15.01it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:02<00:00, 14.95it/s]\u001b[A\n","100%|██████████| 37/37 [00:02<00:00, 15.11it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  8%|▊         | 3/37 [00:00<00:01, 22.74it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:00<00:01, 22.73it/s]\u001b[A\n"," 24%|██▍       | 9/37 [00:00<00:01, 23.85it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:00<00:01, 23.27it/s]\u001b[A\n"," 41%|████      | 15/37 [00:00<00:00, 22.87it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:00<00:00, 22.77it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:00<00:00, 22.37it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:01<00:00, 22.32it/s]\u001b[A\n"," 73%|███████▎  | 27/37 [00:01<00:00, 22.37it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:01<00:00, 22.41it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:01<00:00, 22.38it/s]\u001b[A\n","100%|██████████| 37/37 [00:01<00:00, 22.44it/s]\n"," 60%|██████    | 6/10 [07:56<05:22, 80.50s/it]\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:02<01:16,  2.12s/it]\u001b[A\n","  5%|▌         | 2/37 [00:02<00:35,  1.03s/it]\u001b[A\n","  8%|▊         | 3/37 [00:02<00:24,  1.37it/s]\u001b[A\n"," 11%|█         | 4/37 [00:03<00:18,  1.80it/s]\u001b[A\n"," 14%|█▎        | 5/37 [00:03<00:14,  2.23it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:03<00:11,  2.64it/s]\u001b[A\n"," 19%|█▉        | 7/37 [00:03<00:10,  2.90it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:04<00:05,  5.12it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:04<00:05,  5.00it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:04<00:05,  4.70it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:04<00:05,  4.30it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:05<00:06,  3.76it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:05<00:04,  4.32it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:05<00:04,  4.28it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:06<00:04,  4.29it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:06<00:04,  4.19it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:06<00:04,  4.07it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:06<00:04,  3.51it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:07<00:04,  3.42it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:07<00:04,  3.11it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:07<00:02,  4.90it/s]\u001b[A\n"," 78%|███████▊  | 29/37 [00:08<00:01,  6.05it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:08<00:01,  5.60it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:08<00:01,  4.75it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:09<00:00,  4.93it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:09<00:00,  4.31it/s]\u001b[A\n"," 95%|█████████▍| 35/37 [00:09<00:00,  3.98it/s]\u001b[A\n"," 97%|█████████▋| 36/37 [00:10<00:00,  3.94it/s]\u001b[A\n","100%|██████████| 37/37 [00:10<00:00,  3.53it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:00<00:11,  3.06it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:10,  3.25it/s]\u001b[A\n","  8%|▊         | 3/37 [00:00<00:09,  3.41it/s]\u001b[A\n"," 11%|█         | 4/37 [00:01<00:12,  2.66it/s]\u001b[A\n"," 14%|█▎        | 5/37 [00:01<00:12,  2.61it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:02<00:11,  2.66it/s]\u001b[A\n"," 19%|█▉        | 7/37 [00:02<00:10,  2.92it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:02<00:09,  3.03it/s]\u001b[A\n"," 24%|██▍       | 9/37 [00:02<00:08,  3.23it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:03<00:07,  3.38it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:03<00:07,  3.39it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:03<00:07,  3.38it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:04<00:06,  3.45it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:04<00:07,  3.12it/s]\u001b[A\n"," 41%|████      | 15/37 [00:04<00:06,  3.31it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:05<00:06,  3.50it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:05<00:05,  3.56it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:05<00:05,  3.61it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:06<00:06,  2.86it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:06<00:05,  3.02it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:06<00:05,  3.17it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:06<00:04,  3.23it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:07<00:04,  3.44it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:07<00:03,  3.45it/s]\u001b[A\n"," 68%|██████▊   | 25/37 [00:07<00:03,  3.10it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:08<00:03,  3.19it/s]\u001b[A\n"," 73%|███████▎  | 27/37 [00:08<00:03,  3.17it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:08<00:02,  3.15it/s]\u001b[A\n"," 78%|███████▊  | 29/37 [00:09<00:02,  3.21it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:09<00:02,  3.32it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:09<00:01,  3.33it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:10<00:01,  3.20it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:10<00:01,  3.05it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:10<00:00,  3.07it/s]\u001b[A\n"," 95%|█████████▍| 35/37 [00:10<00:00,  3.21it/s]\u001b[A\n"," 97%|█████████▋| 36/37 [00:11<00:00,  3.41it/s]\u001b[A\n","100%|██████████| 37/37 [00:11<00:00,  3.19it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:00<00:13,  2.58it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:12,  2.78it/s]\u001b[A\n","  8%|▊         | 3/37 [00:01<00:14,  2.33it/s]\u001b[A\n"," 14%|█▎        | 5/37 [00:01<00:07,  4.38it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:01<00:06,  5.08it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:01<00:03,  7.26it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:01<00:02,  9.12it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:01<00:02, 10.73it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:02<00:01, 11.97it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:02<00:01, 13.03it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:02<00:01, 13.87it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:02<00:01, 14.33it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:02<00:01, 14.90it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:02<00:00, 15.05it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:02<00:00, 15.28it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:02<00:00, 15.26it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:03<00:00, 15.47it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:03<00:00, 15.65it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:03<00:00, 15.65it/s]\u001b[A\n","100%|██████████| 37/37 [00:03<00:00, 10.60it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:02, 14.23it/s]\u001b[A\n"," 11%|█         | 4/37 [00:00<00:02, 14.39it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:00<00:02, 14.94it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:00<00:01, 15.48it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:00<00:01, 15.30it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:00<00:01, 15.40it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:00<00:01, 15.25it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:01<00:01, 15.12it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:01<00:01, 14.95it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:01<00:01, 14.84it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:01<00:01, 14.85it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:01<00:00, 14.99it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:01<00:00, 15.15it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:01<00:00, 14.99it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:02<00:00, 14.60it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:02<00:00, 14.67it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:02<00:00, 14.55it/s]\u001b[A\n","100%|██████████| 37/37 [00:02<00:00, 14.83it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  8%|▊         | 3/37 [00:00<00:01, 22.77it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:00<00:01, 22.72it/s]\u001b[A\n"," 24%|██▍       | 9/37 [00:00<00:01, 23.51it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:00<00:01, 23.31it/s]\u001b[A\n"," 41%|████      | 15/37 [00:00<00:00, 23.10it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:00<00:00, 22.80it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:00<00:00, 22.87it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:01<00:00, 23.01it/s]\u001b[A\n"," 73%|███████▎  | 27/37 [00:01<00:00, 22.86it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:01<00:00, 21.92it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:01<00:00, 21.74it/s]\u001b[A\n","100%|██████████| 37/37 [00:01<00:00, 22.35it/s]\n"," 70%|███████   | 7/10 [09:15<04:00, 80.09s/it]\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:02<01:19,  2.20s/it]\u001b[A\n","  5%|▌         | 2/37 [00:02<00:39,  1.14s/it]\u001b[A\n","  8%|▊         | 3/37 [00:02<00:25,  1.35it/s]\u001b[A\n"," 11%|█         | 4/37 [00:03<00:18,  1.77it/s]\u001b[A\n"," 14%|█▎        | 5/37 [00:03<00:14,  2.14it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:03<00:12,  2.57it/s]\u001b[A\n"," 19%|█▉        | 7/37 [00:03<00:10,  2.88it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:04<00:05,  5.08it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:04<00:06,  4.15it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:04<00:06,  4.01it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:05<00:06,  3.90it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:05<00:06,  3.75it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:05<00:04,  4.55it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:06<00:04,  4.25it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:06<00:04,  4.00it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:06<00:05,  3.40it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:07<00:05,  3.18it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:07<00:04,  3.28it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:07<00:04,  3.12it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:08<00:04,  3.35it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:08<00:02,  4.80it/s]\u001b[A\n"," 78%|███████▊  | 29/37 [00:08<00:01,  5.77it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:09<00:01,  5.23it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:09<00:01,  5.00it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:09<00:00,  5.45it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:09<00:00,  4.89it/s]\u001b[A\n"," 95%|█████████▍| 35/37 [00:10<00:00,  4.04it/s]\u001b[A\n"," 97%|█████████▋| 36/37 [00:10<00:00,  3.87it/s]\u001b[A\n","100%|██████████| 37/37 [00:11<00:00,  3.36it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:00<00:10,  3.28it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:10,  3.41it/s]\u001b[A\n","  8%|▊         | 3/37 [00:00<00:09,  3.52it/s]\u001b[A\n"," 11%|█         | 4/37 [00:01<00:11,  2.94it/s]\u001b[A\n"," 14%|█▎        | 5/37 [00:01<00:11,  2.75it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:02<00:10,  2.91it/s]\u001b[A\n"," 19%|█▉        | 7/37 [00:02<00:09,  3.09it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:02<00:09,  3.19it/s]\u001b[A\n"," 24%|██▍       | 9/37 [00:02<00:08,  3.33it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:03<00:07,  3.54it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:03<00:07,  3.42it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:03<00:07,  3.42it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:04<00:07,  3.12it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:04<00:07,  3.26it/s]\u001b[A\n"," 41%|████      | 15/37 [00:04<00:07,  3.04it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:05<00:07,  2.96it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:05<00:06,  3.17it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:05<00:05,  3.31it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:05<00:05,  3.33it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:06<00:05,  3.14it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:06<00:04,  3.27it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:06<00:04,  3.05it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:07<00:04,  3.33it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:07<00:03,  3.35it/s]\u001b[A\n"," 68%|██████▊   | 25/37 [00:07<00:03,  3.03it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:08<00:03,  3.20it/s]\u001b[A\n"," 73%|███████▎  | 27/37 [00:08<00:03,  3.17it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:08<00:03,  2.86it/s]\u001b[A\n"," 78%|███████▊  | 29/37 [00:09<00:02,  3.02it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:09<00:02,  2.80it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:09<00:01,  3.09it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:10<00:01,  3.11it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:10<00:01,  3.16it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:10<00:00,  3.34it/s]\u001b[A\n"," 95%|█████████▍| 35/37 [00:11<00:00,  3.34it/s]\u001b[A\n"," 97%|█████████▋| 36/37 [00:11<00:00,  3.41it/s]\u001b[A\n","100%|██████████| 37/37 [00:11<00:00,  3.17it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:00<00:14,  2.44it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:14,  2.41it/s]\u001b[A\n","  8%|▊         | 3/37 [00:01<00:15,  2.21it/s]\u001b[A\n"," 11%|█         | 4/37 [00:01<00:12,  2.72it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:01<00:06,  4.70it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:01<00:04,  6.71it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:01<00:03,  8.64it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:02<00:02, 10.30it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:02<00:01, 11.78it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:02<00:01, 12.97it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:02<00:01, 13.96it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:02<00:01, 14.21it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:02<00:01, 14.77it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:02<00:00, 15.12it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:02<00:00, 15.43it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:03<00:00, 15.56it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:03<00:00, 15.80it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:03<00:00, 15.89it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:03<00:00, 16.08it/s]\u001b[A\n","100%|██████████| 37/37 [00:03<00:00, 10.15it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:02, 15.59it/s]\u001b[A\n"," 11%|█         | 4/37 [00:00<00:02, 15.03it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:00<00:02, 15.25it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:00<00:01, 15.39it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:00<00:01, 15.37it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:00<00:01, 15.63it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:00<00:01, 15.42it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:01<00:01, 15.28it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:01<00:01, 15.28it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:01<00:01, 15.39it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:01<00:00, 15.06it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:01<00:00, 15.07it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:01<00:00, 15.31it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:01<00:00, 15.39it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:01<00:00, 15.31it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:02<00:00, 15.17it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:02<00:00, 15.22it/s]\u001b[A\n","100%|██████████| 37/37 [00:02<00:00, 15.23it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  8%|▊         | 3/37 [00:00<00:01, 22.64it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:00<00:01, 22.72it/s]\u001b[A\n"," 24%|██▍       | 9/37 [00:00<00:01, 23.32it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:00<00:01, 22.70it/s]\u001b[A\n"," 41%|████      | 15/37 [00:00<00:00, 22.39it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:00<00:00, 22.46it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:00<00:00, 22.68it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:01<00:00, 22.58it/s]\u001b[A\n"," 73%|███████▎  | 27/37 [00:01<00:00, 22.32it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:01<00:00, 22.59it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:01<00:00, 22.51it/s]\u001b[A\n","100%|██████████| 37/37 [00:01<00:00, 22.43it/s]\n"," 80%|████████  | 8/10 [10:36<02:40, 80.26s/it]\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:02<01:20,  2.24s/it]\u001b[A\n","  5%|▌         | 2/37 [00:02<00:38,  1.09s/it]\u001b[A\n","  8%|▊         | 3/37 [00:02<00:24,  1.38it/s]\u001b[A\n"," 11%|█         | 4/37 [00:03<00:17,  1.84it/s]\u001b[A\n"," 14%|█▎        | 5/37 [00:03<00:14,  2.28it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:03<00:11,  2.69it/s]\u001b[A\n"," 19%|█▉        | 7/37 [00:03<00:11,  2.72it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:04<00:05,  4.87it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:04<00:05,  4.34it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:04<00:06,  4.16it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:05<00:05,  4.17it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:05<00:05,  3.98it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:05<00:04,  5.06it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:05<00:04,  4.85it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:06<00:04,  4.03it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:06<00:04,  4.01it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:06<00:04,  3.78it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:07<00:04,  3.46it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:07<00:04,  3.54it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:07<00:03,  3.68it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:07<00:01,  5.68it/s]\u001b[A\n"," 78%|███████▊  | 29/37 [00:08<00:01,  7.09it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:08<00:01,  6.36it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:08<00:01,  4.76it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:09<00:00,  4.92it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:09<00:00,  4.13it/s]\u001b[A\n"," 95%|█████████▍| 35/37 [00:09<00:00,  3.74it/s]\u001b[A\n"," 97%|█████████▋| 36/37 [00:10<00:00,  3.75it/s]\u001b[A\n","100%|██████████| 37/37 [00:10<00:00,  3.53it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:00<00:08,  4.09it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:08,  3.98it/s]\u001b[A\n","  8%|▊         | 3/37 [00:00<00:09,  3.74it/s]\u001b[A\n"," 11%|█         | 4/37 [00:01<00:08,  3.71it/s]\u001b[A\n"," 14%|█▎        | 5/37 [00:01<00:08,  3.61it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:01<00:09,  3.12it/s]\u001b[A\n"," 19%|█▉        | 7/37 [00:02<00:09,  3.28it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:02<00:09,  3.09it/s]\u001b[A\n"," 24%|██▍       | 9/37 [00:02<00:09,  2.88it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:03<00:08,  3.08it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:03<00:07,  3.27it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:03<00:07,  3.26it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:03<00:06,  3.44it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:04<00:07,  3.26it/s]\u001b[A\n"," 41%|████      | 15/37 [00:04<00:06,  3.25it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:04<00:06,  3.31it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:05<00:06,  3.30it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:05<00:06,  3.02it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:05<00:06,  2.86it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:06<00:06,  2.74it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:06<00:05,  2.89it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:06<00:04,  3.05it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:07<00:04,  3.31it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:07<00:03,  3.36it/s]\u001b[A\n"," 68%|██████▊   | 25/37 [00:07<00:03,  3.06it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:08<00:03,  3.15it/s]\u001b[A\n"," 73%|███████▎  | 27/37 [00:08<00:03,  3.24it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:08<00:03,  2.93it/s]\u001b[A\n"," 78%|███████▊  | 29/37 [00:09<00:02,  2.84it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:09<00:02,  2.96it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:09<00:01,  3.15it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:10<00:01,  3.20it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:10<00:01,  3.34it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:10<00:00,  3.35it/s]\u001b[A\n"," 95%|█████████▍| 35/37 [00:10<00:00,  3.49it/s]\u001b[A\n"," 97%|█████████▋| 36/37 [00:11<00:00,  3.05it/s]\u001b[A\n","100%|██████████| 37/37 [00:11<00:00,  3.17it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:00<00:11,  3.27it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:11,  3.08it/s]\u001b[A\n","  8%|▊         | 3/37 [00:01<00:15,  2.16it/s]\u001b[A\n"," 11%|█         | 4/37 [00:01<00:12,  2.67it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:01<00:06,  4.64it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:01<00:04,  6.58it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:01<00:03,  8.06it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:02<00:03,  8.16it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:02<00:02,  9.53it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:02<00:01, 10.86it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:02<00:01, 11.54it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:02<00:01, 12.37it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:02<00:01, 13.18it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:03<00:00, 13.65it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:03<00:00, 13.72it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:03<00:00, 13.56it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:03<00:00, 13.77it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:03<00:00, 13.97it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:03<00:00, 14.31it/s]\u001b[A\n","100%|██████████| 37/37 [00:03<00:00,  9.41it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:02, 15.29it/s]\u001b[A\n"," 11%|█         | 4/37 [00:00<00:02, 15.33it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:00<00:02, 14.60it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:00<00:01, 14.68it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:00<00:01, 14.77it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:00<00:01, 14.63it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:00<00:01, 14.70it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:01<00:01, 14.85it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:01<00:01, 15.05it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:01<00:01, 14.97it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:01<00:01, 14.96it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:01<00:00, 15.13it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:01<00:00, 14.89it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:01<00:00, 15.17it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:02<00:00, 14.98it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:02<00:00, 14.97it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:02<00:00, 15.00it/s]\u001b[A\n","100%|██████████| 37/37 [00:02<00:00, 14.78it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  8%|▊         | 3/37 [00:00<00:01, 21.78it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:00<00:01, 22.00it/s]\u001b[A\n"," 24%|██▍       | 9/37 [00:00<00:01, 23.07it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:00<00:01, 22.87it/s]\u001b[A\n"," 41%|████      | 15/37 [00:00<00:00, 22.91it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:00<00:00, 22.74it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:00<00:00, 22.61it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:01<00:00, 22.37it/s]\u001b[A\n"," 73%|███████▎  | 27/37 [00:01<00:00, 22.42it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:01<00:00, 22.25it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:01<00:00, 22.00it/s]\u001b[A\n","100%|██████████| 37/37 [00:01<00:00, 22.37it/s]\n"," 90%|█████████ | 9/10 [11:56<01:20, 80.05s/it]\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:02<01:28,  2.45s/it]\u001b[A\n","  5%|▌         | 2/37 [00:02<00:42,  1.20s/it]\u001b[A\n","  8%|▊         | 3/37 [00:03<00:26,  1.30it/s]\u001b[A\n"," 11%|█         | 4/37 [00:03<00:18,  1.75it/s]\u001b[A\n"," 14%|█▎        | 5/37 [00:03<00:15,  2.05it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:03<00:12,  2.48it/s]\u001b[A\n"," 19%|█▉        | 7/37 [00:04<00:11,  2.51it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:04<00:06,  4.45it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:04<00:06,  4.22it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:05<00:06,  3.98it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:05<00:06,  3.91it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:05<00:05,  3.93it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:06<00:04,  4.45it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:06<00:04,  4.20it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:06<00:04,  4.06it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:06<00:04,  3.68it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:07<00:05,  3.40it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:07<00:04,  3.56it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:07<00:04,  3.42it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:08<00:04,  3.10it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:08<00:02,  5.20it/s]\u001b[A\n"," 78%|███████▊  | 29/37 [00:08<00:01,  6.56it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:09<00:01,  5.83it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:09<00:01,  5.46it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:09<00:00,  5.95it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:09<00:00,  5.14it/s]\u001b[A\n"," 95%|█████████▍| 35/37 [00:10<00:00,  4.20it/s]\u001b[A\n"," 97%|█████████▋| 36/37 [00:10<00:00,  4.06it/s]\u001b[A\n","100%|██████████| 37/37 [00:10<00:00,  3.42it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:00<00:09,  3.98it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:09,  3.81it/s]\u001b[A\n","  8%|▊         | 3/37 [00:01<00:13,  2.57it/s]\u001b[A\n"," 11%|█         | 4/37 [00:01<00:11,  2.86it/s]\u001b[A\n"," 14%|█▎        | 5/37 [00:01<00:11,  2.77it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:01<00:10,  3.04it/s]\u001b[A\n"," 19%|█▉        | 7/37 [00:02<00:09,  3.33it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:02<00:08,  3.31it/s]\u001b[A\n"," 24%|██▍       | 9/37 [00:02<00:08,  3.25it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:03<00:07,  3.40it/s]\u001b[A\n"," 30%|██▉       | 11/37 [00:03<00:07,  3.48it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:03<00:07,  3.46it/s]\u001b[A\n"," 35%|███▌      | 13/37 [00:04<00:07,  3.06it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:04<00:07,  2.91it/s]\u001b[A\n"," 41%|████      | 15/37 [00:04<00:07,  3.11it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:05<00:06,  3.20it/s]\u001b[A\n"," 46%|████▌     | 17/37 [00:05<00:06,  3.24it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:05<00:05,  3.21it/s]\u001b[A\n"," 51%|█████▏    | 19/37 [00:05<00:05,  3.16it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:06<00:05,  3.31it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:06<00:04,  3.33it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:06<00:04,  3.22it/s]\u001b[A\n"," 62%|██████▏   | 23/37 [00:07<00:04,  3.24it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:07<00:04,  2.97it/s]\u001b[A\n"," 68%|██████▊   | 25/37 [00:07<00:04,  2.88it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:08<00:03,  3.07it/s]\u001b[A\n"," 73%|███████▎  | 27/37 [00:08<00:03,  3.16it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:08<00:02,  3.32it/s]\u001b[A\n"," 78%|███████▊  | 29/37 [00:09<00:02,  3.41it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:09<00:02,  3.42it/s]\u001b[A\n"," 84%|████████▍ | 31/37 [00:09<00:01,  3.44it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:10<00:01,  3.09it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:10<00:01,  3.15it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:10<00:00,  3.42it/s]\u001b[A\n"," 95%|█████████▍| 35/37 [00:10<00:00,  3.49it/s]\u001b[A\n"," 97%|█████████▋| 36/37 [00:11<00:00,  3.59it/s]\u001b[A\n","100%|██████████| 37/37 [00:11<00:00,  3.24it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 1/37 [00:00<00:11,  3.01it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:11,  3.08it/s]\u001b[A\n","  8%|▊         | 3/37 [00:01<00:14,  2.37it/s]\u001b[A\n"," 11%|█         | 4/37 [00:01<00:10,  3.01it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:01<00:06,  5.14it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:01<00:04,  7.19it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:01<00:02,  9.00it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:01<00:02, 10.48it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:02<00:02, 11.26it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:02<00:01, 12.43it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:02<00:01, 13.23it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:02<00:01, 13.94it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:02<00:01, 14.54it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:02<00:00, 14.90it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:02<00:00, 15.16it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:02<00:00, 14.70it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:03<00:00, 15.06it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:03<00:00, 15.43it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:03<00:00, 15.54it/s]\u001b[A\n","100%|██████████| 37/37 [00:03<00:00, 10.43it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  5%|▌         | 2/37 [00:00<00:02, 15.92it/s]\u001b[A\n"," 11%|█         | 4/37 [00:00<00:02, 15.04it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:00<00:02, 14.73it/s]\u001b[A\n"," 22%|██▏       | 8/37 [00:00<00:01, 14.75it/s]\u001b[A\n"," 27%|██▋       | 10/37 [00:00<00:01, 15.06it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:00<00:01, 15.15it/s]\u001b[A\n"," 38%|███▊      | 14/37 [00:00<00:01, 15.20it/s]\u001b[A\n"," 43%|████▎     | 16/37 [00:01<00:01, 15.10it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:01<00:01, 14.84it/s]\u001b[A\n"," 54%|█████▍    | 20/37 [00:01<00:01, 14.67it/s]\u001b[A\n"," 59%|█████▉    | 22/37 [00:01<00:01, 14.46it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:01<00:00, 14.69it/s]\u001b[A\n"," 70%|███████   | 26/37 [00:01<00:00, 14.57it/s]\u001b[A\n"," 76%|███████▌  | 28/37 [00:01<00:00, 14.67it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:02<00:00, 14.98it/s]\u001b[A\n"," 86%|████████▋ | 32/37 [00:02<00:00, 12.03it/s]\u001b[A\n"," 92%|█████████▏| 34/37 [00:02<00:00, 12.59it/s]\u001b[A\n","100%|██████████| 37/37 [00:02<00:00, 14.21it/s]\n","\n","  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n","  8%|▊         | 3/37 [00:00<00:01, 22.02it/s]\u001b[A\n"," 16%|█▌        | 6/37 [00:00<00:01, 22.15it/s]\u001b[A\n"," 24%|██▍       | 9/37 [00:00<00:01, 21.49it/s]\u001b[A\n"," 32%|███▏      | 12/37 [00:00<00:01, 21.86it/s]\u001b[A\n"," 41%|████      | 15/37 [00:00<00:01, 21.85it/s]\u001b[A\n"," 49%|████▊     | 18/37 [00:00<00:00, 21.35it/s]\u001b[A\n"," 57%|█████▋    | 21/37 [00:00<00:00, 21.68it/s]\u001b[A\n"," 65%|██████▍   | 24/37 [00:01<00:00, 21.62it/s]\u001b[A\n"," 73%|███████▎  | 27/37 [00:01<00:00, 21.88it/s]\u001b[A\n"," 81%|████████  | 30/37 [00:01<00:00, 21.97it/s]\u001b[A\n"," 89%|████████▉ | 33/37 [00:01<00:00, 22.18it/s]\u001b[A\n","100%|██████████| 37/37 [00:01<00:00, 21.82it/s]\n","100%|██████████| 10/10 [13:16<00:00, 79.62s/it]\n"]}]},{"cell_type":"code","source":["# 이부분 세부적으로 다 바꿔야한다.\n","\n","\n","\n","for k in tqdm(range(10)):\n","\n","  globals()[f'set_df_{k}'] = pd.DataFrame()\n","  answer_df_list = glob(f'/content/drive/MyDrive/농산물예측/aT_data/model_output{try_cnt}/set_{k}/*.csv') # 예측한 결과 불러오기\n","  pum_list = glob(f'/content/drive/MyDrive/농산물예측/aT_data/aT_test_raw/sep_{k}/*.csv') # 기존 test input 불러오기\n","  pummok = [a for a in pum_list if 'pummok' in a.split('/')[-1]]\n","\n","  for i in answer_df_list:\n","    df = pd.read_csv(i)\n","    number = i.split('_')[-1].split('.')[0]\n","\n","    base_number = 0\n","    for p in pummok:\n","      if number == p.split('_')[-1].split('.')[0]:\n","        pum_df = pd.read_csv(p)\n","\n","        if len(pum_df) != 0:\n","           base_number = pum_df.iloc[len(pum_df)-1]['해당일자_전체평균가격(원)']  # 기존 각 sep 마다 test input의 마지막 target 값 가져오기 (변동률 계산을 위해)\n","        else:\n","          base_number = np.nan\n","\n","    globals()[f'set_df_{k}'][f'품목{number}']  = [base_number] + list(df[df.columns[-1]].values) # 각 품목당 순서를 t, t+1 ... t+28 로 변경\n","\n","  globals()[f'set_df_{k}'] = globals()[f'set_df_{k}'][[f'품목{col}' for col in range(37)]] # 열 순서를 품목0 ~ 품목36 으로 변경"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cnhg3A5xMPWB","executionInfo":{"status":"ok","timestamp":1664517432150,"user_tz":-540,"elapsed":5360,"user":{"displayName":"이태영","userId":"15551978220746215296"}},"outputId":"9fc8c8c2-71a5-4254-9628-3edd1c6e70da"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:04<00:00,  2.07it/s]\n"]}]},{"cell_type":"code","source":["pd.options.display.max_columns=37\n","set_df_0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"SeT3MAiNMQiS","executionInfo":{"status":"ok","timestamp":1664517435484,"user_tz":-540,"elapsed":430,"user":{"displayName":"이태영","userId":"15551978220746215296"}},"outputId":"e8f4fd7b-156a-42b7-d889-7553b3ad592f"},"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["          품목0          품목1          품목2          품목3          품목4  \\\n","0   3871.1250  1362.117613  2909.783785  3400.075583  3947.809169   \n","1   4363.2773  1025.380400  2056.491700  2243.165500  2459.392800   \n","2   4376.0054  1009.790470  2052.715800  2245.457000  2461.756600   \n","3   4368.1410  1020.591000  2050.284000  2246.834500  2472.237500   \n","4   4372.6950  1021.180100  2049.494100  2249.164000  2470.855200   \n","5   4376.0130  1012.240500  2046.172000  2248.615000  2470.610400   \n","6   4381.2560  1028.929100  2044.332200  2249.211200  2470.554400   \n","7   4368.9087  1003.084400  2042.408600  2250.895000  2471.116700   \n","8   4377.3010  1012.228600  2041.653100  2249.627400  2472.195300   \n","9   4398.3940   997.064150  2039.052400  2248.931000  2466.202400   \n","10  4368.8726  1004.026500  2038.934000  2248.081800  2472.374800   \n","11  4380.0547   988.855100  2035.013100  2247.253700  2468.036000   \n","12  4382.9100  1006.914600  2032.090200  2246.327600  2467.551500   \n","13  4379.5630   995.902470  2029.945800  2244.875000  2457.370400   \n","14  4384.2040   987.461300  2026.925500  2243.222400  2461.161400   \n","15  4390.4556   992.620360  2021.834800  2241.337200  2453.185000   \n","16  4383.1420   984.529900  2020.538800  2239.993700  2451.404500   \n","17  4364.3574   979.400400  2015.836500  2237.835700  2453.408000   \n","18  4369.4130   978.614900  2013.178300  2235.501000  2446.358000   \n","19  4374.0005   994.931100  2009.649000  2233.663800  2443.432600   \n","20  4367.1094   983.952400  2008.710100  2231.739700  2442.687500   \n","21  4374.1943   978.628360  2004.651700  2229.878700  2436.769500   \n","22  4365.4550   974.446100  2002.294800  2228.068400  2436.470000   \n","23  4378.4350   974.763500  2000.168500  2225.862800  2424.546900   \n","24  4365.0180   971.709350  1996.681300  2223.435800  2421.472400   \n","25  4367.1800   965.750240  1994.764300  2223.172600  2428.065000   \n","26  4363.0080   971.078370  1991.352200  2217.588000  2412.565400   \n","27  4359.4185   972.753850  1990.506000  2218.793700  2410.770500   \n","28  4356.3770   957.827150  1985.336200  2215.656000  2408.304200   \n","\n","            품목5        품목6          품목7          품목8          품목9        품목10  \\\n","0   9253.947514   2717.280  3361.030923  4911.899864  1173.018633  1337.03482   \n","1   4494.140600  10272.226  2225.790000  4607.241700  1588.164600  1583.15100   \n","2   4498.826000  10100.294  2235.514600  4605.363300  1588.122800  1583.08130   \n","3   4502.787000   9822.758  2246.782700  4626.419400  1588.423300  1582.73690   \n","4   4504.714000   9736.144  2250.494000  4603.014600  1588.471100  1582.18800   \n","5   4510.641000   9848.188  2257.663300  4610.831500  1588.501600  1581.56570   \n","6   4516.108000   9818.868  2262.525000  4606.632300  1588.471700  1581.45240   \n","7   4518.214000   9844.656  2266.632300  4615.229500  1588.661300  1581.13730   \n","8   4531.210000   9712.606  2269.921000  4601.515000  1588.513700  1580.94060   \n","9   4536.026000   9619.022  2271.264200  4598.191400  1588.183100  1580.20590   \n","10  4537.136700   9886.677  2276.613000  4600.249500  1588.345800  1580.22340   \n","11  4542.489700   9701.177  2280.993700  4593.460000  1588.373000  1579.95520   \n","12  4536.432600   9536.283  2285.345000  4588.801000  1587.935500  1579.39040   \n","13  4539.892000  10149.810  2283.093300  4589.890600  1587.596600  1579.26200   \n","14  4540.805000  10902.602  2286.425300  4591.846000  1587.433200  1579.22550   \n","15  4546.477500  12286.571  2280.165300  4577.593300  1587.102400  1579.35030   \n","16  4554.914000  13940.685  2273.657200  4583.872000  1586.446800  1579.14170   \n","17  4549.037000  15279.708  2258.078900  4586.849600  1586.075900  1579.05700   \n","18  4551.966300  16629.604  2250.446000  4571.595700  1585.695000  1578.71040   \n","19  4549.728500  17554.820  2240.664600  4575.291500  1585.316700  1578.72110   \n","20  4544.664000  18198.096  2218.599000  4570.766000  1584.825800  1579.03930   \n","21  4545.845700  18056.248  2205.714800  4574.019500  1584.725300  1579.38620   \n","22  4544.135300  18359.178  2183.822500  4559.408700  1584.393300  1579.62050   \n","23  4539.164600  18423.426  2167.589800  4568.273000  1583.931800  1579.40440   \n","24  4548.329600  18077.936  2150.113000  4556.689000  1583.355500  1579.18920   \n","25  4540.393000  18444.967  2128.486600  4556.950000  1582.886100  1579.02190   \n","26  4541.528000  18541.283  2107.012000  4562.030000  1582.554900  1578.99550   \n","27  4533.883000  18546.280  2091.216300  4554.184600  1581.991500  1578.76670   \n","28  4530.926000  18657.197  2066.257600  4552.204000  1581.732700  1578.57060   \n","\n","           품목11         품목12         품목13         품목14         품목15  \\\n","0   2053.354945  3003.205509  3148.414256  6573.839289  1089.126835   \n","1   4468.022500  3066.208300  2676.414800  7901.071300  1576.327600   \n","2   4458.986000  3065.659000  2678.416500  7893.069000  1581.891800   \n","3   4468.094700  3069.384000  2691.541000  7890.295000  1576.137300   \n","4   4456.674300  3066.288600  2701.479700  7902.788600  1582.182000   \n","5   4469.873000  3063.538800  2701.593500  7887.092000  1579.827800   \n","6   4464.326000  3061.246000  2709.084200  7885.292000  1579.035500   \n","7   4472.728500  3058.918200  2695.567600  7885.981000  1581.645400   \n","8   4462.956500  3056.403800  2722.192000  7896.580000  1580.104400   \n","9   4474.931600  3056.892000  2708.599600  7895.097700  1584.818200   \n","10  4465.021000  3054.149200  2710.242200  7889.436000  1583.395500   \n","11  4462.599600  3055.510500  2714.124800  7891.769500  1583.433200   \n","12  4461.100000  3052.424000  2708.998300  7892.162600  1582.131100   \n","13  4461.099600  3052.356000  2705.001000  7883.253400  1580.404200   \n","14  4456.346000  3052.021500  2713.757800  7872.249000  1580.325200   \n","15  4445.655000  3049.399700  2711.200400  7891.968800  1580.209600   \n","16  4439.308000  3049.026900  2723.620400  7877.388700  1578.259900   \n","17  4436.403300  3049.403300  2721.608600  7875.462000  1580.903100   \n","18  4435.593000  3048.757300  2731.594000  7856.063000  1578.383200   \n","19  4416.905000  3046.466600  2733.872600  7856.762700  1575.498300   \n","20  4428.102000  3042.706500  2734.425800  7880.401400  1573.595800   \n","21  4408.749500  3041.739500  2745.630000  7876.510300  1576.938000   \n","22  4398.229500  3040.901600  2729.394000  7863.093000  1580.005500   \n","23  4402.414600  3037.582000  2752.883800  7850.789000  1576.200000   \n","24  4390.703600  3037.150600  2761.178700  7858.791000  1570.929200   \n","25  4394.451000  3035.157500  2750.497000  7848.744000  1566.814000   \n","26  4377.555000  3038.085700  2754.196300  7859.098000  1562.961800   \n","27  4365.991000  3030.712200  2748.283200  7855.883000  1563.806900   \n","28  4365.940400  3034.206800  2743.208700  7845.728000  1554.447100   \n","\n","           품목16         품목17          품목18         품목19         품목20  \\\n","0   3838.815254  2254.390206  12538.931552  2990.863631  2965.711352   \n","1   2571.321300  2295.513200   6948.220000  2065.235000  1676.735100   \n","2   2585.134800  2294.456300   6941.982400  2066.582300  1681.460100   \n","3   2591.003700  2298.130000   6914.483400  2064.534000  1682.278100   \n","4   2598.765900  2304.747600   6904.107400  2066.848600  1684.719100   \n","5   2607.422600  2311.710700   6863.033000  2067.322000  1687.013800   \n","6   2619.433800  2309.453400   6858.658000  2069.447800  1687.211900   \n","7   2625.879600  2317.133300   6862.085400  2073.890400  1685.804400   \n","8   2633.073500  2318.480700   6785.822300  2075.335000  1690.135100   \n","9   2638.261700  2322.687500   6802.514000  2077.085200  1683.195600   \n","10  2645.737300  2322.902300   6822.602000  2076.041300  1684.911900   \n","11  2649.026100  2326.734900   6763.979000  2076.904000  1686.420300   \n","12  2647.062300  2329.689200   6770.532700  2080.269500  1687.946800   \n","13  2649.627400  2328.481200   6772.323700  2083.805400  1689.830700   \n","14  2649.080300  2329.475300   6745.188500  2081.824700  1693.329600   \n","15  2645.383300  2330.801300   6766.069000  2083.722400  1689.683800   \n","16  2645.009300  2332.372300   6753.937500  2087.585400  1693.700100   \n","17  2640.232200  2333.006800   6742.897000  2087.070000  1692.805900   \n","18  2640.005000  2329.002700   6722.927000  2082.358200  1694.876000   \n","19  2635.808800  2333.338600   6735.512000  2090.561500  1687.990800   \n","20  2628.527800  2328.399700   6760.732000  2093.039600  1689.064000   \n","21  2622.955800  2329.577600   6729.579600  2090.123800  1695.447800   \n","22  2617.445800  2328.418200   6710.951700  2090.270000  1693.656400   \n","23  2607.787400  2327.962600   6709.235400  2094.175000  1695.050200   \n","24  2597.231700  2322.177700   6721.586000  2088.406200  1692.537200   \n","25  2590.212400  2320.971700   6689.449700  2090.284200  1693.084000   \n","26  2582.215000  2320.557400   6723.793000  2084.843500  1688.386400   \n","27  2571.571000  2315.920200   6682.018000  2089.973100  1686.343400   \n","28  2563.746300  2314.080600   6668.375000  2086.935300  1687.127300   \n","\n","          품목21        품목22         품목23        품목24         품목25         품목26  \\\n","0   505.929304  937.367871  4147.240906  1231.11322  3323.910031  3056.787271   \n","1   528.880200  395.028200  3547.502200  1106.39930  1690.261200  2132.794000   \n","2   524.518800  411.006500  3562.684000  1108.73290  1614.690900  2138.867200   \n","3   532.657040  424.770540  3583.898400  1108.45630  1666.836300  2138.366200   \n","4   530.004330  412.511750  3592.571500  1106.33800  1738.143400  2144.398200   \n","5   530.390400  421.690120  3603.020000  1107.79380  1642.286500  2150.013400   \n","6   530.351700  403.211400  3612.137000  1100.70470  1637.391000  2144.477500   \n","7   537.248540  404.707860  3621.046600  1100.12290  1602.075300  2149.125700   \n","8   533.439150  416.411250  3631.982700  1102.21770  1548.632200  2143.011700   \n","9   535.785030  423.788670  3631.387200  1096.41660  1505.579000  2142.000000   \n","10  539.290700  405.755460  3639.164000  1095.48130  1545.625600  2143.977500   \n","11  532.168300  407.087200  3636.760000  1093.36730  1646.414000  2145.337200   \n","12  528.727100  411.214700  3632.669200  1102.63660  1573.749500  2146.161000   \n","13  532.661200  418.606080  3640.773200  1095.66540  1573.814000  2141.338400   \n","14  544.671300  425.692000  3634.551000  1093.50710  1483.019000  2137.129000   \n","15  531.039000  401.402700  3633.659000  1092.07450  1605.224200  2137.118700   \n","16  540.901370  409.576570  3630.251700  1094.32780  1527.979700  2135.342800   \n","17  537.615400  400.409970  3628.201700  1094.25940  1543.048000  2127.672900   \n","18  526.455100  410.738950  3627.161400  1094.85030  1550.895800  2128.529000   \n","19  533.531740  414.579650  3618.275400  1092.76460  1504.585600  2121.595000   \n","20  524.830750  428.297820  3599.861000  1094.97460  1493.929200  2117.116000   \n","21  538.038300  404.850370  3584.308000  1095.33100  1518.782100  2115.013700   \n","22  521.109300  409.885380  3568.401400  1091.11770  1533.641200  2105.824500   \n","23  525.354740  412.214400  3545.829800  1086.32970  1547.212200  2106.362300   \n","24  531.757700  422.102720  3530.578400  1092.83010  1515.894900  2102.850000   \n","25  526.609560  411.397830  3504.919000  1092.00630  1526.229600  2102.057100   \n","26  528.610800  414.540340  3486.935800  1090.54960  1499.466400  2089.349000   \n","27  517.231140  416.548770  3461.993200  1088.51340  1596.854000  2082.740700   \n","28  526.982540  412.384900  3446.438500  1092.07290  1507.366800  2081.271000   \n","\n","           품목27        품목28         품목29         품목30         품목31  \\\n","0   8640.811309  602.005658  1105.412623  1566.274239  3633.464557   \n","1   4735.611000  484.014860  1086.239100   841.472900  3283.650100   \n","2   4748.017600  488.266050  1086.749900   843.383700  3296.601300   \n","3   4758.663600  483.864500  1083.538300   841.354860  3308.074700   \n","4   4768.712400  490.275180  1086.581900   840.575260  3329.013400   \n","5   4780.249500  483.289800  1082.476100   841.002600  3331.508300   \n","6   4792.556000  482.067000  1084.993200   839.123500  3341.713900   \n","7   4803.294000  482.849370  1087.250100   839.597530  3350.736300   \n","8   4811.292000  483.508500  1088.807400   841.145140  3355.743200   \n","9   4818.212400  484.794530  1087.827600   840.135130  3373.532700   \n","10  4824.002000  483.250120  1084.590500   838.653300  3371.140400   \n","11  4827.020000  479.221400  1089.145000   840.397340  3377.761500   \n","12  4827.698000  481.139770  1083.326900   838.543700  3387.726000   \n","13  4827.174000  479.019040  1082.660500   839.642940  3386.597200   \n","14  4828.976000  479.943000  1081.764300   838.420100  3387.851000   \n","15  4830.480000  481.230680  1081.424300   837.667000  3397.449500   \n","16  4830.380000  480.902250  1082.440600   835.858200  3383.752400   \n","17  4829.376000  477.954680  1081.980600   838.418700  3387.384500   \n","18  4827.591000  476.538330  1082.530800   838.903400  3387.187000   \n","19  4824.878000  476.238650  1081.822500   838.208400  3394.013000   \n","20  4821.475000  474.981320  1082.821000   835.279850  3394.956800   \n","21  4812.072300  472.108700  1077.779400   833.504100  3401.254200   \n","22  4804.027300  467.643400  1079.669800   835.132600  3397.605000   \n","23  4793.815000  473.544130  1077.293600   835.025630  3397.754000   \n","24  4781.082500  469.759460  1076.836500   833.440300  3394.606700   \n","25  4767.651400  469.596300  1078.094400   832.830900  3394.323000   \n","26  4754.171400  468.008200  1076.613900   833.228000  3391.077400   \n","27  4743.815000  463.583800  1074.436500   832.030700  3389.421400   \n","28  4733.362000  465.285740  1072.992000   831.146200  3386.440000   \n","\n","           품목32         품목33         품목34         품목35         품목36  \n","0   5454.710444  5619.188362  5230.620027  2905.100888  2087.675036  \n","1   3777.129200  7351.846000  3299.310800  2332.078100  1810.412500  \n","2   3766.500700  7362.293000  3319.573000  2335.734900  1814.273000  \n","3   3754.078600  7366.339000  3314.697800  2337.918200  1813.115100  \n","4   3772.426500  7360.354500  3321.473100  2341.173000  1815.968000  \n","5   3748.737500  7369.337000  3329.497300  2344.488800  1813.274000  \n","6   3719.226800  7368.860400  3331.819600  2346.578000  1818.231100  \n","7   3726.960200  7369.940000  3342.400100  2349.782500  1814.060000  \n","8   3757.692000  7371.444000  3344.586700  2352.539800  1819.230200  \n","9   3730.376500  7368.006300  3341.569600  2355.249000  1826.726200  \n","10  3742.110400  7362.249000  3338.942100  2358.695600  1824.573400  \n","11  3721.692600  7370.874500  3362.157000  2360.479200  1820.276500  \n","12  3721.042200  7361.217000  3349.780000  2361.173800  1817.420200  \n","13  3695.952400  7365.278000  3368.115000  2363.313700  1828.517000  \n","14  3691.836400  7360.975600  3350.376700  2364.925800  1817.611600  \n","15  3700.265900  7357.315400  3366.004400  2365.447000  1826.138400  \n","16  3700.359000  7360.675300  3355.895300  2365.886700  1818.039900  \n","17  3697.494600  7349.252000  3364.907500  2365.384800  1822.330300  \n","18  3698.481000  7347.745600  3349.836200  2363.548000  1822.351400  \n","19  3669.062500  7346.926000  3355.719000  2361.924300  1812.173500  \n","20  3672.515000  7346.706500  3351.007800  2358.887700  1815.817900  \n","21  3677.457500  7335.551300  3348.912400  2356.394300  1814.242300  \n","22  3667.864300  7341.502000  3346.084700  2353.300000  1814.849900  \n","23  3674.004000  7337.556000  3329.152000  2351.045200  1816.811200  \n","24  3667.116200  7335.786000  3326.426300  2348.532200  1815.694300  \n","25  3656.531200  7337.984400  3324.256600  2344.823000  1803.806600  \n","26  3648.243700  7328.824000  3310.292700  2341.223600  1818.593800  \n","27  3652.032700  7330.042000  3299.863500  2338.869600  1805.844000  \n","28  3637.037800  7333.403300  3304.216600  2336.137200  1801.111200  "],"text/html":["\n","  <div id=\"df-123c8d5e-528e-474a-bdfc-208be1772559\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>품목0</th>\n","      <th>품목1</th>\n","      <th>품목2</th>\n","      <th>품목3</th>\n","      <th>품목4</th>\n","      <th>품목5</th>\n","      <th>품목6</th>\n","      <th>품목7</th>\n","      <th>품목8</th>\n","      <th>품목9</th>\n","      <th>품목10</th>\n","      <th>품목11</th>\n","      <th>품목12</th>\n","      <th>품목13</th>\n","      <th>품목14</th>\n","      <th>품목15</th>\n","      <th>품목16</th>\n","      <th>품목17</th>\n","      <th>품목18</th>\n","      <th>품목19</th>\n","      <th>품목20</th>\n","      <th>품목21</th>\n","      <th>품목22</th>\n","      <th>품목23</th>\n","      <th>품목24</th>\n","      <th>품목25</th>\n","      <th>품목26</th>\n","      <th>품목27</th>\n","      <th>품목28</th>\n","      <th>품목29</th>\n","      <th>품목30</th>\n","      <th>품목31</th>\n","      <th>품목32</th>\n","      <th>품목33</th>\n","      <th>품목34</th>\n","      <th>품목35</th>\n","      <th>품목36</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3871.1250</td>\n","      <td>1362.117613</td>\n","      <td>2909.783785</td>\n","      <td>3400.075583</td>\n","      <td>3947.809169</td>\n","      <td>9253.947514</td>\n","      <td>2717.280</td>\n","      <td>3361.030923</td>\n","      <td>4911.899864</td>\n","      <td>1173.018633</td>\n","      <td>1337.03482</td>\n","      <td>2053.354945</td>\n","      <td>3003.205509</td>\n","      <td>3148.414256</td>\n","      <td>6573.839289</td>\n","      <td>1089.126835</td>\n","      <td>3838.815254</td>\n","      <td>2254.390206</td>\n","      <td>12538.931552</td>\n","      <td>2990.863631</td>\n","      <td>2965.711352</td>\n","      <td>505.929304</td>\n","      <td>937.367871</td>\n","      <td>4147.240906</td>\n","      <td>1231.11322</td>\n","      <td>3323.910031</td>\n","      <td>3056.787271</td>\n","      <td>8640.811309</td>\n","      <td>602.005658</td>\n","      <td>1105.412623</td>\n","      <td>1566.274239</td>\n","      <td>3633.464557</td>\n","      <td>5454.710444</td>\n","      <td>5619.188362</td>\n","      <td>5230.620027</td>\n","      <td>2905.100888</td>\n","      <td>2087.675036</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4363.2773</td>\n","      <td>1025.380400</td>\n","      <td>2056.491700</td>\n","      <td>2243.165500</td>\n","      <td>2459.392800</td>\n","      <td>4494.140600</td>\n","      <td>10272.226</td>\n","      <td>2225.790000</td>\n","      <td>4607.241700</td>\n","      <td>1588.164600</td>\n","      <td>1583.15100</td>\n","      <td>4468.022500</td>\n","      <td>3066.208300</td>\n","      <td>2676.414800</td>\n","      <td>7901.071300</td>\n","      <td>1576.327600</td>\n","      <td>2571.321300</td>\n","      <td>2295.513200</td>\n","      <td>6948.220000</td>\n","      <td>2065.235000</td>\n","      <td>1676.735100</td>\n","      <td>528.880200</td>\n","      <td>395.028200</td>\n","      <td>3547.502200</td>\n","      <td>1106.39930</td>\n","      <td>1690.261200</td>\n","      <td>2132.794000</td>\n","      <td>4735.611000</td>\n","      <td>484.014860</td>\n","      <td>1086.239100</td>\n","      <td>841.472900</td>\n","      <td>3283.650100</td>\n","      <td>3777.129200</td>\n","      <td>7351.846000</td>\n","      <td>3299.310800</td>\n","      <td>2332.078100</td>\n","      <td>1810.412500</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4376.0054</td>\n","      <td>1009.790470</td>\n","      <td>2052.715800</td>\n","      <td>2245.457000</td>\n","      <td>2461.756600</td>\n","      <td>4498.826000</td>\n","      <td>10100.294</td>\n","      <td>2235.514600</td>\n","      <td>4605.363300</td>\n","      <td>1588.122800</td>\n","      <td>1583.08130</td>\n","      <td>4458.986000</td>\n","      <td>3065.659000</td>\n","      <td>2678.416500</td>\n","      <td>7893.069000</td>\n","      <td>1581.891800</td>\n","      <td>2585.134800</td>\n","      <td>2294.456300</td>\n","      <td>6941.982400</td>\n","      <td>2066.582300</td>\n","      <td>1681.460100</td>\n","      <td>524.518800</td>\n","      <td>411.006500</td>\n","      <td>3562.684000</td>\n","      <td>1108.73290</td>\n","      <td>1614.690900</td>\n","      <td>2138.867200</td>\n","      <td>4748.017600</td>\n","      <td>488.266050</td>\n","      <td>1086.749900</td>\n","      <td>843.383700</td>\n","      <td>3296.601300</td>\n","      <td>3766.500700</td>\n","      <td>7362.293000</td>\n","      <td>3319.573000</td>\n","      <td>2335.734900</td>\n","      <td>1814.273000</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4368.1410</td>\n","      <td>1020.591000</td>\n","      <td>2050.284000</td>\n","      <td>2246.834500</td>\n","      <td>2472.237500</td>\n","      <td>4502.787000</td>\n","      <td>9822.758</td>\n","      <td>2246.782700</td>\n","      <td>4626.419400</td>\n","      <td>1588.423300</td>\n","      <td>1582.73690</td>\n","      <td>4468.094700</td>\n","      <td>3069.384000</td>\n","      <td>2691.541000</td>\n","      <td>7890.295000</td>\n","      <td>1576.137300</td>\n","      <td>2591.003700</td>\n","      <td>2298.130000</td>\n","      <td>6914.483400</td>\n","      <td>2064.534000</td>\n","      <td>1682.278100</td>\n","      <td>532.657040</td>\n","      <td>424.770540</td>\n","      <td>3583.898400</td>\n","      <td>1108.45630</td>\n","      <td>1666.836300</td>\n","      <td>2138.366200</td>\n","      <td>4758.663600</td>\n","      <td>483.864500</td>\n","      <td>1083.538300</td>\n","      <td>841.354860</td>\n","      <td>3308.074700</td>\n","      <td>3754.078600</td>\n","      <td>7366.339000</td>\n","      <td>3314.697800</td>\n","      <td>2337.918200</td>\n","      <td>1813.115100</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4372.6950</td>\n","      <td>1021.180100</td>\n","      <td>2049.494100</td>\n","      <td>2249.164000</td>\n","      <td>2470.855200</td>\n","      <td>4504.714000</td>\n","      <td>9736.144</td>\n","      <td>2250.494000</td>\n","      <td>4603.014600</td>\n","      <td>1588.471100</td>\n","      <td>1582.18800</td>\n","      <td>4456.674300</td>\n","      <td>3066.288600</td>\n","      <td>2701.479700</td>\n","      <td>7902.788600</td>\n","      <td>1582.182000</td>\n","      <td>2598.765900</td>\n","      <td>2304.747600</td>\n","      <td>6904.107400</td>\n","      <td>2066.848600</td>\n","      <td>1684.719100</td>\n","      <td>530.004330</td>\n","      <td>412.511750</td>\n","      <td>3592.571500</td>\n","      <td>1106.33800</td>\n","      <td>1738.143400</td>\n","      <td>2144.398200</td>\n","      <td>4768.712400</td>\n","      <td>490.275180</td>\n","      <td>1086.581900</td>\n","      <td>840.575260</td>\n","      <td>3329.013400</td>\n","      <td>3772.426500</td>\n","      <td>7360.354500</td>\n","      <td>3321.473100</td>\n","      <td>2341.173000</td>\n","      <td>1815.968000</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>4376.0130</td>\n","      <td>1012.240500</td>\n","      <td>2046.172000</td>\n","      <td>2248.615000</td>\n","      <td>2470.610400</td>\n","      <td>4510.641000</td>\n","      <td>9848.188</td>\n","      <td>2257.663300</td>\n","      <td>4610.831500</td>\n","      <td>1588.501600</td>\n","      <td>1581.56570</td>\n","      <td>4469.873000</td>\n","      <td>3063.538800</td>\n","      <td>2701.593500</td>\n","      <td>7887.092000</td>\n","      <td>1579.827800</td>\n","      <td>2607.422600</td>\n","      <td>2311.710700</td>\n","      <td>6863.033000</td>\n","      <td>2067.322000</td>\n","      <td>1687.013800</td>\n","      <td>530.390400</td>\n","      <td>421.690120</td>\n","      <td>3603.020000</td>\n","      <td>1107.79380</td>\n","      <td>1642.286500</td>\n","      <td>2150.013400</td>\n","      <td>4780.249500</td>\n","      <td>483.289800</td>\n","      <td>1082.476100</td>\n","      <td>841.002600</td>\n","      <td>3331.508300</td>\n","      <td>3748.737500</td>\n","      <td>7369.337000</td>\n","      <td>3329.497300</td>\n","      <td>2344.488800</td>\n","      <td>1813.274000</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>4381.2560</td>\n","      <td>1028.929100</td>\n","      <td>2044.332200</td>\n","      <td>2249.211200</td>\n","      <td>2470.554400</td>\n","      <td>4516.108000</td>\n","      <td>9818.868</td>\n","      <td>2262.525000</td>\n","      <td>4606.632300</td>\n","      <td>1588.471700</td>\n","      <td>1581.45240</td>\n","      <td>4464.326000</td>\n","      <td>3061.246000</td>\n","      <td>2709.084200</td>\n","      <td>7885.292000</td>\n","      <td>1579.035500</td>\n","      <td>2619.433800</td>\n","      <td>2309.453400</td>\n","      <td>6858.658000</td>\n","      <td>2069.447800</td>\n","      <td>1687.211900</td>\n","      <td>530.351700</td>\n","      <td>403.211400</td>\n","      <td>3612.137000</td>\n","      <td>1100.70470</td>\n","      <td>1637.391000</td>\n","      <td>2144.477500</td>\n","      <td>4792.556000</td>\n","      <td>482.067000</td>\n","      <td>1084.993200</td>\n","      <td>839.123500</td>\n","      <td>3341.713900</td>\n","      <td>3719.226800</td>\n","      <td>7368.860400</td>\n","      <td>3331.819600</td>\n","      <td>2346.578000</td>\n","      <td>1818.231100</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>4368.9087</td>\n","      <td>1003.084400</td>\n","      <td>2042.408600</td>\n","      <td>2250.895000</td>\n","      <td>2471.116700</td>\n","      <td>4518.214000</td>\n","      <td>9844.656</td>\n","      <td>2266.632300</td>\n","      <td>4615.229500</td>\n","      <td>1588.661300</td>\n","      <td>1581.13730</td>\n","      <td>4472.728500</td>\n","      <td>3058.918200</td>\n","      <td>2695.567600</td>\n","      <td>7885.981000</td>\n","      <td>1581.645400</td>\n","      <td>2625.879600</td>\n","      <td>2317.133300</td>\n","      <td>6862.085400</td>\n","      <td>2073.890400</td>\n","      <td>1685.804400</td>\n","      <td>537.248540</td>\n","      <td>404.707860</td>\n","      <td>3621.046600</td>\n","      <td>1100.12290</td>\n","      <td>1602.075300</td>\n","      <td>2149.125700</td>\n","      <td>4803.294000</td>\n","      <td>482.849370</td>\n","      <td>1087.250100</td>\n","      <td>839.597530</td>\n","      <td>3350.736300</td>\n","      <td>3726.960200</td>\n","      <td>7369.940000</td>\n","      <td>3342.400100</td>\n","      <td>2349.782500</td>\n","      <td>1814.060000</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>4377.3010</td>\n","      <td>1012.228600</td>\n","      <td>2041.653100</td>\n","      <td>2249.627400</td>\n","      <td>2472.195300</td>\n","      <td>4531.210000</td>\n","      <td>9712.606</td>\n","      <td>2269.921000</td>\n","      <td>4601.515000</td>\n","      <td>1588.513700</td>\n","      <td>1580.94060</td>\n","      <td>4462.956500</td>\n","      <td>3056.403800</td>\n","      <td>2722.192000</td>\n","      <td>7896.580000</td>\n","      <td>1580.104400</td>\n","      <td>2633.073500</td>\n","      <td>2318.480700</td>\n","      <td>6785.822300</td>\n","      <td>2075.335000</td>\n","      <td>1690.135100</td>\n","      <td>533.439150</td>\n","      <td>416.411250</td>\n","      <td>3631.982700</td>\n","      <td>1102.21770</td>\n","      <td>1548.632200</td>\n","      <td>2143.011700</td>\n","      <td>4811.292000</td>\n","      <td>483.508500</td>\n","      <td>1088.807400</td>\n","      <td>841.145140</td>\n","      <td>3355.743200</td>\n","      <td>3757.692000</td>\n","      <td>7371.444000</td>\n","      <td>3344.586700</td>\n","      <td>2352.539800</td>\n","      <td>1819.230200</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>4398.3940</td>\n","      <td>997.064150</td>\n","      <td>2039.052400</td>\n","      <td>2248.931000</td>\n","      <td>2466.202400</td>\n","      <td>4536.026000</td>\n","      <td>9619.022</td>\n","      <td>2271.264200</td>\n","      <td>4598.191400</td>\n","      <td>1588.183100</td>\n","      <td>1580.20590</td>\n","      <td>4474.931600</td>\n","      <td>3056.892000</td>\n","      <td>2708.599600</td>\n","      <td>7895.097700</td>\n","      <td>1584.818200</td>\n","      <td>2638.261700</td>\n","      <td>2322.687500</td>\n","      <td>6802.514000</td>\n","      <td>2077.085200</td>\n","      <td>1683.195600</td>\n","      <td>535.785030</td>\n","      <td>423.788670</td>\n","      <td>3631.387200</td>\n","      <td>1096.41660</td>\n","      <td>1505.579000</td>\n","      <td>2142.000000</td>\n","      <td>4818.212400</td>\n","      <td>484.794530</td>\n","      <td>1087.827600</td>\n","      <td>840.135130</td>\n","      <td>3373.532700</td>\n","      <td>3730.376500</td>\n","      <td>7368.006300</td>\n","      <td>3341.569600</td>\n","      <td>2355.249000</td>\n","      <td>1826.726200</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>4368.8726</td>\n","      <td>1004.026500</td>\n","      <td>2038.934000</td>\n","      <td>2248.081800</td>\n","      <td>2472.374800</td>\n","      <td>4537.136700</td>\n","      <td>9886.677</td>\n","      <td>2276.613000</td>\n","      <td>4600.249500</td>\n","      <td>1588.345800</td>\n","      <td>1580.22340</td>\n","      <td>4465.021000</td>\n","      <td>3054.149200</td>\n","      <td>2710.242200</td>\n","      <td>7889.436000</td>\n","      <td>1583.395500</td>\n","      <td>2645.737300</td>\n","      <td>2322.902300</td>\n","      <td>6822.602000</td>\n","      <td>2076.041300</td>\n","      <td>1684.911900</td>\n","      <td>539.290700</td>\n","      <td>405.755460</td>\n","      <td>3639.164000</td>\n","      <td>1095.48130</td>\n","      <td>1545.625600</td>\n","      <td>2143.977500</td>\n","      <td>4824.002000</td>\n","      <td>483.250120</td>\n","      <td>1084.590500</td>\n","      <td>838.653300</td>\n","      <td>3371.140400</td>\n","      <td>3742.110400</td>\n","      <td>7362.249000</td>\n","      <td>3338.942100</td>\n","      <td>2358.695600</td>\n","      <td>1824.573400</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>4380.0547</td>\n","      <td>988.855100</td>\n","      <td>2035.013100</td>\n","      <td>2247.253700</td>\n","      <td>2468.036000</td>\n","      <td>4542.489700</td>\n","      <td>9701.177</td>\n","      <td>2280.993700</td>\n","      <td>4593.460000</td>\n","      <td>1588.373000</td>\n","      <td>1579.95520</td>\n","      <td>4462.599600</td>\n","      <td>3055.510500</td>\n","      <td>2714.124800</td>\n","      <td>7891.769500</td>\n","      <td>1583.433200</td>\n","      <td>2649.026100</td>\n","      <td>2326.734900</td>\n","      <td>6763.979000</td>\n","      <td>2076.904000</td>\n","      <td>1686.420300</td>\n","      <td>532.168300</td>\n","      <td>407.087200</td>\n","      <td>3636.760000</td>\n","      <td>1093.36730</td>\n","      <td>1646.414000</td>\n","      <td>2145.337200</td>\n","      <td>4827.020000</td>\n","      <td>479.221400</td>\n","      <td>1089.145000</td>\n","      <td>840.397340</td>\n","      <td>3377.761500</td>\n","      <td>3721.692600</td>\n","      <td>7370.874500</td>\n","      <td>3362.157000</td>\n","      <td>2360.479200</td>\n","      <td>1820.276500</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>4382.9100</td>\n","      <td>1006.914600</td>\n","      <td>2032.090200</td>\n","      <td>2246.327600</td>\n","      <td>2467.551500</td>\n","      <td>4536.432600</td>\n","      <td>9536.283</td>\n","      <td>2285.345000</td>\n","      <td>4588.801000</td>\n","      <td>1587.935500</td>\n","      <td>1579.39040</td>\n","      <td>4461.100000</td>\n","      <td>3052.424000</td>\n","      <td>2708.998300</td>\n","      <td>7892.162600</td>\n","      <td>1582.131100</td>\n","      <td>2647.062300</td>\n","      <td>2329.689200</td>\n","      <td>6770.532700</td>\n","      <td>2080.269500</td>\n","      <td>1687.946800</td>\n","      <td>528.727100</td>\n","      <td>411.214700</td>\n","      <td>3632.669200</td>\n","      <td>1102.63660</td>\n","      <td>1573.749500</td>\n","      <td>2146.161000</td>\n","      <td>4827.698000</td>\n","      <td>481.139770</td>\n","      <td>1083.326900</td>\n","      <td>838.543700</td>\n","      <td>3387.726000</td>\n","      <td>3721.042200</td>\n","      <td>7361.217000</td>\n","      <td>3349.780000</td>\n","      <td>2361.173800</td>\n","      <td>1817.420200</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>4379.5630</td>\n","      <td>995.902470</td>\n","      <td>2029.945800</td>\n","      <td>2244.875000</td>\n","      <td>2457.370400</td>\n","      <td>4539.892000</td>\n","      <td>10149.810</td>\n","      <td>2283.093300</td>\n","      <td>4589.890600</td>\n","      <td>1587.596600</td>\n","      <td>1579.26200</td>\n","      <td>4461.099600</td>\n","      <td>3052.356000</td>\n","      <td>2705.001000</td>\n","      <td>7883.253400</td>\n","      <td>1580.404200</td>\n","      <td>2649.627400</td>\n","      <td>2328.481200</td>\n","      <td>6772.323700</td>\n","      <td>2083.805400</td>\n","      <td>1689.830700</td>\n","      <td>532.661200</td>\n","      <td>418.606080</td>\n","      <td>3640.773200</td>\n","      <td>1095.66540</td>\n","      <td>1573.814000</td>\n","      <td>2141.338400</td>\n","      <td>4827.174000</td>\n","      <td>479.019040</td>\n","      <td>1082.660500</td>\n","      <td>839.642940</td>\n","      <td>3386.597200</td>\n","      <td>3695.952400</td>\n","      <td>7365.278000</td>\n","      <td>3368.115000</td>\n","      <td>2363.313700</td>\n","      <td>1828.517000</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>4384.2040</td>\n","      <td>987.461300</td>\n","      <td>2026.925500</td>\n","      <td>2243.222400</td>\n","      <td>2461.161400</td>\n","      <td>4540.805000</td>\n","      <td>10902.602</td>\n","      <td>2286.425300</td>\n","      <td>4591.846000</td>\n","      <td>1587.433200</td>\n","      <td>1579.22550</td>\n","      <td>4456.346000</td>\n","      <td>3052.021500</td>\n","      <td>2713.757800</td>\n","      <td>7872.249000</td>\n","      <td>1580.325200</td>\n","      <td>2649.080300</td>\n","      <td>2329.475300</td>\n","      <td>6745.188500</td>\n","      <td>2081.824700</td>\n","      <td>1693.329600</td>\n","      <td>544.671300</td>\n","      <td>425.692000</td>\n","      <td>3634.551000</td>\n","      <td>1093.50710</td>\n","      <td>1483.019000</td>\n","      <td>2137.129000</td>\n","      <td>4828.976000</td>\n","      <td>479.943000</td>\n","      <td>1081.764300</td>\n","      <td>838.420100</td>\n","      <td>3387.851000</td>\n","      <td>3691.836400</td>\n","      <td>7360.975600</td>\n","      <td>3350.376700</td>\n","      <td>2364.925800</td>\n","      <td>1817.611600</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>4390.4556</td>\n","      <td>992.620360</td>\n","      <td>2021.834800</td>\n","      <td>2241.337200</td>\n","      <td>2453.185000</td>\n","      <td>4546.477500</td>\n","      <td>12286.571</td>\n","      <td>2280.165300</td>\n","      <td>4577.593300</td>\n","      <td>1587.102400</td>\n","      <td>1579.35030</td>\n","      <td>4445.655000</td>\n","      <td>3049.399700</td>\n","      <td>2711.200400</td>\n","      <td>7891.968800</td>\n","      <td>1580.209600</td>\n","      <td>2645.383300</td>\n","      <td>2330.801300</td>\n","      <td>6766.069000</td>\n","      <td>2083.722400</td>\n","      <td>1689.683800</td>\n","      <td>531.039000</td>\n","      <td>401.402700</td>\n","      <td>3633.659000</td>\n","      <td>1092.07450</td>\n","      <td>1605.224200</td>\n","      <td>2137.118700</td>\n","      <td>4830.480000</td>\n","      <td>481.230680</td>\n","      <td>1081.424300</td>\n","      <td>837.667000</td>\n","      <td>3397.449500</td>\n","      <td>3700.265900</td>\n","      <td>7357.315400</td>\n","      <td>3366.004400</td>\n","      <td>2365.447000</td>\n","      <td>1826.138400</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>4383.1420</td>\n","      <td>984.529900</td>\n","      <td>2020.538800</td>\n","      <td>2239.993700</td>\n","      <td>2451.404500</td>\n","      <td>4554.914000</td>\n","      <td>13940.685</td>\n","      <td>2273.657200</td>\n","      <td>4583.872000</td>\n","      <td>1586.446800</td>\n","      <td>1579.14170</td>\n","      <td>4439.308000</td>\n","      <td>3049.026900</td>\n","      <td>2723.620400</td>\n","      <td>7877.388700</td>\n","      <td>1578.259900</td>\n","      <td>2645.009300</td>\n","      <td>2332.372300</td>\n","      <td>6753.937500</td>\n","      <td>2087.585400</td>\n","      <td>1693.700100</td>\n","      <td>540.901370</td>\n","      <td>409.576570</td>\n","      <td>3630.251700</td>\n","      <td>1094.32780</td>\n","      <td>1527.979700</td>\n","      <td>2135.342800</td>\n","      <td>4830.380000</td>\n","      <td>480.902250</td>\n","      <td>1082.440600</td>\n","      <td>835.858200</td>\n","      <td>3383.752400</td>\n","      <td>3700.359000</td>\n","      <td>7360.675300</td>\n","      <td>3355.895300</td>\n","      <td>2365.886700</td>\n","      <td>1818.039900</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>4364.3574</td>\n","      <td>979.400400</td>\n","      <td>2015.836500</td>\n","      <td>2237.835700</td>\n","      <td>2453.408000</td>\n","      <td>4549.037000</td>\n","      <td>15279.708</td>\n","      <td>2258.078900</td>\n","      <td>4586.849600</td>\n","      <td>1586.075900</td>\n","      <td>1579.05700</td>\n","      <td>4436.403300</td>\n","      <td>3049.403300</td>\n","      <td>2721.608600</td>\n","      <td>7875.462000</td>\n","      <td>1580.903100</td>\n","      <td>2640.232200</td>\n","      <td>2333.006800</td>\n","      <td>6742.897000</td>\n","      <td>2087.070000</td>\n","      <td>1692.805900</td>\n","      <td>537.615400</td>\n","      <td>400.409970</td>\n","      <td>3628.201700</td>\n","      <td>1094.25940</td>\n","      <td>1543.048000</td>\n","      <td>2127.672900</td>\n","      <td>4829.376000</td>\n","      <td>477.954680</td>\n","      <td>1081.980600</td>\n","      <td>838.418700</td>\n","      <td>3387.384500</td>\n","      <td>3697.494600</td>\n","      <td>7349.252000</td>\n","      <td>3364.907500</td>\n","      <td>2365.384800</td>\n","      <td>1822.330300</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>4369.4130</td>\n","      <td>978.614900</td>\n","      <td>2013.178300</td>\n","      <td>2235.501000</td>\n","      <td>2446.358000</td>\n","      <td>4551.966300</td>\n","      <td>16629.604</td>\n","      <td>2250.446000</td>\n","      <td>4571.595700</td>\n","      <td>1585.695000</td>\n","      <td>1578.71040</td>\n","      <td>4435.593000</td>\n","      <td>3048.757300</td>\n","      <td>2731.594000</td>\n","      <td>7856.063000</td>\n","      <td>1578.383200</td>\n","      <td>2640.005000</td>\n","      <td>2329.002700</td>\n","      <td>6722.927000</td>\n","      <td>2082.358200</td>\n","      <td>1694.876000</td>\n","      <td>526.455100</td>\n","      <td>410.738950</td>\n","      <td>3627.161400</td>\n","      <td>1094.85030</td>\n","      <td>1550.895800</td>\n","      <td>2128.529000</td>\n","      <td>4827.591000</td>\n","      <td>476.538330</td>\n","      <td>1082.530800</td>\n","      <td>838.903400</td>\n","      <td>3387.187000</td>\n","      <td>3698.481000</td>\n","      <td>7347.745600</td>\n","      <td>3349.836200</td>\n","      <td>2363.548000</td>\n","      <td>1822.351400</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>4374.0005</td>\n","      <td>994.931100</td>\n","      <td>2009.649000</td>\n","      <td>2233.663800</td>\n","      <td>2443.432600</td>\n","      <td>4549.728500</td>\n","      <td>17554.820</td>\n","      <td>2240.664600</td>\n","      <td>4575.291500</td>\n","      <td>1585.316700</td>\n","      <td>1578.72110</td>\n","      <td>4416.905000</td>\n","      <td>3046.466600</td>\n","      <td>2733.872600</td>\n","      <td>7856.762700</td>\n","      <td>1575.498300</td>\n","      <td>2635.808800</td>\n","      <td>2333.338600</td>\n","      <td>6735.512000</td>\n","      <td>2090.561500</td>\n","      <td>1687.990800</td>\n","      <td>533.531740</td>\n","      <td>414.579650</td>\n","      <td>3618.275400</td>\n","      <td>1092.76460</td>\n","      <td>1504.585600</td>\n","      <td>2121.595000</td>\n","      <td>4824.878000</td>\n","      <td>476.238650</td>\n","      <td>1081.822500</td>\n","      <td>838.208400</td>\n","      <td>3394.013000</td>\n","      <td>3669.062500</td>\n","      <td>7346.926000</td>\n","      <td>3355.719000</td>\n","      <td>2361.924300</td>\n","      <td>1812.173500</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>4367.1094</td>\n","      <td>983.952400</td>\n","      <td>2008.710100</td>\n","      <td>2231.739700</td>\n","      <td>2442.687500</td>\n","      <td>4544.664000</td>\n","      <td>18198.096</td>\n","      <td>2218.599000</td>\n","      <td>4570.766000</td>\n","      <td>1584.825800</td>\n","      <td>1579.03930</td>\n","      <td>4428.102000</td>\n","      <td>3042.706500</td>\n","      <td>2734.425800</td>\n","      <td>7880.401400</td>\n","      <td>1573.595800</td>\n","      <td>2628.527800</td>\n","      <td>2328.399700</td>\n","      <td>6760.732000</td>\n","      <td>2093.039600</td>\n","      <td>1689.064000</td>\n","      <td>524.830750</td>\n","      <td>428.297820</td>\n","      <td>3599.861000</td>\n","      <td>1094.97460</td>\n","      <td>1493.929200</td>\n","      <td>2117.116000</td>\n","      <td>4821.475000</td>\n","      <td>474.981320</td>\n","      <td>1082.821000</td>\n","      <td>835.279850</td>\n","      <td>3394.956800</td>\n","      <td>3672.515000</td>\n","      <td>7346.706500</td>\n","      <td>3351.007800</td>\n","      <td>2358.887700</td>\n","      <td>1815.817900</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>4374.1943</td>\n","      <td>978.628360</td>\n","      <td>2004.651700</td>\n","      <td>2229.878700</td>\n","      <td>2436.769500</td>\n","      <td>4545.845700</td>\n","      <td>18056.248</td>\n","      <td>2205.714800</td>\n","      <td>4574.019500</td>\n","      <td>1584.725300</td>\n","      <td>1579.38620</td>\n","      <td>4408.749500</td>\n","      <td>3041.739500</td>\n","      <td>2745.630000</td>\n","      <td>7876.510300</td>\n","      <td>1576.938000</td>\n","      <td>2622.955800</td>\n","      <td>2329.577600</td>\n","      <td>6729.579600</td>\n","      <td>2090.123800</td>\n","      <td>1695.447800</td>\n","      <td>538.038300</td>\n","      <td>404.850370</td>\n","      <td>3584.308000</td>\n","      <td>1095.33100</td>\n","      <td>1518.782100</td>\n","      <td>2115.013700</td>\n","      <td>4812.072300</td>\n","      <td>472.108700</td>\n","      <td>1077.779400</td>\n","      <td>833.504100</td>\n","      <td>3401.254200</td>\n","      <td>3677.457500</td>\n","      <td>7335.551300</td>\n","      <td>3348.912400</td>\n","      <td>2356.394300</td>\n","      <td>1814.242300</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>4365.4550</td>\n","      <td>974.446100</td>\n","      <td>2002.294800</td>\n","      <td>2228.068400</td>\n","      <td>2436.470000</td>\n","      <td>4544.135300</td>\n","      <td>18359.178</td>\n","      <td>2183.822500</td>\n","      <td>4559.408700</td>\n","      <td>1584.393300</td>\n","      <td>1579.62050</td>\n","      <td>4398.229500</td>\n","      <td>3040.901600</td>\n","      <td>2729.394000</td>\n","      <td>7863.093000</td>\n","      <td>1580.005500</td>\n","      <td>2617.445800</td>\n","      <td>2328.418200</td>\n","      <td>6710.951700</td>\n","      <td>2090.270000</td>\n","      <td>1693.656400</td>\n","      <td>521.109300</td>\n","      <td>409.885380</td>\n","      <td>3568.401400</td>\n","      <td>1091.11770</td>\n","      <td>1533.641200</td>\n","      <td>2105.824500</td>\n","      <td>4804.027300</td>\n","      <td>467.643400</td>\n","      <td>1079.669800</td>\n","      <td>835.132600</td>\n","      <td>3397.605000</td>\n","      <td>3667.864300</td>\n","      <td>7341.502000</td>\n","      <td>3346.084700</td>\n","      <td>2353.300000</td>\n","      <td>1814.849900</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>4378.4350</td>\n","      <td>974.763500</td>\n","      <td>2000.168500</td>\n","      <td>2225.862800</td>\n","      <td>2424.546900</td>\n","      <td>4539.164600</td>\n","      <td>18423.426</td>\n","      <td>2167.589800</td>\n","      <td>4568.273000</td>\n","      <td>1583.931800</td>\n","      <td>1579.40440</td>\n","      <td>4402.414600</td>\n","      <td>3037.582000</td>\n","      <td>2752.883800</td>\n","      <td>7850.789000</td>\n","      <td>1576.200000</td>\n","      <td>2607.787400</td>\n","      <td>2327.962600</td>\n","      <td>6709.235400</td>\n","      <td>2094.175000</td>\n","      <td>1695.050200</td>\n","      <td>525.354740</td>\n","      <td>412.214400</td>\n","      <td>3545.829800</td>\n","      <td>1086.32970</td>\n","      <td>1547.212200</td>\n","      <td>2106.362300</td>\n","      <td>4793.815000</td>\n","      <td>473.544130</td>\n","      <td>1077.293600</td>\n","      <td>835.025630</td>\n","      <td>3397.754000</td>\n","      <td>3674.004000</td>\n","      <td>7337.556000</td>\n","      <td>3329.152000</td>\n","      <td>2351.045200</td>\n","      <td>1816.811200</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>4365.0180</td>\n","      <td>971.709350</td>\n","      <td>1996.681300</td>\n","      <td>2223.435800</td>\n","      <td>2421.472400</td>\n","      <td>4548.329600</td>\n","      <td>18077.936</td>\n","      <td>2150.113000</td>\n","      <td>4556.689000</td>\n","      <td>1583.355500</td>\n","      <td>1579.18920</td>\n","      <td>4390.703600</td>\n","      <td>3037.150600</td>\n","      <td>2761.178700</td>\n","      <td>7858.791000</td>\n","      <td>1570.929200</td>\n","      <td>2597.231700</td>\n","      <td>2322.177700</td>\n","      <td>6721.586000</td>\n","      <td>2088.406200</td>\n","      <td>1692.537200</td>\n","      <td>531.757700</td>\n","      <td>422.102720</td>\n","      <td>3530.578400</td>\n","      <td>1092.83010</td>\n","      <td>1515.894900</td>\n","      <td>2102.850000</td>\n","      <td>4781.082500</td>\n","      <td>469.759460</td>\n","      <td>1076.836500</td>\n","      <td>833.440300</td>\n","      <td>3394.606700</td>\n","      <td>3667.116200</td>\n","      <td>7335.786000</td>\n","      <td>3326.426300</td>\n","      <td>2348.532200</td>\n","      <td>1815.694300</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>4367.1800</td>\n","      <td>965.750240</td>\n","      <td>1994.764300</td>\n","      <td>2223.172600</td>\n","      <td>2428.065000</td>\n","      <td>4540.393000</td>\n","      <td>18444.967</td>\n","      <td>2128.486600</td>\n","      <td>4556.950000</td>\n","      <td>1582.886100</td>\n","      <td>1579.02190</td>\n","      <td>4394.451000</td>\n","      <td>3035.157500</td>\n","      <td>2750.497000</td>\n","      <td>7848.744000</td>\n","      <td>1566.814000</td>\n","      <td>2590.212400</td>\n","      <td>2320.971700</td>\n","      <td>6689.449700</td>\n","      <td>2090.284200</td>\n","      <td>1693.084000</td>\n","      <td>526.609560</td>\n","      <td>411.397830</td>\n","      <td>3504.919000</td>\n","      <td>1092.00630</td>\n","      <td>1526.229600</td>\n","      <td>2102.057100</td>\n","      <td>4767.651400</td>\n","      <td>469.596300</td>\n","      <td>1078.094400</td>\n","      <td>832.830900</td>\n","      <td>3394.323000</td>\n","      <td>3656.531200</td>\n","      <td>7337.984400</td>\n","      <td>3324.256600</td>\n","      <td>2344.823000</td>\n","      <td>1803.806600</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>4363.0080</td>\n","      <td>971.078370</td>\n","      <td>1991.352200</td>\n","      <td>2217.588000</td>\n","      <td>2412.565400</td>\n","      <td>4541.528000</td>\n","      <td>18541.283</td>\n","      <td>2107.012000</td>\n","      <td>4562.030000</td>\n","      <td>1582.554900</td>\n","      <td>1578.99550</td>\n","      <td>4377.555000</td>\n","      <td>3038.085700</td>\n","      <td>2754.196300</td>\n","      <td>7859.098000</td>\n","      <td>1562.961800</td>\n","      <td>2582.215000</td>\n","      <td>2320.557400</td>\n","      <td>6723.793000</td>\n","      <td>2084.843500</td>\n","      <td>1688.386400</td>\n","      <td>528.610800</td>\n","      <td>414.540340</td>\n","      <td>3486.935800</td>\n","      <td>1090.54960</td>\n","      <td>1499.466400</td>\n","      <td>2089.349000</td>\n","      <td>4754.171400</td>\n","      <td>468.008200</td>\n","      <td>1076.613900</td>\n","      <td>833.228000</td>\n","      <td>3391.077400</td>\n","      <td>3648.243700</td>\n","      <td>7328.824000</td>\n","      <td>3310.292700</td>\n","      <td>2341.223600</td>\n","      <td>1818.593800</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>4359.4185</td>\n","      <td>972.753850</td>\n","      <td>1990.506000</td>\n","      <td>2218.793700</td>\n","      <td>2410.770500</td>\n","      <td>4533.883000</td>\n","      <td>18546.280</td>\n","      <td>2091.216300</td>\n","      <td>4554.184600</td>\n","      <td>1581.991500</td>\n","      <td>1578.76670</td>\n","      <td>4365.991000</td>\n","      <td>3030.712200</td>\n","      <td>2748.283200</td>\n","      <td>7855.883000</td>\n","      <td>1563.806900</td>\n","      <td>2571.571000</td>\n","      <td>2315.920200</td>\n","      <td>6682.018000</td>\n","      <td>2089.973100</td>\n","      <td>1686.343400</td>\n","      <td>517.231140</td>\n","      <td>416.548770</td>\n","      <td>3461.993200</td>\n","      <td>1088.51340</td>\n","      <td>1596.854000</td>\n","      <td>2082.740700</td>\n","      <td>4743.815000</td>\n","      <td>463.583800</td>\n","      <td>1074.436500</td>\n","      <td>832.030700</td>\n","      <td>3389.421400</td>\n","      <td>3652.032700</td>\n","      <td>7330.042000</td>\n","      <td>3299.863500</td>\n","      <td>2338.869600</td>\n","      <td>1805.844000</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>4356.3770</td>\n","      <td>957.827150</td>\n","      <td>1985.336200</td>\n","      <td>2215.656000</td>\n","      <td>2408.304200</td>\n","      <td>4530.926000</td>\n","      <td>18657.197</td>\n","      <td>2066.257600</td>\n","      <td>4552.204000</td>\n","      <td>1581.732700</td>\n","      <td>1578.57060</td>\n","      <td>4365.940400</td>\n","      <td>3034.206800</td>\n","      <td>2743.208700</td>\n","      <td>7845.728000</td>\n","      <td>1554.447100</td>\n","      <td>2563.746300</td>\n","      <td>2314.080600</td>\n","      <td>6668.375000</td>\n","      <td>2086.935300</td>\n","      <td>1687.127300</td>\n","      <td>526.982540</td>\n","      <td>412.384900</td>\n","      <td>3446.438500</td>\n","      <td>1092.07290</td>\n","      <td>1507.366800</td>\n","      <td>2081.271000</td>\n","      <td>4733.362000</td>\n","      <td>465.285740</td>\n","      <td>1072.992000</td>\n","      <td>831.146200</td>\n","      <td>3386.440000</td>\n","      <td>3637.037800</td>\n","      <td>7333.403300</td>\n","      <td>3304.216600</td>\n","      <td>2336.137200</td>\n","      <td>1801.111200</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-123c8d5e-528e-474a-bdfc-208be1772559')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-123c8d5e-528e-474a-bdfc-208be1772559 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-123c8d5e-528e-474a-bdfc-208be1772559');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["date = [f'd+{i}' for i in range(1,15)] + ['d+22 ~ 28 평균']\n","\n","\n","for k in range(10):\n","  globals()[f'answer_df_{k}'] = pd.DataFrame()\n","  for c in globals()[f'set_df_{k}'].columns:\n","    base_d = globals()[f'set_df_{k}'][c][0] # 변동률 기준 t 값\n","\n","    ans_1_14 = []\n","    for i in range(14):\n","      ans_1_14.append((globals()[f'set_df_{k}'][c].iloc[i+1]- base_d)/base_d)  # t+1 ~ t+14 까지는 (t+n - t)/t 로 계산\n","\n","    ans_22_28 = (globals()[f'set_df_{k}'][c][22:29].mean() - base_d)/base_d # t+22 ~ t+28은 np.mean(t+22 ~ t+28) - t / t\n","\n","    globals()[f'answer_df_{k}'][f'{c} 변동률'] = ans_1_14 + [ans_22_28]\n","  \n","  globals()[f'answer_df_{k}']['Set'] = k # set 번호 설정\n","  globals()[f'answer_df_{k}']['일자'] = date # 일자 설정"],"metadata":{"id":"K0PKPYlOMRZ9","executionInfo":{"status":"ok","timestamp":1664517437627,"user_tz":-540,"elapsed":670,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["# 위에서 계산된 변동률 들을 합쳐주는 과정\n","\n","all_df =pd.DataFrame()\n","for i in range(10):\n","  if i== 0 :\n","    all_df = pd.concat([all_df, globals()[f'answer_df_{i}']],axis=1)\n","  else:\n","    all_df = pd.concat([all_df, globals()[f'answer_df_{i}']])\n","\n","\n","all_df = all_df[['Set','일자'] + list(all_df.columns[:-2])]\n","all_df.reset_index(drop=True, inplace=True)"],"metadata":{"id":"fywX_l93MSap","executionInfo":{"status":"ok","timestamp":1664517440291,"user_tz":-540,"elapsed":388,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["# set, 일자 기억하기위해 따로 저장\n","\n","re_set = list(all_df['Set'])\n","re_date = list(all_df['일자'])\n","\n","\n","# 정답 양식 불러오기\n","out_ans = pd.read_csv('/content/drive/MyDrive/농산물예측/aT_data/answer_example.csv')\n","\n","# 두 dataframe 합치기 (nan + 숫자 = nan 이용)\n","submit_df = all_df + out_ans\n","\n","submit_df['Set'] = re_set\n","submit_df['일자'] = re_date\n","\n","\n","# 최종 저장\n","submit_df.to_csv(f'/content/drive/MyDrive/농산물예측/submit{try_cnt}.csv',index=False)"],"metadata":{"id":"wYQdShAqMTn_","executionInfo":{"status":"ok","timestamp":1664517441500,"user_tz":-540,"elapsed":438,"user":{"displayName":"이태영","userId":"15551978220746215296"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["all_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":540},"id":"pMr62pVJMUkD","executionInfo":{"status":"ok","timestamp":1664517442602,"user_tz":-540,"elapsed":8,"user":{"displayName":"이태영","userId":"15551978220746215296"}},"outputId":"48c2ddae-c1a7-42af-e82f-2540403a7c49"},"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     Set            일자   품목0 변동률   품목1 변동률   품목2 변동률   품목3 변동률   품목4 변동률  \\\n","0      0           d+1  0.127134 -0.247216 -0.293249 -0.340260 -0.377023   \n","1      0           d+2  0.130422 -0.258661 -0.294547 -0.339586 -0.376425   \n","2      0           d+3  0.128391 -0.250732 -0.295383 -0.339181 -0.373770   \n","3      0           d+4  0.129567 -0.250300 -0.295654 -0.338496 -0.374120   \n","4      0           d+5  0.130424 -0.256863 -0.296796 -0.338657 -0.374182   \n","..   ...           ...       ...       ...       ...       ...       ...   \n","145    9          d+11 -0.315563 -0.294538  0.517746 -0.027588 -0.137611   \n","146    9          d+12 -0.314320 -0.287023  0.515570 -0.028304 -0.137783   \n","147    9          d+13 -0.315636 -0.292982  0.513992 -0.028636 -0.141238   \n","148    9          d+14 -0.314352 -0.295481  0.511644 -0.029611 -0.140017   \n","149    9  d+22 ~ 28 평균 -0.316867 -0.309953  0.487567 -0.038927 -0.154240   \n","\n","      품목5 변동률   품목6 변동률   품목7 변동률   품목8 변동률   품목9 변동률  품목10 변동률  품목11 변동률  \\\n","0   -0.514354  2.780334 -0.337766 -0.062025  0.353913  0.184076  1.175962   \n","1   -0.513848  2.717060 -0.334872 -0.062407  0.353877  0.184024  1.171561   \n","2   -0.513420  2.614923 -0.331520 -0.058120  0.354133  0.183766  1.175997   \n","3   -0.513212  2.583048 -0.330416 -0.062885  0.354174  0.183356  1.170435   \n","4   -0.512571  2.624282 -0.328282 -0.061294  0.354200  0.182890  1.176863   \n","..        ...       ...       ...       ...       ...       ...       ...   \n","145 -0.024437 -0.292025       NaN -0.163089 -0.263739 -0.316575 -0.353504   \n","146 -0.025348 -0.287594       NaN -0.164038 -0.263926 -0.316824 -0.353733   \n","147 -0.024575 -0.274127       NaN -0.163841 -0.264068 -0.316876 -0.353771   \n","148 -0.024457 -0.253886       NaN -0.163575 -0.264135 -0.316897 -0.354429   \n","149 -0.024829  0.056013       NaN -0.169562 -0.266109 -0.316917 -0.364730   \n","\n","     품목12 변동률  품목13 변동률  품목14 변동률  품목15 변동률  ...  품목19 변동률  품목20 변동률  \\\n","0    0.020979 -0.149917  0.201896  0.447332  ... -0.309485 -0.434626   \n","1    0.020796 -0.149281  0.200679  0.452440  ... -0.309035 -0.433033   \n","2    0.022036 -0.145112  0.200257  0.447157  ... -0.309720 -0.432757   \n","3    0.021005 -0.141955  0.202157  0.452707  ... -0.308946 -0.431934   \n","4    0.020090 -0.141919  0.199770  0.450545  ... -0.308788 -0.431160   \n","..        ...       ...       ...       ...  ...       ...       ...   \n","145 -0.214992 -0.338639  0.144461 -0.277591  ...  0.389910  0.556477   \n","146 -0.215837 -0.342832  0.144421 -0.278624  ...  0.390793  0.557164   \n","147 -0.215857 -0.346155  0.142965 -0.281058  ...  0.394285  0.559368   \n","148 -0.215901 -0.348805  0.141517 -0.279738  ...  0.393540  0.559183   \n","149 -0.219956 -0.370431  0.139017 -0.285639  ...  0.397990  0.560289   \n","\n","     품목21 변동률  품목22 변동률  품목23 변동률  품목24 변동률  품목25 변동률  품목26 변동률  품목27 변동률  \\\n","0    0.045364 -0.578577 -0.144611 -0.101302 -0.491484 -0.302276 -0.451948   \n","1    0.036743 -0.561531 -0.140951 -0.099406 -0.514219 -0.300289 -0.450513   \n","2    0.052829 -0.546848 -0.135835 -0.099631 -0.498531 -0.300453 -0.449280   \n","3    0.047586 -0.559925 -0.133744 -0.101352 -0.477079 -0.298480 -0.448118   \n","4    0.048349 -0.550134 -0.131225 -0.100169 -0.505917 -0.296643 -0.446782   \n","..        ...       ...       ...       ...       ...       ...       ...   \n","145 -0.375792 -0.289497  0.792341 -0.008557 -0.366522  0.396114 -0.081817   \n","146 -0.373180 -0.307038  0.790785 -0.000075 -0.417863  0.394084 -0.081589   \n","147 -0.380756 -0.296589  0.794356 -0.006158 -0.373066  0.391550 -0.081599   \n","148 -0.378326 -0.282483  0.790901 -0.008084 -0.380274  0.391104 -0.081238   \n","149 -0.381921 -0.302421  0.728091 -0.011417 -0.403733  0.362108 -0.093289   \n","\n","     품목28 변동률  품목29 변동률  품목30 변동률  품목31 변동률  품목32 변동률  품목33 변동률  품목34 변동률  \\\n","0   -0.195996 -0.017345 -0.462755 -0.096276 -0.307547  0.308347 -0.369231   \n","1   -0.188934 -0.016883 -0.461535 -0.092711 -0.309496  0.310206 -0.365358   \n","2   -0.196246 -0.019788 -0.462830 -0.089554 -0.311773  0.310926 -0.366290   \n","3   -0.185597 -0.017035 -0.463328 -0.083791 -0.308409  0.309861 -0.364994   \n","4   -0.197201 -0.020749 -0.463055 -0.083104 -0.312752  0.311459 -0.363460   \n","..        ...       ...       ...       ...       ...       ...       ...   \n","145 -0.073207 -0.087892 -0.152119  0.025682 -0.169873  1.689788  0.750789   \n","146 -0.067754 -0.091781 -0.153674  0.028624 -0.170006  1.686174  0.744950   \n","147 -0.071780 -0.092443 -0.152914  0.028387 -0.175643  1.687887  0.753689   \n","148 -0.076592 -0.093147 -0.153986  0.028727 -0.176536  1.686111  0.744518   \n","149 -0.094712 -0.097719 -0.159099  0.030391 -0.184176  1.676719  0.728847   \n","\n","     품목35 변동률  품목36 변동률  \n","0   -0.197247 -0.132809  \n","1   -0.195988 -0.130960  \n","2   -0.195237 -0.131515  \n","3   -0.194116 -0.130148  \n","4   -0.192975 -0.131439  \n","..        ...       ...  \n","145 -0.034596  0.163012  \n","146 -0.034207  0.161094  \n","147 -0.033325  0.167779  \n","148 -0.032660  0.160881  \n","149 -0.040714  0.157476  \n","\n","[150 rows x 39 columns]"],"text/html":["\n","  <div id=\"df-be2f1525-e3b2-4c7b-ad9c-007807144fab\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Set</th>\n","      <th>일자</th>\n","      <th>품목0 변동률</th>\n","      <th>품목1 변동률</th>\n","      <th>품목2 변동률</th>\n","      <th>품목3 변동률</th>\n","      <th>품목4 변동률</th>\n","      <th>품목5 변동률</th>\n","      <th>품목6 변동률</th>\n","      <th>품목7 변동률</th>\n","      <th>품목8 변동률</th>\n","      <th>품목9 변동률</th>\n","      <th>품목10 변동률</th>\n","      <th>품목11 변동률</th>\n","      <th>품목12 변동률</th>\n","      <th>품목13 변동률</th>\n","      <th>품목14 변동률</th>\n","      <th>품목15 변동률</th>\n","      <th>...</th>\n","      <th>품목19 변동률</th>\n","      <th>품목20 변동률</th>\n","      <th>품목21 변동률</th>\n","      <th>품목22 변동률</th>\n","      <th>품목23 변동률</th>\n","      <th>품목24 변동률</th>\n","      <th>품목25 변동률</th>\n","      <th>품목26 변동률</th>\n","      <th>품목27 변동률</th>\n","      <th>품목28 변동률</th>\n","      <th>품목29 변동률</th>\n","      <th>품목30 변동률</th>\n","      <th>품목31 변동률</th>\n","      <th>품목32 변동률</th>\n","      <th>품목33 변동률</th>\n","      <th>품목34 변동률</th>\n","      <th>품목35 변동률</th>\n","      <th>품목36 변동률</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>d+1</td>\n","      <td>0.127134</td>\n","      <td>-0.247216</td>\n","      <td>-0.293249</td>\n","      <td>-0.340260</td>\n","      <td>-0.377023</td>\n","      <td>-0.514354</td>\n","      <td>2.780334</td>\n","      <td>-0.337766</td>\n","      <td>-0.062025</td>\n","      <td>0.353913</td>\n","      <td>0.184076</td>\n","      <td>1.175962</td>\n","      <td>0.020979</td>\n","      <td>-0.149917</td>\n","      <td>0.201896</td>\n","      <td>0.447332</td>\n","      <td>...</td>\n","      <td>-0.309485</td>\n","      <td>-0.434626</td>\n","      <td>0.045364</td>\n","      <td>-0.578577</td>\n","      <td>-0.144611</td>\n","      <td>-0.101302</td>\n","      <td>-0.491484</td>\n","      <td>-0.302276</td>\n","      <td>-0.451948</td>\n","      <td>-0.195996</td>\n","      <td>-0.017345</td>\n","      <td>-0.462755</td>\n","      <td>-0.096276</td>\n","      <td>-0.307547</td>\n","      <td>0.308347</td>\n","      <td>-0.369231</td>\n","      <td>-0.197247</td>\n","      <td>-0.132809</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>d+2</td>\n","      <td>0.130422</td>\n","      <td>-0.258661</td>\n","      <td>-0.294547</td>\n","      <td>-0.339586</td>\n","      <td>-0.376425</td>\n","      <td>-0.513848</td>\n","      <td>2.717060</td>\n","      <td>-0.334872</td>\n","      <td>-0.062407</td>\n","      <td>0.353877</td>\n","      <td>0.184024</td>\n","      <td>1.171561</td>\n","      <td>0.020796</td>\n","      <td>-0.149281</td>\n","      <td>0.200679</td>\n","      <td>0.452440</td>\n","      <td>...</td>\n","      <td>-0.309035</td>\n","      <td>-0.433033</td>\n","      <td>0.036743</td>\n","      <td>-0.561531</td>\n","      <td>-0.140951</td>\n","      <td>-0.099406</td>\n","      <td>-0.514219</td>\n","      <td>-0.300289</td>\n","      <td>-0.450513</td>\n","      <td>-0.188934</td>\n","      <td>-0.016883</td>\n","      <td>-0.461535</td>\n","      <td>-0.092711</td>\n","      <td>-0.309496</td>\n","      <td>0.310206</td>\n","      <td>-0.365358</td>\n","      <td>-0.195988</td>\n","      <td>-0.130960</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>d+3</td>\n","      <td>0.128391</td>\n","      <td>-0.250732</td>\n","      <td>-0.295383</td>\n","      <td>-0.339181</td>\n","      <td>-0.373770</td>\n","      <td>-0.513420</td>\n","      <td>2.614923</td>\n","      <td>-0.331520</td>\n","      <td>-0.058120</td>\n","      <td>0.354133</td>\n","      <td>0.183766</td>\n","      <td>1.175997</td>\n","      <td>0.022036</td>\n","      <td>-0.145112</td>\n","      <td>0.200257</td>\n","      <td>0.447157</td>\n","      <td>...</td>\n","      <td>-0.309720</td>\n","      <td>-0.432757</td>\n","      <td>0.052829</td>\n","      <td>-0.546848</td>\n","      <td>-0.135835</td>\n","      <td>-0.099631</td>\n","      <td>-0.498531</td>\n","      <td>-0.300453</td>\n","      <td>-0.449280</td>\n","      <td>-0.196246</td>\n","      <td>-0.019788</td>\n","      <td>-0.462830</td>\n","      <td>-0.089554</td>\n","      <td>-0.311773</td>\n","      <td>0.310926</td>\n","      <td>-0.366290</td>\n","      <td>-0.195237</td>\n","      <td>-0.131515</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>d+4</td>\n","      <td>0.129567</td>\n","      <td>-0.250300</td>\n","      <td>-0.295654</td>\n","      <td>-0.338496</td>\n","      <td>-0.374120</td>\n","      <td>-0.513212</td>\n","      <td>2.583048</td>\n","      <td>-0.330416</td>\n","      <td>-0.062885</td>\n","      <td>0.354174</td>\n","      <td>0.183356</td>\n","      <td>1.170435</td>\n","      <td>0.021005</td>\n","      <td>-0.141955</td>\n","      <td>0.202157</td>\n","      <td>0.452707</td>\n","      <td>...</td>\n","      <td>-0.308946</td>\n","      <td>-0.431934</td>\n","      <td>0.047586</td>\n","      <td>-0.559925</td>\n","      <td>-0.133744</td>\n","      <td>-0.101352</td>\n","      <td>-0.477079</td>\n","      <td>-0.298480</td>\n","      <td>-0.448118</td>\n","      <td>-0.185597</td>\n","      <td>-0.017035</td>\n","      <td>-0.463328</td>\n","      <td>-0.083791</td>\n","      <td>-0.308409</td>\n","      <td>0.309861</td>\n","      <td>-0.364994</td>\n","      <td>-0.194116</td>\n","      <td>-0.130148</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>d+5</td>\n","      <td>0.130424</td>\n","      <td>-0.256863</td>\n","      <td>-0.296796</td>\n","      <td>-0.338657</td>\n","      <td>-0.374182</td>\n","      <td>-0.512571</td>\n","      <td>2.624282</td>\n","      <td>-0.328282</td>\n","      <td>-0.061294</td>\n","      <td>0.354200</td>\n","      <td>0.182890</td>\n","      <td>1.176863</td>\n","      <td>0.020090</td>\n","      <td>-0.141919</td>\n","      <td>0.199770</td>\n","      <td>0.450545</td>\n","      <td>...</td>\n","      <td>-0.308788</td>\n","      <td>-0.431160</td>\n","      <td>0.048349</td>\n","      <td>-0.550134</td>\n","      <td>-0.131225</td>\n","      <td>-0.100169</td>\n","      <td>-0.505917</td>\n","      <td>-0.296643</td>\n","      <td>-0.446782</td>\n","      <td>-0.197201</td>\n","      <td>-0.020749</td>\n","      <td>-0.463055</td>\n","      <td>-0.083104</td>\n","      <td>-0.312752</td>\n","      <td>0.311459</td>\n","      <td>-0.363460</td>\n","      <td>-0.192975</td>\n","      <td>-0.131439</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>145</th>\n","      <td>9</td>\n","      <td>d+11</td>\n","      <td>-0.315563</td>\n","      <td>-0.294538</td>\n","      <td>0.517746</td>\n","      <td>-0.027588</td>\n","      <td>-0.137611</td>\n","      <td>-0.024437</td>\n","      <td>-0.292025</td>\n","      <td>NaN</td>\n","      <td>-0.163089</td>\n","      <td>-0.263739</td>\n","      <td>-0.316575</td>\n","      <td>-0.353504</td>\n","      <td>-0.214992</td>\n","      <td>-0.338639</td>\n","      <td>0.144461</td>\n","      <td>-0.277591</td>\n","      <td>...</td>\n","      <td>0.389910</td>\n","      <td>0.556477</td>\n","      <td>-0.375792</td>\n","      <td>-0.289497</td>\n","      <td>0.792341</td>\n","      <td>-0.008557</td>\n","      <td>-0.366522</td>\n","      <td>0.396114</td>\n","      <td>-0.081817</td>\n","      <td>-0.073207</td>\n","      <td>-0.087892</td>\n","      <td>-0.152119</td>\n","      <td>0.025682</td>\n","      <td>-0.169873</td>\n","      <td>1.689788</td>\n","      <td>0.750789</td>\n","      <td>-0.034596</td>\n","      <td>0.163012</td>\n","    </tr>\n","    <tr>\n","      <th>146</th>\n","      <td>9</td>\n","      <td>d+12</td>\n","      <td>-0.314320</td>\n","      <td>-0.287023</td>\n","      <td>0.515570</td>\n","      <td>-0.028304</td>\n","      <td>-0.137783</td>\n","      <td>-0.025348</td>\n","      <td>-0.287594</td>\n","      <td>NaN</td>\n","      <td>-0.164038</td>\n","      <td>-0.263926</td>\n","      <td>-0.316824</td>\n","      <td>-0.353733</td>\n","      <td>-0.215837</td>\n","      <td>-0.342832</td>\n","      <td>0.144421</td>\n","      <td>-0.278624</td>\n","      <td>...</td>\n","      <td>0.390793</td>\n","      <td>0.557164</td>\n","      <td>-0.373180</td>\n","      <td>-0.307038</td>\n","      <td>0.790785</td>\n","      <td>-0.000075</td>\n","      <td>-0.417863</td>\n","      <td>0.394084</td>\n","      <td>-0.081589</td>\n","      <td>-0.067754</td>\n","      <td>-0.091781</td>\n","      <td>-0.153674</td>\n","      <td>0.028624</td>\n","      <td>-0.170006</td>\n","      <td>1.686174</td>\n","      <td>0.744950</td>\n","      <td>-0.034207</td>\n","      <td>0.161094</td>\n","    </tr>\n","    <tr>\n","      <th>147</th>\n","      <td>9</td>\n","      <td>d+13</td>\n","      <td>-0.315636</td>\n","      <td>-0.292982</td>\n","      <td>0.513992</td>\n","      <td>-0.028636</td>\n","      <td>-0.141238</td>\n","      <td>-0.024575</td>\n","      <td>-0.274127</td>\n","      <td>NaN</td>\n","      <td>-0.163841</td>\n","      <td>-0.264068</td>\n","      <td>-0.316876</td>\n","      <td>-0.353771</td>\n","      <td>-0.215857</td>\n","      <td>-0.346155</td>\n","      <td>0.142965</td>\n","      <td>-0.281058</td>\n","      <td>...</td>\n","      <td>0.394285</td>\n","      <td>0.559368</td>\n","      <td>-0.380756</td>\n","      <td>-0.296589</td>\n","      <td>0.794356</td>\n","      <td>-0.006158</td>\n","      <td>-0.373066</td>\n","      <td>0.391550</td>\n","      <td>-0.081599</td>\n","      <td>-0.071780</td>\n","      <td>-0.092443</td>\n","      <td>-0.152914</td>\n","      <td>0.028387</td>\n","      <td>-0.175643</td>\n","      <td>1.687887</td>\n","      <td>0.753689</td>\n","      <td>-0.033325</td>\n","      <td>0.167779</td>\n","    </tr>\n","    <tr>\n","      <th>148</th>\n","      <td>9</td>\n","      <td>d+14</td>\n","      <td>-0.314352</td>\n","      <td>-0.295481</td>\n","      <td>0.511644</td>\n","      <td>-0.029611</td>\n","      <td>-0.140017</td>\n","      <td>-0.024457</td>\n","      <td>-0.253886</td>\n","      <td>NaN</td>\n","      <td>-0.163575</td>\n","      <td>-0.264135</td>\n","      <td>-0.316897</td>\n","      <td>-0.354429</td>\n","      <td>-0.215901</td>\n","      <td>-0.348805</td>\n","      <td>0.141517</td>\n","      <td>-0.279738</td>\n","      <td>...</td>\n","      <td>0.393540</td>\n","      <td>0.559183</td>\n","      <td>-0.378326</td>\n","      <td>-0.282483</td>\n","      <td>0.790901</td>\n","      <td>-0.008084</td>\n","      <td>-0.380274</td>\n","      <td>0.391104</td>\n","      <td>-0.081238</td>\n","      <td>-0.076592</td>\n","      <td>-0.093147</td>\n","      <td>-0.153986</td>\n","      <td>0.028727</td>\n","      <td>-0.176536</td>\n","      <td>1.686111</td>\n","      <td>0.744518</td>\n","      <td>-0.032660</td>\n","      <td>0.160881</td>\n","    </tr>\n","    <tr>\n","      <th>149</th>\n","      <td>9</td>\n","      <td>d+22 ~ 28 평균</td>\n","      <td>-0.316867</td>\n","      <td>-0.309953</td>\n","      <td>0.487567</td>\n","      <td>-0.038927</td>\n","      <td>-0.154240</td>\n","      <td>-0.024829</td>\n","      <td>0.056013</td>\n","      <td>NaN</td>\n","      <td>-0.169562</td>\n","      <td>-0.266109</td>\n","      <td>-0.316917</td>\n","      <td>-0.364730</td>\n","      <td>-0.219956</td>\n","      <td>-0.370431</td>\n","      <td>0.139017</td>\n","      <td>-0.285639</td>\n","      <td>...</td>\n","      <td>0.397990</td>\n","      <td>0.560289</td>\n","      <td>-0.381921</td>\n","      <td>-0.302421</td>\n","      <td>0.728091</td>\n","      <td>-0.011417</td>\n","      <td>-0.403733</td>\n","      <td>0.362108</td>\n","      <td>-0.093289</td>\n","      <td>-0.094712</td>\n","      <td>-0.097719</td>\n","      <td>-0.159099</td>\n","      <td>0.030391</td>\n","      <td>-0.184176</td>\n","      <td>1.676719</td>\n","      <td>0.728847</td>\n","      <td>-0.040714</td>\n","      <td>0.157476</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>150 rows × 39 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-be2f1525-e3b2-4c7b-ad9c-007807144fab')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-be2f1525-e3b2-4c7b-ad9c-007807144fab button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-be2f1525-e3b2-4c7b-ad9c-007807144fab');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":["out_ans"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":609},"id":"-NB0HwOnMVZM","executionInfo":{"status":"ok","timestamp":1664517442603,"user_tz":-540,"elapsed":8,"user":{"displayName":"이태영","userId":"15551978220746215296"}},"outputId":"3149cb75-04a4-4b77-e363-fa7dd862ccb9"},"execution_count":49,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     Set            일자  품목0 변동률  품목1 변동률  품목2 변동률  품목3 변동률  품목4 변동률  품목5 변동률  \\\n","0      0           d+1      0.0      0.0      0.0      0.0      0.0      0.0   \n","1      0           d+2      0.0      0.0      0.0      0.0      0.0      0.0   \n","2      0           d+3      NaN      NaN      NaN      NaN      NaN      NaN   \n","3      0           d+4      0.0      0.0      0.0      0.0      0.0      0.0   \n","4      0           d+5      0.0      0.0      0.0      0.0      0.0      0.0   \n","..   ...           ...      ...      ...      ...      ...      ...      ...   \n","145    9          d+11      0.0      0.0      0.0      0.0      0.0      0.0   \n","146    9          d+12      0.0      0.0      0.0      0.0      0.0      0.0   \n","147    9          d+13      0.0      0.0      0.0      0.0      0.0      0.0   \n","148    9          d+14      0.0      0.0      0.0      0.0      0.0      0.0   \n","149    9  d+22 ~ 28 평균      0.0      0.0      0.0      0.0      0.0      0.0   \n","\n","     품목6 변동률  품목7 변동률  품목8 변동률  품목9 변동률  품목10 변동률  품목11 변동률  품목12 변동률  \\\n","0        0.0      0.0      0.0      0.0       0.0       0.0       0.0   \n","1        NaN      0.0      0.0      0.0       0.0       0.0       0.0   \n","2        0.0      NaN      NaN      NaN       NaN       NaN       NaN   \n","3        NaN      0.0      0.0      0.0       0.0       0.0       0.0   \n","4        NaN      0.0      0.0      0.0       0.0       0.0       0.0   \n","..       ...      ...      ...      ...       ...       ...       ...   \n","145      0.0      NaN      0.0      0.0       0.0       0.0       0.0   \n","146      0.0      NaN      0.0      0.0       0.0       0.0       0.0   \n","147      0.0      NaN      0.0      0.0       0.0       0.0       0.0   \n","148      0.0      NaN      0.0      0.0       0.0       0.0       0.0   \n","149      NaN      NaN      0.0      0.0       0.0       0.0       0.0   \n","\n","     품목13 변동률  품목14 변동률  품목15 변동률  ...  품목19 변동률  품목20 변동률  품목21 변동률  \\\n","0         0.0       0.0       0.0  ...       0.0       0.0       0.0   \n","1         0.0       0.0       0.0  ...       0.0       0.0       0.0   \n","2         NaN       NaN       NaN  ...       NaN       NaN       NaN   \n","3         0.0       0.0       0.0  ...       0.0       0.0       0.0   \n","4         0.0       0.0       0.0  ...       0.0       0.0       0.0   \n","..        ...       ...       ...  ...       ...       ...       ...   \n","145       0.0       0.0       0.0  ...       0.0       0.0       0.0   \n","146       0.0       0.0       0.0  ...       0.0       0.0       0.0   \n","147       0.0       0.0       0.0  ...       0.0       0.0       0.0   \n","148       0.0       0.0       0.0  ...       0.0       0.0       0.0   \n","149       0.0       0.0       0.0  ...       0.0       0.0       0.0   \n","\n","     품목22 변동률  품목23 변동률  품목24 변동률  품목25 변동률  품목26 변동률  품목27 변동률  품목28 변동률  \\\n","0         0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n","1         0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n","2         NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","3         0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n","4         0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n","..        ...       ...       ...       ...       ...       ...       ...   \n","145       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n","146       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n","147       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n","148       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n","149       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n","\n","     품목29 변동률  품목30 변동률  품목31 변동률  품목32 변동률  품목33 변동률  품목34 변동률  품목35 변동률  \\\n","0         0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n","1         0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n","2         NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","3         0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n","4         0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n","..        ...       ...       ...       ...       ...       ...       ...   \n","145       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n","146       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n","147       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n","148       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n","149       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n","\n","     품목36 변동률  \n","0         0.0  \n","1         0.0  \n","2         NaN  \n","3         0.0  \n","4         0.0  \n","..        ...  \n","145       0.0  \n","146       0.0  \n","147       0.0  \n","148       0.0  \n","149       0.0  \n","\n","[150 rows x 39 columns]"],"text/html":["\n","  <div id=\"df-237aca36-4b14-44a3-9936-b01a4ab6302b\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Set</th>\n","      <th>일자</th>\n","      <th>품목0 변동률</th>\n","      <th>품목1 변동률</th>\n","      <th>품목2 변동률</th>\n","      <th>품목3 변동률</th>\n","      <th>품목4 변동률</th>\n","      <th>품목5 변동률</th>\n","      <th>품목6 변동률</th>\n","      <th>품목7 변동률</th>\n","      <th>품목8 변동률</th>\n","      <th>품목9 변동률</th>\n","      <th>품목10 변동률</th>\n","      <th>품목11 변동률</th>\n","      <th>품목12 변동률</th>\n","      <th>품목13 변동률</th>\n","      <th>품목14 변동률</th>\n","      <th>품목15 변동률</th>\n","      <th>...</th>\n","      <th>품목19 변동률</th>\n","      <th>품목20 변동률</th>\n","      <th>품목21 변동률</th>\n","      <th>품목22 변동률</th>\n","      <th>품목23 변동률</th>\n","      <th>품목24 변동률</th>\n","      <th>품목25 변동률</th>\n","      <th>품목26 변동률</th>\n","      <th>품목27 변동률</th>\n","      <th>품목28 변동률</th>\n","      <th>품목29 변동률</th>\n","      <th>품목30 변동률</th>\n","      <th>품목31 변동률</th>\n","      <th>품목32 변동률</th>\n","      <th>품목33 변동률</th>\n","      <th>품목34 변동률</th>\n","      <th>품목35 변동률</th>\n","      <th>품목36 변동률</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>d+1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>d+2</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>d+3</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>d+4</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>d+5</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>145</th>\n","      <td>9</td>\n","      <td>d+11</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>146</th>\n","      <td>9</td>\n","      <td>d+12</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>147</th>\n","      <td>9</td>\n","      <td>d+13</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>148</th>\n","      <td>9</td>\n","      <td>d+14</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>149</th>\n","      <td>9</td>\n","      <td>d+22 ~ 28 평균</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>150 rows × 39 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-237aca36-4b14-44a3-9936-b01a4ab6302b')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-237aca36-4b14-44a3-9936-b01a4ab6302b button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-237aca36-4b14-44a3-9936-b01a4ab6302b');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":49}]},{"cell_type":"code","source":["submit_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":540},"id":"xBBcu2K0MWLi","executionInfo":{"status":"ok","timestamp":1664517443010,"user_tz":-540,"elapsed":414,"user":{"displayName":"이태영","userId":"15551978220746215296"}},"outputId":"9837a3f8-3472-418e-c492-3c0c9de67f83"},"execution_count":50,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     Set            일자   품목0 변동률   품목1 변동률   품목2 변동률   품목3 변동률   품목4 변동률  \\\n","0      0           d+1  0.127134 -0.247216 -0.293249 -0.340260 -0.377023   \n","1      0           d+2  0.130422 -0.258661 -0.294547 -0.339586 -0.376425   \n","2      0           d+3       NaN       NaN       NaN       NaN       NaN   \n","3      0           d+4  0.129567 -0.250300 -0.295654 -0.338496 -0.374120   \n","4      0           d+5  0.130424 -0.256863 -0.296796 -0.338657 -0.374182   \n","..   ...           ...       ...       ...       ...       ...       ...   \n","145    9          d+11 -0.315563 -0.294538  0.517746 -0.027588 -0.137611   \n","146    9          d+12 -0.314320 -0.287023  0.515570 -0.028304 -0.137783   \n","147    9          d+13 -0.315636 -0.292982  0.513992 -0.028636 -0.141238   \n","148    9          d+14 -0.314352 -0.295481  0.511644 -0.029611 -0.140017   \n","149    9  d+22 ~ 28 평균 -0.316867 -0.309953  0.487567 -0.038927 -0.154240   \n","\n","      품목5 변동률   품목6 변동률   품목7 변동률   품목8 변동률   품목9 변동률  품목10 변동률  품목11 변동률  \\\n","0   -0.514354  2.780334 -0.337766 -0.062025  0.353913  0.184076  1.175962   \n","1   -0.513848       NaN -0.334872 -0.062407  0.353877  0.184024  1.171561   \n","2         NaN  2.614923       NaN       NaN       NaN       NaN       NaN   \n","3   -0.513212       NaN -0.330416 -0.062885  0.354174  0.183356  1.170435   \n","4   -0.512571       NaN -0.328282 -0.061294  0.354200  0.182890  1.176863   \n","..        ...       ...       ...       ...       ...       ...       ...   \n","145 -0.024437 -0.292025       NaN -0.163089 -0.263739 -0.316575 -0.353504   \n","146 -0.025348 -0.287594       NaN -0.164038 -0.263926 -0.316824 -0.353733   \n","147 -0.024575 -0.274127       NaN -0.163841 -0.264068 -0.316876 -0.353771   \n","148 -0.024457 -0.253886       NaN -0.163575 -0.264135 -0.316897 -0.354429   \n","149 -0.024829       NaN       NaN -0.169562 -0.266109 -0.316917 -0.364730   \n","\n","     품목12 변동률  품목13 변동률  품목14 변동률  품목15 변동률  ...  품목19 변동률  품목20 변동률  \\\n","0    0.020979 -0.149917  0.201896  0.447332  ... -0.309485 -0.434626   \n","1    0.020796 -0.149281  0.200679  0.452440  ... -0.309035 -0.433033   \n","2         NaN       NaN       NaN       NaN  ...       NaN       NaN   \n","3    0.021005 -0.141955  0.202157  0.452707  ... -0.308946 -0.431934   \n","4    0.020090 -0.141919  0.199770  0.450545  ... -0.308788 -0.431160   \n","..        ...       ...       ...       ...  ...       ...       ...   \n","145 -0.214992 -0.338639  0.144461 -0.277591  ...  0.389910  0.556477   \n","146 -0.215837 -0.342832  0.144421 -0.278624  ...  0.390793  0.557164   \n","147 -0.215857 -0.346155  0.142965 -0.281058  ...  0.394285  0.559368   \n","148 -0.215901 -0.348805  0.141517 -0.279738  ...  0.393540  0.559183   \n","149 -0.219956 -0.370431  0.139017 -0.285639  ...  0.397990  0.560289   \n","\n","     품목21 변동률  품목22 변동률  품목23 변동률  품목24 변동률  품목25 변동률  품목26 변동률  품목27 변동률  \\\n","0    0.045364 -0.578577 -0.144611 -0.101302 -0.491484 -0.302276 -0.451948   \n","1    0.036743 -0.561531 -0.140951 -0.099406 -0.514219 -0.300289 -0.450513   \n","2         NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","3    0.047586 -0.559925 -0.133744 -0.101352 -0.477079 -0.298480 -0.448118   \n","4    0.048349 -0.550134 -0.131225 -0.100169 -0.505917 -0.296643 -0.446782   \n","..        ...       ...       ...       ...       ...       ...       ...   \n","145 -0.375792 -0.289497  0.792341 -0.008557 -0.366522  0.396114 -0.081817   \n","146 -0.373180 -0.307038  0.790785 -0.000075 -0.417863  0.394084 -0.081589   \n","147 -0.380756 -0.296589  0.794356 -0.006158 -0.373066  0.391550 -0.081599   \n","148 -0.378326 -0.282483  0.790901 -0.008084 -0.380274  0.391104 -0.081238   \n","149 -0.381921 -0.302421  0.728091 -0.011417 -0.403733  0.362108 -0.093289   \n","\n","     품목28 변동률  품목29 변동률  품목30 변동률  품목31 변동률  품목32 변동률  품목33 변동률  품목34 변동률  \\\n","0   -0.195996 -0.017345 -0.462755 -0.096276 -0.307547  0.308347 -0.369231   \n","1   -0.188934 -0.016883 -0.461535 -0.092711 -0.309496  0.310206 -0.365358   \n","2         NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","3   -0.185597 -0.017035 -0.463328 -0.083791 -0.308409  0.309861 -0.364994   \n","4   -0.197201 -0.020749 -0.463055 -0.083104 -0.312752  0.311459 -0.363460   \n","..        ...       ...       ...       ...       ...       ...       ...   \n","145 -0.073207 -0.087892 -0.152119  0.025682 -0.169873  1.689788  0.750789   \n","146 -0.067754 -0.091781 -0.153674  0.028624 -0.170006  1.686174  0.744950   \n","147 -0.071780 -0.092443 -0.152914  0.028387 -0.175643  1.687887  0.753689   \n","148 -0.076592 -0.093147 -0.153986  0.028727 -0.176536  1.686111  0.744518   \n","149 -0.094712 -0.097719 -0.159099  0.030391 -0.184176  1.676719  0.728847   \n","\n","     품목35 변동률  품목36 변동률  \n","0   -0.197247 -0.132809  \n","1   -0.195988 -0.130960  \n","2         NaN       NaN  \n","3   -0.194116 -0.130148  \n","4   -0.192975 -0.131439  \n","..        ...       ...  \n","145 -0.034596  0.163012  \n","146 -0.034207  0.161094  \n","147 -0.033325  0.167779  \n","148 -0.032660  0.160881  \n","149 -0.040714  0.157476  \n","\n","[150 rows x 39 columns]"],"text/html":["\n","  <div id=\"df-a9a7944d-cd66-43d5-a54f-efc4c828a2c9\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Set</th>\n","      <th>일자</th>\n","      <th>품목0 변동률</th>\n","      <th>품목1 변동률</th>\n","      <th>품목2 변동률</th>\n","      <th>품목3 변동률</th>\n","      <th>품목4 변동률</th>\n","      <th>품목5 변동률</th>\n","      <th>품목6 변동률</th>\n","      <th>품목7 변동률</th>\n","      <th>품목8 변동률</th>\n","      <th>품목9 변동률</th>\n","      <th>품목10 변동률</th>\n","      <th>품목11 변동률</th>\n","      <th>품목12 변동률</th>\n","      <th>품목13 변동률</th>\n","      <th>품목14 변동률</th>\n","      <th>품목15 변동률</th>\n","      <th>...</th>\n","      <th>품목19 변동률</th>\n","      <th>품목20 변동률</th>\n","      <th>품목21 변동률</th>\n","      <th>품목22 변동률</th>\n","      <th>품목23 변동률</th>\n","      <th>품목24 변동률</th>\n","      <th>품목25 변동률</th>\n","      <th>품목26 변동률</th>\n","      <th>품목27 변동률</th>\n","      <th>품목28 변동률</th>\n","      <th>품목29 변동률</th>\n","      <th>품목30 변동률</th>\n","      <th>품목31 변동률</th>\n","      <th>품목32 변동률</th>\n","      <th>품목33 변동률</th>\n","      <th>품목34 변동률</th>\n","      <th>품목35 변동률</th>\n","      <th>품목36 변동률</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>d+1</td>\n","      <td>0.127134</td>\n","      <td>-0.247216</td>\n","      <td>-0.293249</td>\n","      <td>-0.340260</td>\n","      <td>-0.377023</td>\n","      <td>-0.514354</td>\n","      <td>2.780334</td>\n","      <td>-0.337766</td>\n","      <td>-0.062025</td>\n","      <td>0.353913</td>\n","      <td>0.184076</td>\n","      <td>1.175962</td>\n","      <td>0.020979</td>\n","      <td>-0.149917</td>\n","      <td>0.201896</td>\n","      <td>0.447332</td>\n","      <td>...</td>\n","      <td>-0.309485</td>\n","      <td>-0.434626</td>\n","      <td>0.045364</td>\n","      <td>-0.578577</td>\n","      <td>-0.144611</td>\n","      <td>-0.101302</td>\n","      <td>-0.491484</td>\n","      <td>-0.302276</td>\n","      <td>-0.451948</td>\n","      <td>-0.195996</td>\n","      <td>-0.017345</td>\n","      <td>-0.462755</td>\n","      <td>-0.096276</td>\n","      <td>-0.307547</td>\n","      <td>0.308347</td>\n","      <td>-0.369231</td>\n","      <td>-0.197247</td>\n","      <td>-0.132809</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>d+2</td>\n","      <td>0.130422</td>\n","      <td>-0.258661</td>\n","      <td>-0.294547</td>\n","      <td>-0.339586</td>\n","      <td>-0.376425</td>\n","      <td>-0.513848</td>\n","      <td>NaN</td>\n","      <td>-0.334872</td>\n","      <td>-0.062407</td>\n","      <td>0.353877</td>\n","      <td>0.184024</td>\n","      <td>1.171561</td>\n","      <td>0.020796</td>\n","      <td>-0.149281</td>\n","      <td>0.200679</td>\n","      <td>0.452440</td>\n","      <td>...</td>\n","      <td>-0.309035</td>\n","      <td>-0.433033</td>\n","      <td>0.036743</td>\n","      <td>-0.561531</td>\n","      <td>-0.140951</td>\n","      <td>-0.099406</td>\n","      <td>-0.514219</td>\n","      <td>-0.300289</td>\n","      <td>-0.450513</td>\n","      <td>-0.188934</td>\n","      <td>-0.016883</td>\n","      <td>-0.461535</td>\n","      <td>-0.092711</td>\n","      <td>-0.309496</td>\n","      <td>0.310206</td>\n","      <td>-0.365358</td>\n","      <td>-0.195988</td>\n","      <td>-0.130960</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>d+3</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>2.614923</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>d+4</td>\n","      <td>0.129567</td>\n","      <td>-0.250300</td>\n","      <td>-0.295654</td>\n","      <td>-0.338496</td>\n","      <td>-0.374120</td>\n","      <td>-0.513212</td>\n","      <td>NaN</td>\n","      <td>-0.330416</td>\n","      <td>-0.062885</td>\n","      <td>0.354174</td>\n","      <td>0.183356</td>\n","      <td>1.170435</td>\n","      <td>0.021005</td>\n","      <td>-0.141955</td>\n","      <td>0.202157</td>\n","      <td>0.452707</td>\n","      <td>...</td>\n","      <td>-0.308946</td>\n","      <td>-0.431934</td>\n","      <td>0.047586</td>\n","      <td>-0.559925</td>\n","      <td>-0.133744</td>\n","      <td>-0.101352</td>\n","      <td>-0.477079</td>\n","      <td>-0.298480</td>\n","      <td>-0.448118</td>\n","      <td>-0.185597</td>\n","      <td>-0.017035</td>\n","      <td>-0.463328</td>\n","      <td>-0.083791</td>\n","      <td>-0.308409</td>\n","      <td>0.309861</td>\n","      <td>-0.364994</td>\n","      <td>-0.194116</td>\n","      <td>-0.130148</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>d+5</td>\n","      <td>0.130424</td>\n","      <td>-0.256863</td>\n","      <td>-0.296796</td>\n","      <td>-0.338657</td>\n","      <td>-0.374182</td>\n","      <td>-0.512571</td>\n","      <td>NaN</td>\n","      <td>-0.328282</td>\n","      <td>-0.061294</td>\n","      <td>0.354200</td>\n","      <td>0.182890</td>\n","      <td>1.176863</td>\n","      <td>0.020090</td>\n","      <td>-0.141919</td>\n","      <td>0.199770</td>\n","      <td>0.450545</td>\n","      <td>...</td>\n","      <td>-0.308788</td>\n","      <td>-0.431160</td>\n","      <td>0.048349</td>\n","      <td>-0.550134</td>\n","      <td>-0.131225</td>\n","      <td>-0.100169</td>\n","      <td>-0.505917</td>\n","      <td>-0.296643</td>\n","      <td>-0.446782</td>\n","      <td>-0.197201</td>\n","      <td>-0.020749</td>\n","      <td>-0.463055</td>\n","      <td>-0.083104</td>\n","      <td>-0.312752</td>\n","      <td>0.311459</td>\n","      <td>-0.363460</td>\n","      <td>-0.192975</td>\n","      <td>-0.131439</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>145</th>\n","      <td>9</td>\n","      <td>d+11</td>\n","      <td>-0.315563</td>\n","      <td>-0.294538</td>\n","      <td>0.517746</td>\n","      <td>-0.027588</td>\n","      <td>-0.137611</td>\n","      <td>-0.024437</td>\n","      <td>-0.292025</td>\n","      <td>NaN</td>\n","      <td>-0.163089</td>\n","      <td>-0.263739</td>\n","      <td>-0.316575</td>\n","      <td>-0.353504</td>\n","      <td>-0.214992</td>\n","      <td>-0.338639</td>\n","      <td>0.144461</td>\n","      <td>-0.277591</td>\n","      <td>...</td>\n","      <td>0.389910</td>\n","      <td>0.556477</td>\n","      <td>-0.375792</td>\n","      <td>-0.289497</td>\n","      <td>0.792341</td>\n","      <td>-0.008557</td>\n","      <td>-0.366522</td>\n","      <td>0.396114</td>\n","      <td>-0.081817</td>\n","      <td>-0.073207</td>\n","      <td>-0.087892</td>\n","      <td>-0.152119</td>\n","      <td>0.025682</td>\n","      <td>-0.169873</td>\n","      <td>1.689788</td>\n","      <td>0.750789</td>\n","      <td>-0.034596</td>\n","      <td>0.163012</td>\n","    </tr>\n","    <tr>\n","      <th>146</th>\n","      <td>9</td>\n","      <td>d+12</td>\n","      <td>-0.314320</td>\n","      <td>-0.287023</td>\n","      <td>0.515570</td>\n","      <td>-0.028304</td>\n","      <td>-0.137783</td>\n","      <td>-0.025348</td>\n","      <td>-0.287594</td>\n","      <td>NaN</td>\n","      <td>-0.164038</td>\n","      <td>-0.263926</td>\n","      <td>-0.316824</td>\n","      <td>-0.353733</td>\n","      <td>-0.215837</td>\n","      <td>-0.342832</td>\n","      <td>0.144421</td>\n","      <td>-0.278624</td>\n","      <td>...</td>\n","      <td>0.390793</td>\n","      <td>0.557164</td>\n","      <td>-0.373180</td>\n","      <td>-0.307038</td>\n","      <td>0.790785</td>\n","      <td>-0.000075</td>\n","      <td>-0.417863</td>\n","      <td>0.394084</td>\n","      <td>-0.081589</td>\n","      <td>-0.067754</td>\n","      <td>-0.091781</td>\n","      <td>-0.153674</td>\n","      <td>0.028624</td>\n","      <td>-0.170006</td>\n","      <td>1.686174</td>\n","      <td>0.744950</td>\n","      <td>-0.034207</td>\n","      <td>0.161094</td>\n","    </tr>\n","    <tr>\n","      <th>147</th>\n","      <td>9</td>\n","      <td>d+13</td>\n","      <td>-0.315636</td>\n","      <td>-0.292982</td>\n","      <td>0.513992</td>\n","      <td>-0.028636</td>\n","      <td>-0.141238</td>\n","      <td>-0.024575</td>\n","      <td>-0.274127</td>\n","      <td>NaN</td>\n","      <td>-0.163841</td>\n","      <td>-0.264068</td>\n","      <td>-0.316876</td>\n","      <td>-0.353771</td>\n","      <td>-0.215857</td>\n","      <td>-0.346155</td>\n","      <td>0.142965</td>\n","      <td>-0.281058</td>\n","      <td>...</td>\n","      <td>0.394285</td>\n","      <td>0.559368</td>\n","      <td>-0.380756</td>\n","      <td>-0.296589</td>\n","      <td>0.794356</td>\n","      <td>-0.006158</td>\n","      <td>-0.373066</td>\n","      <td>0.391550</td>\n","      <td>-0.081599</td>\n","      <td>-0.071780</td>\n","      <td>-0.092443</td>\n","      <td>-0.152914</td>\n","      <td>0.028387</td>\n","      <td>-0.175643</td>\n","      <td>1.687887</td>\n","      <td>0.753689</td>\n","      <td>-0.033325</td>\n","      <td>0.167779</td>\n","    </tr>\n","    <tr>\n","      <th>148</th>\n","      <td>9</td>\n","      <td>d+14</td>\n","      <td>-0.314352</td>\n","      <td>-0.295481</td>\n","      <td>0.511644</td>\n","      <td>-0.029611</td>\n","      <td>-0.140017</td>\n","      <td>-0.024457</td>\n","      <td>-0.253886</td>\n","      <td>NaN</td>\n","      <td>-0.163575</td>\n","      <td>-0.264135</td>\n","      <td>-0.316897</td>\n","      <td>-0.354429</td>\n","      <td>-0.215901</td>\n","      <td>-0.348805</td>\n","      <td>0.141517</td>\n","      <td>-0.279738</td>\n","      <td>...</td>\n","      <td>0.393540</td>\n","      <td>0.559183</td>\n","      <td>-0.378326</td>\n","      <td>-0.282483</td>\n","      <td>0.790901</td>\n","      <td>-0.008084</td>\n","      <td>-0.380274</td>\n","      <td>0.391104</td>\n","      <td>-0.081238</td>\n","      <td>-0.076592</td>\n","      <td>-0.093147</td>\n","      <td>-0.153986</td>\n","      <td>0.028727</td>\n","      <td>-0.176536</td>\n","      <td>1.686111</td>\n","      <td>0.744518</td>\n","      <td>-0.032660</td>\n","      <td>0.160881</td>\n","    </tr>\n","    <tr>\n","      <th>149</th>\n","      <td>9</td>\n","      <td>d+22 ~ 28 평균</td>\n","      <td>-0.316867</td>\n","      <td>-0.309953</td>\n","      <td>0.487567</td>\n","      <td>-0.038927</td>\n","      <td>-0.154240</td>\n","      <td>-0.024829</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>-0.169562</td>\n","      <td>-0.266109</td>\n","      <td>-0.316917</td>\n","      <td>-0.364730</td>\n","      <td>-0.219956</td>\n","      <td>-0.370431</td>\n","      <td>0.139017</td>\n","      <td>-0.285639</td>\n","      <td>...</td>\n","      <td>0.397990</td>\n","      <td>0.560289</td>\n","      <td>-0.381921</td>\n","      <td>-0.302421</td>\n","      <td>0.728091</td>\n","      <td>-0.011417</td>\n","      <td>-0.403733</td>\n","      <td>0.362108</td>\n","      <td>-0.093289</td>\n","      <td>-0.094712</td>\n","      <td>-0.097719</td>\n","      <td>-0.159099</td>\n","      <td>0.030391</td>\n","      <td>-0.184176</td>\n","      <td>1.676719</td>\n","      <td>0.728847</td>\n","      <td>-0.040714</td>\n","      <td>0.157476</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>150 rows × 39 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a9a7944d-cd66-43d5-a54f-efc4c828a2c9')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-a9a7944d-cd66-43d5-a54f-efc4c828a2c9 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-a9a7944d-cd66-43d5-a54f-efc4c828a2c9');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":50}]},{"cell_type":"code","source":[],"metadata":{"id":"nZI_y6ghMXTZ"},"execution_count":null,"outputs":[]}]}